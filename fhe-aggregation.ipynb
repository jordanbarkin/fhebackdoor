{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6246c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import  torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0ca13876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure GPU (change if not M1 mac)\n",
    "mps = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5ea75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "\n",
    "# Using CIFAR-10 again as in the programming assignments\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98e915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving And helpers\n",
    "\n",
    "def save_tracker(tracker, path):\n",
    "  np.savetxt(path, tracker, delimiter=',') \n",
    "\n",
    "def save_trackers(device, filename):\n",
    "  \"\"\"Save all trackers and current total_time to a file.\"\"\"\n",
    "  torch.save((device['train_loss_tracker'], device['train_acc_tracker'], device['test_loss_tracker'], device['test_acc_tracker'], total_time), filename)\n",
    "  print(\"Saved trackers to \" + filename)\n",
    "\n",
    "def moving_average(a, n=100):\n",
    "  '''Helper function used for visualization'''\n",
    "  ret = torch.cumsum(torch.Tensor(a), 0)\n",
    "  ret[n:] = ret[n:] - ret[:-n]\n",
    "  return ret[n - 1:] / n\n",
    "\n",
    "# Plotting helpers! \n",
    "def make_plot(trackers, num_epochs, title, y_axis_lab, should_average=False, legend=True, fix_ax=True):\n",
    "  avg_fn = moving_average if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  if fix_ax:\n",
    "    ax.set_ylim([0, 100])\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "\n",
    "  if legend:\n",
    "    _ = plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def make_plot_better(trackers, num_epochs, title, y_axis_lab, should_average=False, n = 100):\n",
    "  avg_fn = (lambda x : moving_average(x, n)) if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "  _ = plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999f6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "def iid_sampler(dataset, num_devices, data_pct):\n",
    "    '''\n",
    "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
    "    num_devices: integer number of devices to create subsets for\n",
    "    data_pct: percentalge of training samples to give each device\n",
    "              e.g., 0.1 represents 10%\n",
    "\n",
    "    return: a dictionary of the following format:\n",
    "      {\n",
    "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
    "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    iid (independent and identically distributed) means that the indexes\n",
    "    should be drawn independently in a uniformly random fashion.\n",
    "    '''\n",
    "    total_samples = len(dataset)\n",
    "    sampled = {}\n",
    "    number_samples = int((data_pct)*(total_samples)) \n",
    "\n",
    "    for i in range(num_devices):\n",
    "      sampled[i] = random.sample(range(total_samples), number_samples)\n",
    "        \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a92b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8465a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net definitions\n",
    "\n",
    "# Same ConvNet as in Assignment 2 and 3\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30b1dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated learning helpers\n",
    "\n",
    "# The baseline `average' function. \n",
    "def average_weights(devices):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    max_magnitude = 0\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "      w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            max_magnitude = max(max_magnitude, abs(torch.max(state_dicts[i][k].type(torch.float32))))\n",
    "            w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg, max_magnitude\n",
    "\n",
    "\n",
    "def get_devices_for_round(devices, device_pct):  \n",
    "    return random.sample(devices, int(device_pct * len(devices)))\n",
    "\n",
    "def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n",
    "                  milestones=None, batch_size=128):\n",
    "    if milestones == None:\n",
    "        milestones = [25, 50, 75]\n",
    "\n",
    "    device_net = copy.deepcopy(net)\n",
    "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=milestones,\n",
    "                                                     gamma=0.1)\n",
    "    device_trainset = DatasetSplit(trainset, data_idxs)\n",
    "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "    return {\n",
    "        'net': device_net,\n",
    "        'id': device_id,\n",
    "        'dataloader': device_trainloader, \n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_tracker': [],\n",
    "        'train_acc_tracker': [],\n",
    "        'test_loss_tracker': [],\n",
    "        'test_acc_tracker': [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd222559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local device training and testing\n",
    "def train(epoch, device, criterion):\n",
    "    device['net'].train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
    "        inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "        device['optimizer'].zero_grad()\n",
    "        outputs = device['net'](inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        device['optimizer'].step()\n",
    "        train_loss += loss.item()\n",
    "        device['train_loss_tracker'].append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        dev_id = device['id']\n",
    "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "                         f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    device['train_acc_tracker'].append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(epoch, device, criterion):\n",
    "    device['net'].eval()\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "            outputs = device['net'](inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            device['test_loss_tracker'].append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()  \n",
    "    acc = 100.*correct/total\n",
    "    device['test_acc_tracker'].append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1682870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function tests (the main experiment routine)\n",
    "\n",
    "# Given two different sets of aggregated weights, \n",
    "# gives a value representing the difference between them,\n",
    "# as a way to measure the direct cost of using our aggregated function\n",
    "def diff_aggregated_weights(strat, baseline):\n",
    "    result = 0 \n",
    "    for k in strat.keys():\n",
    "        result += torch.linalg.norm(strat[k] - baseline[k])\n",
    "    return result\n",
    "\n",
    "# A data type for experiment results\n",
    "BackdoorResult = namedtuple(\"BackdoorResult\", [\"scheme_loss\", \"test_accuracy\", \"backdoor_success\", \"devices\", \"avg_weight_history\"])\n",
    "\n",
    "# The main routine! \n",
    "def run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),  # Pass in aggregation function, \n",
    "                                                  # Device list -> aggregated weights \n",
    "                       rounds = 10,               # Rounds of FL\n",
    "                       local_epochs = 4,          # Epochs per device                      \n",
    "                       num_devices = 50,          # Total # devices\n",
    "                       device_pct = 0.1,          # % of devices per round\n",
    "                       data_pct = 0.1,            # % of data each device gets\n",
    "                       net = ConvNet().to(mps),   # Net design; make sure on mps\n",
    "                       evil_round = None,         # If None, no attack; else attacker will mount attack on this round\n",
    "                       attacker_strategy = None,  # device --> void --- set up the local weights on the attacker\n",
    "                       evil_device_id = None,     # Which device attacks? \n",
    "                       evaluate_attack = None,    # Function devices --> <any> measuring how well the attack worked at the end\n",
    "                       output_filename = None):   \n",
    "                                                 \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "    # Part 1.3: Implement device creation here\n",
    "    devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)] # Implement this!\n",
    "    \n",
    "    scheme_loss = []\n",
    "    max_magnitudes = []\n",
    "    avg_weight_history = []\n",
    "\n",
    "    ## IID Federated Learning\n",
    "    start_time = time.time()\n",
    "    for round_num in range(rounds):\n",
    "        # Part 1.3: Implement getting devices for each round here\n",
    "        round_devices = get_devices_for_round(devices, device_pct)\n",
    "\n",
    "        print('Round: ', round_num)\n",
    "        # Train locally \n",
    "        for device in round_devices:\n",
    "            for local_epoch in range(local_epochs):\n",
    "                train(local_epoch, device, criterion)\n",
    "        \n",
    "        # One device becomes evil if required\n",
    "        if (evil_round and round_num == evil_round):\n",
    "            assert (evil_device_id is not None)\n",
    "            assert (attacker_strategy is not None)\n",
    "            print(\"Attacking!\\n\")\n",
    "            \n",
    "            attacker_strategy(devices[evil_device_id])\n",
    "            # Make sure evil guy gets averaged in \n",
    "            if evil_device_id not in round_devices:\n",
    "                round_devices.append(devices[evil_device_id])\n",
    "            \n",
    "        \n",
    "        # Weight averaging\n",
    "        w_baseline, max_magnitude = average_weights(round_devices)\n",
    "        w_avg = agg_fn(round_devices)\n",
    "        max_magnitudes.append(max_magnitude)\n",
    "        \n",
    "        # Track the difference between the two; should be 0 if straight average\n",
    "        scheme_loss.append((float(diff_aggregated_weights(w_avg, w_baseline))))\n",
    "        \n",
    "        avg_weight_history.append(copy.deepcopy(w_avg))\n",
    "        \n",
    "        # Gradients         \n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(w_avg)\n",
    "            device['optimizer'].zero_grad()\n",
    "            device['optimizer'].step()\n",
    "            device['scheduler'].step()\n",
    "\n",
    "        # test accuracy after aggregation\n",
    "        # device 0 is the unique device with all of the \n",
    "        # test accuracies and losses in its tracker\n",
    "        test(round_num, devices[0], criterion)\n",
    "        \n",
    "        print(f\"\\nDiff: {scheme_loss[-1]}\\n\")\n",
    "\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print('Total training time: {} seconds'.format(total_time))\n",
    "    \n",
    "    \n",
    "    def lighten_device(d):\n",
    "        return {\n",
    "            k: d[k] for k in ( 'id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker')\n",
    "        }\n",
    "    \n",
    "    # Pack up everything we care about and the devices for good measure\n",
    "    result = BackdoorResult(\n",
    "        scheme_loss = scheme_loss, \n",
    "        test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "        backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "        devices = [lighten_device(d) for d in devices], \n",
    "        avg_weight_history = avg_weight_history\n",
    "    )\n",
    "\n",
    "    \n",
    "    if output_filename is not None: \n",
    "        with open(output_filename, 'wb') as file: \n",
    "            pickle.dump(result, file)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Load output files back into memory\n",
    "def load_result(filename):\n",
    "    with open(filename, 'rb') as file: \n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ecef1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 7/Epoch 0) Train Loss: 2.036 | Train Acc: 22.720 | Test Loss: 2.492 | Test Acc: 22.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  1\n",
      "(Device 7/Epoch 0) Train Loss: 1.935 | Train Acc: 27.660 | Test Loss: 2.054 | Test Acc: 27.370\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Total training time: 22.866957902908325 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "results = run_federated_test(local_epochs=1, num_devices = 10, rounds = 2, output_filename = \"testout.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad7670b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22.07, 27.37]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also load it back\n",
    "results = load_result(\"testout.pickle\")\n",
    "results.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "96f2e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A silly attack that just sends massive weights all of magnitude 10000\n",
    "def sample_attack(device):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    weights = device[\"net\"].state_dict().copy()\n",
    "    \n",
    "    for w in weights.keys():\n",
    "        weights[w] = torch.full(weights[w].size(), 10000)\n",
    "    \n",
    "    device['net'].load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1841cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function that squashes any weights with magnitude over 1000 (except on the first device but whatever)\n",
    "def super_smart_aggregation(devices):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights, excluding huge updates.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    \n",
    "    # initialize w_avg to all 0s\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            if (torch.max(state_dicts[i][k].type(torch.float32)) <= 1000): \n",
    "                w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "\n",
    "        return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "729c2d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 4/Epoch 0) Train Loss: 2.034 | Train Acc: 23.760 | Test Loss: 2.223 | Test Acc: 17.630\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  1\n",
      "(Device 9/Epoch 0) Train Loss: 1.864 | Train Acc: 28.560Attacking!\n",
      "\n",
      " | Test Loss: nan | Test Acc: 9.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  2\n",
      "(Device 0/Epoch 0) Train Loss: inf | Train Acc: 10.420 | Test Loss: 2.430 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Total training time: 60.91644906997681 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example of how this works\n",
    "\n",
    "# Here, we carry out the attack but use the ordinary average\n",
    "results_straight_avg = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline_trivial_attack.pickle\")  \n",
    "# Observe that it absolutely destroys our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d9aceed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 7/Epoch 0) Train Loss: 2.018 | Train Acc: 23.180 | Test Loss: 18.179 | Test Acc: 12.360\n",
      "\n",
      "Diff: 84.85118865966797\n",
      "\n",
      "Round:  1\n",
      "(Device 9/Epoch 0) Train Loss: 1.873 | Train Acc: 29.680Attacking!\n",
      "\n",
      " | Test Loss: 3.511 | Test Acc: 17.340\n",
      "\n",
      "Diff: 10114592.0\n",
      "\n",
      "Round:  2\n",
      "(Device 2/Epoch 0) Train Loss: 1.789 | Train Acc: 32.900 | Test Loss: 2.722 | Test Acc: 23.900\n",
      "\n",
      "Diff: 46.39040756225586\n",
      "\n",
      "Total training time: 56.98331904411316 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now we do it again but with the aggregation that rejects huge updates\n",
    "results_rejec_huge = run_federated_test(agg_fn = super_smart_aggregation,                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None,\n",
    "                                         output_filename = \"simple_attack_and_defense.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d7cac83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17f0bf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "aa47ed2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7e2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "48ccbdf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 2/Epoch 3) Train Loss: 1.727 | Train Acc: 34.8200 | Test Loss: 2.821 | Test Acc: 10.200\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  1\n",
      "(Device 24/Epoch 3) Train Loss: 1.562 | Train Acc: 41.420 | Test Loss: 1.498 | Test Acc: 44.420\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  2\n",
      "(Device 21/Epoch 3) Train Loss: 1.314 | Train Acc: 50.700 | Test Loss: 1.242 | Test Acc: 54.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  3\n",
      "(Device 9/Epoch 3) Train Loss: 1.142 | Train Acc: 59.3200 | Test Loss: 1.086 | Test Acc: 60.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  4\n",
      "(Device 15/Epoch 3) Train Loss: 1.108 | Train Acc: 59.800 | Test Loss: 0.965 | Test Acc: 65.690\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  5\n",
      "(Device 1/Epoch 3) Train Loss: 1.012 | Train Acc: 63.1400 | Test Loss: 0.869 | Test Acc: 69.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  6\n",
      "(Device 28/Epoch 3) Train Loss: 0.883 | Train Acc: 68.640 | Test Loss: 0.815 | Test Acc: 71.450\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  7\n",
      "(Device 10/Epoch 3) Train Loss: 0.804 | Train Acc: 72.000 | Test Loss: 0.757 | Test Acc: 73.380\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  8\n",
      "(Device 41/Epoch 3) Train Loss: 0.766 | Train Acc: 73.160 | Test Loss: 0.683 | Test Acc: 76.610\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  9\n",
      "(Device 16/Epoch 3) Train Loss: 0.682 | Train Acc: 75.940 | Test Loss: 0.648 | Test Acc: 77.530\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  10\n",
      "(Device 24/Epoch 3) Train Loss: 0.643 | Train Acc: 77.680 | Test Loss: 0.602 | Test Acc: 79.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  11\n",
      "(Device 27/Epoch 3) Train Loss: 0.578 | Train Acc: 79.520 | Test Loss: 0.647 | Test Acc: 77.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  12\n",
      "(Device 27/Epoch 3) Train Loss: 0.583 | Train Acc: 79.420 | Test Loss: 0.595 | Test Acc: 79.950\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  13\n",
      "(Device 23/Epoch 3) Train Loss: 0.551 | Train Acc: 80.640 | Test Loss: 0.548 | Test Acc: 81.380\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  14\n",
      "(Device 24/Epoch 3) Train Loss: 0.537 | Train Acc: 81.080 | Test Loss: 0.521 | Test Acc: 82.400\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  15\n",
      "(Device 13/Epoch 3) Train Loss: 0.447 | Train Acc: 84.500 | Test Loss: 0.510 | Test Acc: 82.860\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  16\n",
      "(Device 41/Epoch 3) Train Loss: 0.539 | Train Acc: 80.840 | Test Loss: 0.504 | Test Acc: 83.430\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  17\n",
      "(Device 41/Epoch 3) Train Loss: 0.435 | Train Acc: 85.920 | Test Loss: 0.505 | Test Acc: 83.600\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  18\n",
      "(Device 36/Epoch 3) Train Loss: 0.414 | Train Acc: 85.340 | Test Loss: 0.469 | Test Acc: 84.710\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  19\n",
      "(Device 19/Epoch 3) Train Loss: 0.429 | Train Acc: 85.720 | Test Loss: 0.461 | Test Acc: 85.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  20\n",
      "(Device 15/Epoch 3) Train Loss: 0.356 | Train Acc: 87.620 | Test Loss: 0.449 | Test Acc: 85.460\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  21\n",
      "(Device 12/Epoch 3) Train Loss: 0.379 | Train Acc: 86.120 | Test Loss: 0.431 | Test Acc: 85.900\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  22\n",
      "(Device 28/Epoch 3) Train Loss: 0.382 | Train Acc: 87.000 | Test Loss: 0.442 | Test Acc: 85.790\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  23\n",
      "(Device 20/Epoch 3) Train Loss: 0.321 | Train Acc: 88.840 | Test Loss: 0.413 | Test Acc: 86.540\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  24\n",
      "(Device 15/Epoch 3) Train Loss: 0.270 | Train Acc: 90.920 | Test Loss: 0.419 | Test Acc: 86.480\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  25\n",
      "(Device 39/Epoch 3) Train Loss: 0.236 | Train Acc: 91.420 | Test Loss: 0.391 | Test Acc: 87.190\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  26\n",
      "(Device 36/Epoch 3) Train Loss: 0.245 | Train Acc: 92.040 | Test Loss: 0.390 | Test Acc: 87.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  27\n",
      "(Device 19/Epoch 3) Train Loss: 0.221 | Train Acc: 92.440 | Test Loss: 0.388 | Test Acc: 87.520\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  28\n",
      "(Device 19/Epoch 3) Train Loss: 0.235 | Train Acc: 92.680 | Test Loss: 0.386 | Test Acc: 87.520\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  29\n",
      "(Device 30/Epoch 3) Train Loss: 0.221 | Train Acc: 92.260 | Test Loss: 0.387 | Test Acc: 87.370\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  30\n",
      "(Device 26/Epoch 3) Train Loss: 0.221 | Train Acc: 92.580 | Test Loss: 0.386 | Test Acc: 87.480\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  31\n",
      "(Device 49/Epoch 3) Train Loss: 0.195 | Train Acc: 93.360 | Test Loss: 0.386 | Test Acc: 87.650\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  32\n",
      "(Device 34/Epoch 3) Train Loss: 0.216 | Train Acc: 93.480 | Test Loss: 0.389 | Test Acc: 87.620\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  33\n",
      "(Device 7/Epoch 3) Train Loss: 0.246 | Train Acc: 91.8800 | Test Loss: 0.384 | Test Acc: 87.560\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  34\n",
      "(Device 38/Epoch 3) Train Loss: 0.186 | Train Acc: 94.140 | Test Loss: 0.385 | Test Acc: 87.580\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  35\n",
      "(Device 44/Epoch 3) Train Loss: 0.189 | Train Acc: 93.860 | Test Loss: 0.387 | Test Acc: 87.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  36\n",
      "(Device 25/Epoch 3) Train Loss: 0.210 | Train Acc: 93.400 | Test Loss: 0.387 | Test Acc: 87.630\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  37\n",
      "(Device 38/Epoch 3) Train Loss: 0.184 | Train Acc: 94.600 | Test Loss: 0.389 | Test Acc: 87.620\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  38\n",
      "(Device 10/Epoch 3) Train Loss: 0.209 | Train Acc: 92.800 | Test Loss: 0.388 | Test Acc: 87.800\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  39\n",
      "(Device 47/Epoch 3) Train Loss: 0.180 | Train Acc: 93.820 | Test Loss: 0.388 | Test Acc: 87.760\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  40\n",
      "(Device 2/Epoch 3) Train Loss: 0.192 | Train Acc: 93.3200 | Test Loss: 0.390 | Test Acc: 87.920\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  41\n",
      "(Device 11/Epoch 3) Train Loss: 0.198 | Train Acc: 93.520 | Test Loss: 0.388 | Test Acc: 87.820\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  42\n",
      "(Device 11/Epoch 3) Train Loss: 0.218 | Train Acc: 93.460 | Test Loss: 0.391 | Test Acc: 87.870\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  43\n",
      "(Device 48/Epoch 3) Train Loss: 0.186 | Train Acc: 94.340 | Test Loss: 0.389 | Test Acc: 87.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  44\n",
      "(Device 28/Epoch 3) Train Loss: 0.175 | Train Acc: 93.680 | Test Loss: 0.390 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  45\n",
      "(Device 26/Epoch 3) Train Loss: 0.187 | Train Acc: 93.960 | Test Loss: 0.391 | Test Acc: 87.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  46\n",
      "(Device 29/Epoch 3) Train Loss: 0.171 | Train Acc: 93.880 | Test Loss: 0.388 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  47\n",
      "(Device 34/Epoch 3) Train Loss: 0.164 | Train Acc: 94.000 | Test Loss: 0.393 | Test Acc: 87.850\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  48\n",
      "(Device 40/Epoch 3) Train Loss: 0.191 | Train Acc: 94.680 | Test Loss: 0.393 | Test Acc: 87.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  49\n",
      "(Device 30/Epoch 3) Train Loss: 0.162 | Train Acc: 94.740 | Test Loss: 0.392 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  50\n",
      "(Device 48/Epoch 3) Train Loss: 0.177 | Train Acc: 93.800 | Test Loss: 0.391 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  51\n",
      "(Device 9/Epoch 3) Train Loss: 0.173 | Train Acc: 94.1000 | Test Loss: 0.390 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  52\n",
      "(Device 24/Epoch 3) Train Loss: 0.175 | Train Acc: 93.900 | Test Loss: 0.390 | Test Acc: 88.010\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  53\n",
      "(Device 49/Epoch 3) Train Loss: 0.174 | Train Acc: 94.080 | Test Loss: 0.390 | Test Acc: 87.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  54\n",
      "(Device 7/Epoch 3) Train Loss: 0.185 | Train Acc: 93.3800 | Test Loss: 0.389 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  55\n",
      "(Device 28/Epoch 3) Train Loss: 0.190 | Train Acc: 94.320 | Test Loss: 0.389 | Test Acc: 88.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  56\n",
      "(Device 37/Epoch 3) Train Loss: 0.165 | Train Acc: 94.220 | Test Loss: 0.388 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  57\n",
      "(Device 17/Epoch 3) Train Loss: 0.199 | Train Acc: 93.800 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  58\n",
      "(Device 37/Epoch 3) Train Loss: 0.170 | Train Acc: 94.200 | Test Loss: 0.387 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  59\n",
      "(Device 18/Epoch 3) Train Loss: 0.162 | Train Acc: 94.760 | Test Loss: 0.387 | Test Acc: 88.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  60\n",
      "(Device 7/Epoch 3) Train Loss: 0.184 | Train Acc: 93.3600 | Test Loss: 0.388 | Test Acc: 88.170\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  61\n",
      "(Device 40/Epoch 3) Train Loss: 0.170 | Train Acc: 94.060 | Test Loss: 0.387 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  62\n",
      "(Device 30/Epoch 3) Train Loss: 0.173 | Train Acc: 94.480 | Test Loss: 0.389 | Test Acc: 88.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  63\n",
      "(Device 18/Epoch 3) Train Loss: 0.164 | Train Acc: 94.420 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  64\n",
      "(Device 23/Epoch 3) Train Loss: 0.166 | Train Acc: 94.000 | Test Loss: 0.388 | Test Acc: 88.080\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  65\n",
      "(Device 46/Epoch 3) Train Loss: 0.162 | Train Acc: 94.620 | Test Loss: 0.388 | Test Acc: 87.990\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  66\n",
      "(Device 22/Epoch 3) Train Loss: 0.159 | Train Acc: 94.440 | Test Loss: 0.389 | Test Acc: 88.100\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  67\n",
      "(Device 27/Epoch 3) Train Loss: 0.180 | Train Acc: 95.020 | Test Loss: 0.387 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  68\n",
      "(Device 25/Epoch 3) Train Loss: 0.182 | Train Acc: 93.740 | Test Loss: 0.387 | Test Acc: 88.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Device 11/Epoch 3) Train Loss: 0.174 | Train Acc: 94.300 | Test Loss: 0.386 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  70\n",
      "(Device 0/Epoch 3) Train Loss: 0.190 | Train Acc: 94.0800 | Test Loss: 0.386 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  71\n",
      "(Device 31/Epoch 3) Train Loss: 0.166 | Train Acc: 94.200 | Test Loss: 0.386 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  72\n",
      "(Device 44/Epoch 3) Train Loss: 0.162 | Train Acc: 94.800 | Test Loss: 0.387 | Test Acc: 88.080\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  73\n",
      "(Device 34/Epoch 3) Train Loss: 0.168 | Train Acc: 94.660 | Test Loss: 0.387 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  74\n",
      "(Device 2/Epoch 3) Train Loss: 0.161 | Train Acc: 94.5200 | Test Loss: 0.388 | Test Acc: 88.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  75\n",
      "(Device 0/Epoch 3) Train Loss: 0.175 | Train Acc: 93.9600 | Test Loss: 0.389 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  76\n",
      "(Device 41/Epoch 3) Train Loss: 0.215 | Train Acc: 93.940 | Test Loss: 0.388 | Test Acc: 88.060\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  77\n",
      "(Device 3/Epoch 3) Train Loss: 0.178 | Train Acc: 93.8800 | Test Loss: 0.387 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  78\n",
      "(Device 26/Epoch 3) Train Loss: 0.178 | Train Acc: 93.560 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  79\n",
      "(Device 10/Epoch 3) Train Loss: 0.187 | Train Acc: 93.520 | Test Loss: 0.388 | Test Acc: 88.150\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  80\n",
      "(Device 47/Epoch 3) Train Loss: 0.172 | Train Acc: 94.240 | Test Loss: 0.388 | Test Acc: 88.100\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  81\n",
      "(Device 19/Epoch 3) Train Loss: 0.183 | Train Acc: 93.860 | Test Loss: 0.388 | Test Acc: 88.060\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  82\n",
      "(Device 8/Epoch 3) Train Loss: 0.186 | Train Acc: 94.3200 | Test Loss: 0.388 | Test Acc: 88.160\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  83\n",
      "(Device 4/Epoch 3) Train Loss: 0.180 | Train Acc: 93.7200 | Test Loss: 0.387 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  84\n",
      "(Device 27/Epoch 3) Train Loss: 0.173 | Train Acc: 93.840 | Test Loss: 0.387 | Test Acc: 88.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  85\n",
      "(Device 25/Epoch 3) Train Loss: 0.189 | Train Acc: 93.460 | Test Loss: 0.387 | Test Acc: 88.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  86\n",
      "(Device 32/Epoch 3) Train Loss: 0.203 | Train Acc: 94.000 | Test Loss: 0.387 | Test Acc: 88.180\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  87\n",
      "(Device 16/Epoch 3) Train Loss: 0.181 | Train Acc: 93.780 | Test Loss: 0.387 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  88\n",
      "(Device 15/Epoch 3) Train Loss: 0.174 | Train Acc: 94.200 | Test Loss: 0.388 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  89\n",
      "(Device 35/Epoch 3) Train Loss: 0.169 | Train Acc: 94.280 | Test Loss: 0.388 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  90\n",
      "(Device 37/Epoch 3) Train Loss: 0.176 | Train Acc: 94.000 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  91\n",
      "(Device 33/Epoch 3) Train Loss: 0.174 | Train Acc: 94.080 | Test Loss: 0.387 | Test Acc: 88.080\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  92\n",
      "(Device 40/Epoch 3) Train Loss: 0.191 | Train Acc: 94.040 | Test Loss: 0.388 | Test Acc: 88.160\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  93\n",
      "(Device 35/Epoch 3) Train Loss: 0.174 | Train Acc: 93.920 | Test Loss: 0.388 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  94\n",
      "(Device 39/Epoch 3) Train Loss: 0.177 | Train Acc: 93.940 | Test Loss: 0.387 | Test Acc: 88.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  95\n",
      "(Device 9/Epoch 3) Train Loss: 0.174 | Train Acc: 93.9800 | Test Loss: 0.388 | Test Acc: 88.220\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  96\n",
      "(Device 5/Epoch 3) Train Loss: 0.170 | Train Acc: 93.7600 | Test Loss: 0.388 | Test Acc: 88.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  97\n",
      "(Device 12/Epoch 3) Train Loss: 0.162 | Train Acc: 94.540 | Test Loss: 0.387 | Test Acc: 88.040\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  98\n",
      "(Device 27/Epoch 3) Train Loss: 0.176 | Train Acc: 94.460 | Test Loss: 0.388 | Test Acc: 88.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  99\n",
      "(Device 38/Epoch 3) Train Loss: 0.160 | Train Acc: 94.860 | Test Loss: 0.387 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Total training time: 28255.882014989853 seconds\n"
     ]
    }
   ],
   "source": [
    "# Here, we carry out the attack but use the ordinary average\n",
    "baseline = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 100,              \n",
    "                                         local_epochs = 4,                             \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b72ad410",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_keys = [x for x in (baseline.avg_weight_history[0]).keys() if \"weight\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2ca8c75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5328e-01,  2.9763e-02,  3.1214e-01],\n",
       "          [ 1.2357e-01,  1.0146e-01,  2.7768e-01],\n",
       "          [-3.0389e-01,  9.0462e-02,  2.1724e-01]],\n",
       "\n",
       "         [[-1.5164e-03, -9.8262e-02, -5.2802e-02],\n",
       "          [ 3.8939e-02, -1.4380e-01,  1.9608e-01],\n",
       "          [-3.7223e-01, -2.7624e-01, -1.7831e-01]],\n",
       "\n",
       "         [[-2.6124e-03, -1.4084e-01, -2.1483e-02],\n",
       "          [-8.3629e-02, -5.2726e-02,  1.7127e-01],\n",
       "          [-1.5641e-01,  2.9460e-02,  3.3760e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0776e-01,  1.0967e-01, -7.1621e-02],\n",
       "          [-2.9832e-01, -1.3297e-01,  1.9618e-01],\n",
       "          [-3.4720e-02, -4.4629e-02,  1.9179e-01]],\n",
       "\n",
       "         [[-1.5080e-01, -1.1805e-01,  1.0674e-02],\n",
       "          [-1.8825e-02, -2.0336e-01,  1.7431e-01],\n",
       "          [-1.2987e-01,  9.7447e-02,  1.6608e-01]],\n",
       "\n",
       "         [[-1.1846e-01, -7.1918e-02,  1.7128e-01],\n",
       "          [-2.0417e-01, -8.5783e-02,  1.9556e-01],\n",
       "          [-1.3691e-01,  1.1289e-02, -6.6839e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1679e-01, -1.6645e-01, -2.2742e-01],\n",
       "          [ 2.3235e-01, -1.2028e-01, -1.4871e-01],\n",
       "          [ 2.4921e-01,  5.2481e-02,  1.3819e-02]],\n",
       "\n",
       "         [[ 1.4913e-01, -5.1529e-02, -4.7539e-02],\n",
       "          [ 8.5971e-03, -4.0564e-02, -2.9865e-01],\n",
       "          [ 3.2093e-02,  1.3202e-01, -2.1330e-01]],\n",
       "\n",
       "         [[-3.5924e-02,  7.2685e-02,  1.3373e-01],\n",
       "          [-3.1253e-02, -1.1512e-01, -6.8010e-03],\n",
       "          [ 8.7607e-02,  5.8125e-02, -2.2128e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.3524e-01, -1.0645e-01, -2.6455e-02],\n",
       "          [-6.0006e-02,  5.0577e-02, -5.6926e-02],\n",
       "          [-8.5532e-02, -1.9025e-01, -2.5958e-01]],\n",
       "\n",
       "         [[-2.5286e-01,  3.1687e-02, -2.5988e-01],\n",
       "          [ 1.2112e-01,  2.2726e-01,  1.6109e-01],\n",
       "          [-1.3908e-01, -1.4126e-01, -9.0740e-02]],\n",
       "\n",
       "         [[ 2.1888e-01,  1.9918e-01,  1.0661e-01],\n",
       "          [ 7.8533e-02,  3.2382e-01,  3.2301e-01],\n",
       "          [ 1.5675e-01,  1.7416e-01,  1.5582e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2736e-01,  1.8886e-01,  1.2682e-02],\n",
       "          [-4.3529e-02, -1.4757e-01, -1.3037e-01],\n",
       "          [-1.6453e-01,  9.1173e-02, -8.8410e-02]],\n",
       "\n",
       "         [[ 2.6297e-01,  1.7829e-01,  1.9820e-01],\n",
       "          [-1.7723e-01, -1.6393e-01, -7.2570e-03],\n",
       "          [-1.1585e-01, -2.6512e-01,  8.8679e-02]],\n",
       "\n",
       "         [[ 5.8229e-02, -9.2996e-02,  1.5733e-01],\n",
       "          [-7.2921e-02, -1.2557e-02, -3.8554e-02],\n",
       "          [-6.1138e-02, -1.6129e-01,  1.2136e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.2345e-01, -5.5384e-02,  1.7063e-02],\n",
       "          [-1.8399e-01, -2.4555e-01, -2.9117e-03],\n",
       "          [-8.8645e-02, -9.0196e-02, -4.0833e-02]],\n",
       "\n",
       "         [[-2.9525e-02,  1.5374e-01,  2.6513e-01],\n",
       "          [-8.6706e-02, -1.2559e-02, -1.9519e-01],\n",
       "          [-1.9396e-01, -4.4049e-02, -1.4591e-01]],\n",
       "\n",
       "         [[ 2.5511e-01,  1.9303e-01,  1.5516e-01],\n",
       "          [-1.3469e-01,  1.5602e-02,  8.8091e-02],\n",
       "          [ 1.0710e-01, -6.1944e-02,  5.1955e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.6503e-02,  4.2601e-02,  5.9017e-02],\n",
       "          [-1.8163e-01, -2.2731e-02, -2.9339e-01],\n",
       "          [-2.1861e-01, -5.4246e-02, -1.7861e-01]],\n",
       "\n",
       "         [[-8.2927e-02,  2.1757e-01,  6.4353e-03],\n",
       "          [-1.0980e-01, -1.8521e-01, -3.8957e-02],\n",
       "          [-2.3309e-02, -1.2842e-01, -6.8925e-02]],\n",
       "\n",
       "         [[ 3.3393e-01,  3.6396e-01,  3.6658e-01],\n",
       "          [ 2.5058e-01,  2.7366e-01,  2.1887e-01],\n",
       "          [-2.1058e-02,  9.4961e-02,  2.4312e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4967e-02,  7.0752e-02, -1.6986e-01],\n",
       "          [ 8.6079e-02,  4.3243e-02,  1.2236e-01],\n",
       "          [-3.9894e-02, -1.4241e-01,  1.4537e-01]],\n",
       "\n",
       "         [[-1.5528e-01, -7.0833e-02,  1.3082e-01],\n",
       "          [-3.8969e-02, -1.1714e-01, -3.8385e-02],\n",
       "          [-1.5083e-01, -1.2025e-01,  1.6488e-01]],\n",
       "\n",
       "         [[-1.7756e-01,  2.9651e-03,  2.3526e-03],\n",
       "          [-2.7102e-02, -1.0708e-01,  7.1655e-02],\n",
       "          [-2.2752e-01,  7.9716e-02,  7.9813e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.0307e-01,  1.1052e-01,  9.0248e-02],\n",
       "          [ 2.8683e-01, -5.1961e-02, -1.5521e-01],\n",
       "          [-7.8957e-02, -1.7841e-01,  3.7347e-02]],\n",
       "\n",
       "         [[ 1.3443e-01,  1.7207e-02, -8.4802e-02],\n",
       "          [ 1.7254e-01, -3.2451e-01, -3.3575e-01],\n",
       "          [-4.6931e-02, -2.6773e-01, -2.8061e-01]],\n",
       "\n",
       "         [[ 2.6702e-01,  1.7299e-01, -1.0167e-01],\n",
       "          [-8.3306e-02, -2.3237e-01, -2.2080e-01],\n",
       "          [ 2.3110e-01, -1.8459e-01,  1.2419e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.9974e-01,  2.8723e-01,  1.7745e-01],\n",
       "          [ 2.6540e-02,  5.9079e-02, -7.5981e-02],\n",
       "          [-2.1324e-01, -1.4460e-01, -3.2366e-01]],\n",
       "\n",
       "         [[ 2.0942e-01, -1.7779e-02,  1.8440e-01],\n",
       "          [ 2.2180e-02, -1.4088e-01, -5.1183e-02],\n",
       "          [-4.5919e-02, -3.2582e-01, -1.4136e-01]],\n",
       "\n",
       "         [[ 2.0240e-01, -6.9679e-03, -6.9382e-02],\n",
       "          [-1.0056e-01,  1.7585e-01,  5.2719e-02],\n",
       "          [-1.4118e-01,  1.3384e-01, -1.9103e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.5733e-01,  2.9215e-02, -4.4286e-02],\n",
       "          [ 3.5132e-02, -2.4030e-01, -1.2839e-01],\n",
       "          [ 1.0153e-01,  1.9511e-01, -1.4879e-01]],\n",
       "\n",
       "         [[ 9.4498e-02, -5.3762e-03, -1.1677e-01],\n",
       "          [ 5.1438e-02, -1.8636e-01, -1.6034e-01],\n",
       "          [-5.2159e-02,  2.1791e-01,  8.2106e-02]],\n",
       "\n",
       "         [[ 1.2267e-01, -2.7688e-02,  3.0136e-02],\n",
       "          [-1.6338e-01, -1.4375e-01, -1.9347e-01],\n",
       "          [-4.3575e-02, -1.2439e-01,  5.0799e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 4.3499e-02, -2.7965e-02,  1.2497e-01],\n",
       "          [-3.4936e-02, -9.6115e-03, -1.7990e-01],\n",
       "          [ 1.4013e-01,  1.6002e-01, -2.0852e-01]],\n",
       "\n",
       "         [[ 4.3783e-02, -1.3274e-01,  1.8404e-01],\n",
       "          [-5.2783e-02,  1.8230e-01, -4.3566e-02],\n",
       "          [ 8.3507e-02, -6.8685e-03,  2.1836e-02]],\n",
       "\n",
       "         [[-7.0859e-02,  1.1829e-01,  8.0128e-02],\n",
       "          [-1.0190e-01,  1.7961e-01,  1.3768e-01],\n",
       "          [ 8.2728e-02, -8.7342e-02, -2.1950e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.6767e-02, -2.8287e-01, -2.8484e-02],\n",
       "          [-8.2903e-02, -1.0881e-01, -2.3766e-01],\n",
       "          [ 1.0564e-01, -2.7908e-02,  1.7648e-01]],\n",
       "\n",
       "         [[-1.4703e-01, -1.5996e-02, -1.8155e-01],\n",
       "          [-1.2916e-01, -9.7264e-02, -1.4931e-01],\n",
       "          [ 2.7672e-01,  1.5115e-03,  1.3625e-01]],\n",
       "\n",
       "         [[-1.0387e-01, -2.3001e-01, -2.7541e-02],\n",
       "          [ 7.7437e-02, -7.2554e-02, -1.3883e-01],\n",
       "          [ 3.0390e-01,  2.2532e-01, -1.7371e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.4750e-01, -5.5101e-02,  9.5638e-02],\n",
       "          [-2.3483e-01, -1.4090e-01,  1.6681e-01],\n",
       "          [-4.5331e-03,  1.4897e-01,  1.4619e-01]],\n",
       "\n",
       "         [[-2.6127e-02,  1.2172e-01, -1.5549e-01],\n",
       "          [-1.2677e-01, -6.8530e-02, -1.4733e-01],\n",
       "          [-1.2821e-01, -3.4931e-02, -1.7258e-01]],\n",
       "\n",
       "         [[ 1.2450e-01,  6.4519e-02,  2.3928e-01],\n",
       "          [-9.8021e-02, -2.4614e-02,  2.0011e-01],\n",
       "          [-2.3030e-01,  1.1242e-01,  2.4127e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.3235e-01, -4.3085e-02, -7.3930e-03],\n",
       "          [-9.7571e-04,  1.7830e-01,  9.4101e-02],\n",
       "          [ 1.7235e-02,  3.0569e-02, -9.0616e-02]],\n",
       "\n",
       "         [[-2.0398e-01,  7.3900e-02, -2.4132e-01],\n",
       "          [ 6.7051e-02, -1.9999e-01, -1.4748e-01],\n",
       "          [-3.7248e-02,  1.5478e-01, -7.5151e-02]],\n",
       "\n",
       "         [[ 1.3329e-01,  1.4205e-01,  7.3101e-02],\n",
       "          [ 2.3310e-01,  2.4037e-01,  2.8996e-02],\n",
       "          [ 2.2911e-02,  9.3707e-02,  6.5934e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1775e-01, -8.0327e-02,  9.2182e-02],\n",
       "          [ 5.0109e-03,  2.1194e-01,  1.1367e-01],\n",
       "          [ 1.6539e-01,  1.8505e-01,  4.0211e-02]],\n",
       "\n",
       "         [[-1.8838e-02, -1.9242e-01, -6.3451e-02],\n",
       "          [-5.3875e-03,  1.7618e-01,  1.8163e-01],\n",
       "          [ 5.6514e-02, -2.7529e-02,  4.5675e-02]],\n",
       "\n",
       "         [[-1.6648e-01, -4.7277e-02,  3.4968e-02],\n",
       "          [ 3.3294e-02,  1.9591e-01, -9.8058e-02],\n",
       "          [ 3.7038e-02,  1.4897e-01,  2.5089e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.2966e-01, -2.1374e-01, -8.6523e-02],\n",
       "          [ 1.1104e-01, -7.3526e-02,  5.9149e-02],\n",
       "          [-2.0333e-01,  3.4246e-02, -4.8673e-03]],\n",
       "\n",
       "         [[-1.0829e-01,  3.7841e-02, -1.6666e-01],\n",
       "          [ 8.9270e-02,  1.8303e-01,  9.4935e-02],\n",
       "          [-1.6157e-01, -2.0948e-01, -7.5835e-02]],\n",
       "\n",
       "         [[ 1.4399e-01,  1.7147e-01,  1.3404e-01],\n",
       "          [ 6.3900e-02,  1.6143e-01,  9.9619e-02],\n",
       "          [ 5.2282e-02, -1.4907e-02, -3.5659e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.5967e-01, -2.9727e-02, -1.3513e-01],\n",
       "          [-1.1006e-01,  8.7037e-02, -1.3794e-01],\n",
       "          [ 2.3920e-01,  8.7388e-02,  9.2748e-02]],\n",
       "\n",
       "         [[-7.4858e-02,  8.6947e-02, -7.6260e-02],\n",
       "          [ 1.9178e-01, -9.5573e-02,  1.2839e-02],\n",
       "          [-1.1262e-01,  2.0152e-01,  1.6994e-01]],\n",
       "\n",
       "         [[ 1.9262e-01, -1.4653e-01, -3.3971e-03],\n",
       "          [-1.1380e-02,  4.7810e-02,  6.3720e-02],\n",
       "          [ 2.6219e-01, -2.2729e-02,  2.1798e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2492e-01, -1.2714e-01, -2.2418e-01],\n",
       "          [-1.3487e-01, -1.4213e-01,  4.9029e-02],\n",
       "          [-8.5599e-02,  5.7369e-03,  4.1141e-02]],\n",
       "\n",
       "         [[ 4.2661e-02,  8.1931e-02, -5.2056e-02],\n",
       "          [-8.6051e-03,  1.0782e-01, -9.8773e-02],\n",
       "          [-7.5122e-02,  9.6371e-02, -3.6868e-02]],\n",
       "\n",
       "         [[ 7.7156e-02, -1.0367e-01, -2.6437e-02],\n",
       "          [-2.0516e-01, -2.3638e-01,  3.7883e-02],\n",
       "          [-7.0777e-02, -1.7966e-02, -1.3364e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.7685e-01,  2.8119e-01,  4.9658e-01],\n",
       "          [ 2.2193e-01,  1.6874e-01,  2.2068e-01],\n",
       "          [-3.3583e-01, -3.8808e-01, -3.2115e-01]],\n",
       "\n",
       "         [[ 7.6504e-02,  4.8761e-02, -9.2411e-02],\n",
       "          [-6.2703e-03, -4.1817e-02, -1.4778e-02],\n",
       "          [-5.8709e-02, -1.5722e-01,  2.4986e-02]],\n",
       "\n",
       "         [[ 2.0311e-01, -1.4616e-01, -1.2995e-02],\n",
       "          [-2.2520e-02,  2.6678e-01,  1.4927e-01],\n",
       "          [ 2.0673e-01,  4.4327e-02, -6.9172e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.9679e-01,  3.1199e-02,  4.2497e-02],\n",
       "          [-1.2493e-01, -3.5112e-02,  5.6441e-02],\n",
       "          [-1.1127e-01, -2.0142e-02,  1.8740e-01]],\n",
       "\n",
       "         [[ 1.0470e-01,  6.6403e-02,  1.2143e-01],\n",
       "          [-6.4860e-02,  1.4321e-01,  2.8150e-01],\n",
       "          [ 9.9011e-02,  7.5021e-03,  1.0940e-01]],\n",
       "\n",
       "         [[-2.5684e-01,  6.3514e-02, -6.4199e-02],\n",
       "          [-1.0611e-01,  1.7580e-02, -1.3259e-01],\n",
       "          [-1.9351e-01, -2.1767e-02,  1.4965e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.5949e-02, -1.0897e-01, -2.2147e-02],\n",
       "          [ 1.3303e-01,  6.3724e-03,  8.5917e-02],\n",
       "          [ 7.3758e-02,  4.9811e-02,  1.2927e-02]],\n",
       "\n",
       "         [[ 6.0911e-02,  8.5646e-02, -4.6619e-02],\n",
       "          [ 1.3390e-01,  1.4741e-01,  1.2539e-02],\n",
       "          [ 1.4825e-01,  4.2985e-02, -5.6704e-02]],\n",
       "\n",
       "         [[-8.2802e-02,  1.3940e-01, -2.9830e-03],\n",
       "          [ 9.6595e-02,  4.0402e-02, -5.7536e-02],\n",
       "          [ 1.9577e-01,  1.0622e-01,  7.1164e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.0279e-01,  7.8667e-03,  5.6812e-02],\n",
       "          [ 1.4834e-01, -3.9563e-02, -4.8195e-04],\n",
       "          [ 1.5288e-01,  1.5183e-01,  1.2748e-01]],\n",
       "\n",
       "         [[ 1.7149e-01, -1.2755e-01, -1.1776e-01],\n",
       "          [-8.5509e-02,  1.9219e-01,  1.0829e-01],\n",
       "          [ 6.6582e-02,  1.5677e-01, -1.3594e-01]],\n",
       "\n",
       "         [[ 1.2045e-01,  1.2958e-01, -1.5870e-01],\n",
       "          [-8.4893e-03, -2.6510e-02, -4.3183e-02],\n",
       "          [ 1.2373e-01, -7.6414e-02, -1.6580e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8333e-01, -7.9680e-02,  1.2156e-01],\n",
       "          [-9.7131e-04,  7.8336e-02,  2.5593e-01],\n",
       "          [-2.8046e-02, -1.5336e-01, -8.7263e-02]],\n",
       "\n",
       "         [[ 2.1722e-01, -1.2348e-02, -3.2715e-03],\n",
       "          [ 9.2986e-02, -2.0430e-01, -1.5015e-01],\n",
       "          [-2.2983e-01, -1.0647e-01, -2.5845e-01]],\n",
       "\n",
       "         [[ 6.0060e-02,  2.0555e-01,  3.0630e-01],\n",
       "          [ 2.1945e-01,  1.1159e-01,  1.2248e-02],\n",
       "          [-2.2415e-01, -2.2266e-01, -8.9636e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.4339e-01, -1.7072e-02,  1.7925e-01],\n",
       "          [-4.3745e-02,  2.6842e-01,  8.3441e-02],\n",
       "          [ 4.8871e-03,  1.5665e-01, -1.1299e-01]],\n",
       "\n",
       "         [[-2.0988e-01, -7.7615e-02, -2.9884e-01],\n",
       "          [-6.5544e-02,  9.2217e-02, -2.6665e-01],\n",
       "          [-7.3578e-02,  2.0187e-01, -2.2180e-01]],\n",
       "\n",
       "         [[ 1.9030e-01,  7.5469e-02, -3.1882e-01],\n",
       "          [ 9.4617e-02,  8.8289e-02, -2.3271e-01],\n",
       "          [ 2.5159e-01,  1.3116e-01, -3.6339e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.0010e-01,  1.6261e-01, -1.2920e-01],\n",
       "          [-9.1415e-02,  8.8900e-03,  2.8869e-01],\n",
       "          [-2.0149e-01,  4.5119e-02,  2.1920e-01]],\n",
       "\n",
       "         [[-1.7506e-01, -1.1907e-01, -6.9458e-02],\n",
       "          [-2.0024e-02, -7.8944e-02,  2.0841e-01],\n",
       "          [-2.7408e-02,  1.2583e-01,  1.4814e-01]],\n",
       "\n",
       "         [[-2.6665e-01,  1.6088e-02,  1.8330e-01],\n",
       "          [-1.5365e-01,  6.2072e-02,  2.5249e-01],\n",
       "          [-1.9767e-01,  8.5333e-02,  5.0162e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.1764e-02,  1.9894e-01,  1.7839e-01],\n",
       "          [ 1.4972e-01,  1.1035e-01,  2.1016e-01],\n",
       "          [-3.4964e-01, -3.9238e-01, -3.1600e-01]],\n",
       "\n",
       "         [[ 3.3240e-02,  1.4959e-01, -3.3997e-02],\n",
       "          [ 1.6539e-01,  5.5446e-02,  1.4962e-01],\n",
       "          [-2.7171e-02, -1.9991e-01, -1.9633e-01]],\n",
       "\n",
       "         [[-1.2720e-01, -4.2814e-02, -9.0767e-02],\n",
       "          [ 2.5913e-01, -1.1733e-01,  1.4233e-01],\n",
       "          [-1.7472e-01, -3.6240e-02,  3.6392e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9779e-01, -6.0883e-03, -7.3564e-02],\n",
       "          [ 1.1266e-01,  9.0042e-02, -2.2821e-01],\n",
       "          [ 1.3996e-01,  2.8640e-02, -1.4510e-02]],\n",
       "\n",
       "         [[ 2.8473e-01,  1.2820e-01, -3.3498e-01],\n",
       "          [ 2.3131e-01, -7.0832e-02, -3.1536e-01],\n",
       "          [-5.0758e-02,  5.8797e-02, -2.9974e-01]],\n",
       "\n",
       "         [[ 9.0516e-02, -7.4974e-02, -2.0383e-01],\n",
       "          [ 2.7328e-01,  1.8087e-01, -5.2875e-02],\n",
       "          [ 2.3459e-01, -1.1591e-01, -1.5395e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0797e-01, -1.0770e-01,  2.6718e-01],\n",
       "          [ 1.8342e-01, -2.6094e-01,  6.0971e-02],\n",
       "          [-9.6397e-02, -1.8536e-01, -1.7400e-01]],\n",
       "\n",
       "         [[ 8.8247e-02,  5.5358e-02, -1.2865e-01],\n",
       "          [-1.2869e-01, -2.5292e-01,  3.0731e-02],\n",
       "          [-1.5692e-01, -1.8714e-01,  1.0907e-03]],\n",
       "\n",
       "         [[ 2.9786e-02, -1.6189e-01, -2.5733e-02],\n",
       "          [-3.7609e-02, -1.1998e-01, -1.3205e-02],\n",
       "          [ 2.5563e-01,  9.8271e-02,  6.1579e-03]]],\n",
       "\n",
       "\n",
       "        [[[-4.5557e-02, -1.3479e-01, -2.9592e-01],\n",
       "          [ 9.1810e-02,  1.3416e-01,  8.1938e-02],\n",
       "          [ 1.2148e-01,  3.5427e-02,  5.1794e-02]],\n",
       "\n",
       "         [[-2.3229e-01, -9.6319e-03, -2.4811e-01],\n",
       "          [-1.2192e-01,  1.2794e-02, -6.3507e-02],\n",
       "          [-3.2937e-02,  8.4384e-02, -7.2486e-02]],\n",
       "\n",
       "         [[ 1.6578e-01,  5.8554e-02,  3.2359e-02],\n",
       "          [-4.5859e-02,  1.0873e-01,  1.2577e-01],\n",
       "          [ 1.3225e-01,  1.2947e-01,  1.2321e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.2005e-01, -9.9913e-02, -9.2134e-02],\n",
       "          [ 2.7025e-02,  8.6407e-02, -2.4474e-01],\n",
       "          [ 2.3797e-01,  1.5713e-01, -7.3732e-02]],\n",
       "\n",
       "         [[-1.1213e-01, -1.0706e-01,  9.0059e-02],\n",
       "          [-3.7929e-02,  5.9463e-02,  4.4807e-02],\n",
       "          [-4.0252e-02, -4.6382e-02, -6.5350e-02]],\n",
       "\n",
       "         [[-1.7145e-02,  6.1471e-02, -2.4455e-01],\n",
       "          [ 2.5389e-01,  1.1647e-01, -7.6727e-02],\n",
       "          [ 2.3968e-01, -1.3461e-01, -1.9844e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.7843e-01, -1.8840e-01, -2.8469e-01],\n",
       "          [-1.6567e-01, -1.8494e-01, -9.9276e-02],\n",
       "          [-1.3818e-01,  1.7378e-01,  1.5821e-01]],\n",
       "\n",
       "         [[-2.6121e-01, -1.9396e-01, -1.1956e-01],\n",
       "          [ 1.0550e-01, -9.7612e-02, -1.0795e-03],\n",
       "          [ 7.6661e-02, -1.6651e-02, -7.2074e-02]],\n",
       "\n",
       "         [[ 6.6435e-03, -1.0378e-01, -5.2092e-02],\n",
       "          [ 2.1400e-01,  2.8551e-01,  2.6913e-01],\n",
       "          [ 1.7573e-01,  8.9348e-02,  2.5721e-02]]]], device='mps:0')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.avg_weight_history[0][weight_keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd1b4024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.0.0.weight', 'model.0.1.weight', 'model.0.1.bias', 'model.0.1.running_mean', 'model.0.1.running_var', 'model.0.1.num_batches_tracked', 'model.1.0.weight', 'model.1.1.weight', 'model.1.1.bias', 'model.1.1.running_mean', 'model.1.1.running_var', 'model.1.1.num_batches_tracked', 'model.2.0.weight', 'model.2.1.weight', 'model.2.1.bias', 'model.2.1.running_mean', 'model.2.1.running_var', 'model.2.1.num_batches_tracked', 'model.3.0.weight', 'model.3.1.weight', 'model.3.1.bias', 'model.3.1.running_mean', 'model.3.1.running_var', 'model.3.1.num_batches_tracked', 'model.4.0.weight', 'model.4.1.weight', 'model.4.1.bias', 'model.4.1.running_mean', 'model.4.1.running_var', 'model.4.1.num_batches_tracked', 'model.5.0.weight', 'model.5.1.weight', 'model.5.1.bias', 'model.5.1.running_mean', 'model.5.1.running_var', 'model.5.1.num_batches_tracked', 'model.6.0.weight', 'model.6.1.weight', 'model.6.1.bias', 'model.6.1.running_mean', 'model.6.1.running_var', 'model.6.1.num_batches_tracked', 'model.7.0.weight', 'model.7.1.weight', 'model.7.1.bias', 'model.7.1.running_mean', 'model.7.1.running_var', 'model.7.1.num_batches_tracked', 'model.8.0.weight', 'model.8.1.weight', 'model.8.1.bias', 'model.8.1.running_mean', 'model.8.1.running_var', 'model.8.1.num_batches_tracked', 'classifier.weight', 'classifier.bias'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(baseline.avg_weight_history[0]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb170485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(160., device='mps:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.avg_weight_history[0]['model.3.1.num_batches_tracked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "dcd586a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_history_to_layer_max_magnitude_and_means(w_avg, include_batch_norm = True):\n",
    "    weight_keys = [x for x in w_avg.keys() if \".0.weight\" in x]\n",
    "    if include_batch_norm:\n",
    "        weight_keys += [x for x in w_avg.keys() if \".1.weight\" in x]\n",
    "    def max_magnitude(t):\n",
    "        return torch.max(torch.abs(t))\n",
    "    def mean_magnitude(t):\n",
    "        return torch.mean(torch.abs(t))\n",
    "    all_means = [mean_magnitude(w_avg[w]) for w in weight_keys]\n",
    "    all_max = [max_magnitude(w_avg[w]) for w in weight_keys]\n",
    "    return all_means, all_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "68d85502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.1240, device='mps:0'),\n",
       "  tensor(0.0353, device='mps:0'),\n",
       "  tensor(0.0318, device='mps:0'),\n",
       "  tensor(0.0223, device='mps:0'),\n",
       "  tensor(0.0221, device='mps:0'),\n",
       "  tensor(0.0214, device='mps:0'),\n",
       "  tensor(0.0152, device='mps:0'),\n",
       "  tensor(0.0150, device='mps:0'),\n",
       "  tensor(0.0108, device='mps:0')],\n",
       " [tensor(0.4966, device='mps:0'),\n",
       "  tensor(0.1929, device='mps:0'),\n",
       "  tensor(0.1106, device='mps:0'),\n",
       "  tensor(0.0786, device='mps:0'),\n",
       "  tensor(0.0676, device='mps:0'),\n",
       "  tensor(0.0638, device='mps:0'),\n",
       "  tensor(0.0495, device='mps:0'),\n",
       "  tensor(0.0464, device='mps:0'),\n",
       "  tensor(0.0410, device='mps:0')])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight_history_to_layer_max_magnitude_and_means(baseline.avg_weight_history[0], include_batch_norm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "60ac686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mns, mxs = weight_history_to_layer_max_magnitude_and_means(baseline.avg_weight_history[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3f0b320c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.4966, device='mps:0'),\n",
       " tensor(1.1587, device='mps:0'),\n",
       " tensor(0.1929, device='mps:0'),\n",
       " tensor(1.1092, device='mps:0'),\n",
       " tensor(0.1106, device='mps:0'),\n",
       " tensor(1.0271, device='mps:0'),\n",
       " tensor(0.0786, device='mps:0'),\n",
       " tensor(1.0307, device='mps:0'),\n",
       " tensor(0.0676, device='mps:0'),\n",
       " tensor(1.0157, device='mps:0'),\n",
       " tensor(0.0638, device='mps:0'),\n",
       " tensor(1.0151, device='mps:0'),\n",
       " tensor(0.0495, device='mps:0'),\n",
       " tensor(1.0168, device='mps:0'),\n",
       " tensor(0.0464, device='mps:0'),\n",
       " tensor(1.0143, device='mps:0'),\n",
       " tensor(0.0410, device='mps:0'),\n",
       " tensor(0.9425, device='mps:0'),\n",
       " tensor(0.1981, device='mps:0')]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fbf9d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_history = [weight_history_to_layer_max_magnitude_and_means(baseline.avg_weight_history[t], include_batch_norm = False) for t in range(100)]\n",
    "max_history = [x[1] for x in max_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "622f7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_layer = [[max_history[k][l].cpu() for k in range(len(max_history))] for l in range(len(max_history[0]))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4d145f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-223-704ef11db8ed>:1: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"-k\" (-> color='k'). The keyword argument will take precedence.\n",
      "  [plt.plot([x for x in range(100)],by_layer[l],  '-k', color='blue') for l in range(9)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[<matplotlib.lines.Line2D at 0x495142d60>],\n",
       " [<matplotlib.lines.Line2D at 0x495142ee0>],\n",
       " [<matplotlib.lines.Line2D at 0x495142640>],\n",
       " [<matplotlib.lines.Line2D at 0x495142b20>],\n",
       " [<matplotlib.lines.Line2D at 0x49513a220>],\n",
       " [<matplotlib.lines.Line2D at 0x49513a8b0>],\n",
       " [<matplotlib.lines.Line2D at 0x49513a0a0>],\n",
       " [<matplotlib.lines.Line2D at 0x4951354f0>],\n",
       " [<matplotlib.lines.Line2D at 0x495135040>]]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "[plt.plot([x for x in range(100)],by_layer[l],  '-k', color='blue') for l in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5b0613fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1096e-01, -1.2329e-01, -1.1084e-01],\n",
       "          [-1.2993e-01, -1.1098e-01, -1.3594e-01],\n",
       "          [-6.7381e-02, -1.4634e-02,  2.8533e-02]],\n",
       "\n",
       "         [[-7.7981e-02, -2.1578e-03,  2.3003e-02],\n",
       "          [-1.2117e-01, -5.7711e-02, -1.4878e-02],\n",
       "          [-2.5713e-01, -2.0143e-01, -1.0216e-01]],\n",
       "\n",
       "         [[-4.3915e-03, -1.9280e-02, -2.8378e-02],\n",
       "          [-4.6765e-02, -8.1947e-02, -5.1299e-02],\n",
       "          [-6.9307e-02, -2.1950e-01, -2.1353e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8569e-01, -1.2538e-01, -1.2118e-01],\n",
       "          [ 3.3761e-02, -2.9584e-02,  2.6167e-02],\n",
       "          [-3.0270e-02, -6.6300e-02,  1.9905e-02]],\n",
       "\n",
       "         [[-5.8725e-02, -6.0506e-02, -7.6131e-02],\n",
       "          [ 4.1337e-02, -1.2399e-02,  3.5790e-02],\n",
       "          [ 4.3396e-02, -1.9127e-02, -1.2670e-01]],\n",
       "\n",
       "         [[ 1.0694e-01,  8.9551e-02,  9.3676e-02],\n",
       "          [ 2.4335e-01,  3.2137e-01,  3.1068e-01],\n",
       "          [ 1.2739e-01,  1.9844e-01,  1.4244e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.1267e-02,  1.1858e-01,  2.5228e-02],\n",
       "          [-2.6645e-01, -7.5907e-02,  3.0204e-02],\n",
       "          [ 1.4000e-01,  1.8465e-03, -1.8568e-01]],\n",
       "\n",
       "         [[-1.5179e-02, -4.0632e-03, -1.2352e-01],\n",
       "          [-1.9099e-01,  1.0255e-02,  1.6935e-01],\n",
       "          [-8.0174e-02,  5.1182e-02,  2.6853e-02]],\n",
       "\n",
       "         [[ 6.7075e-03, -8.0412e-03,  3.0122e-02],\n",
       "          [ 1.5871e-01, -1.1401e-02, -2.0761e-01],\n",
       "          [-2.2995e-02,  1.3486e-01,  5.1601e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.7799e-02,  1.0024e-01, -9.3393e-02],\n",
       "          [-2.4381e-01, -3.3734e-01, -1.6471e-01],\n",
       "          [ 2.4885e-01,  4.2311e-01,  1.1714e-01]],\n",
       "\n",
       "         [[-7.7726e-02,  1.5050e-01,  9.4534e-02],\n",
       "          [ 1.0922e-01, -1.2548e-01, -2.7240e-01],\n",
       "          [-1.2119e-01,  9.8591e-02,  2.3489e-01]],\n",
       "\n",
       "         [[-2.3907e-02,  7.5501e-02, -1.1934e-02],\n",
       "          [-1.7533e-01, -1.0020e-01, -7.4062e-02],\n",
       "          [-9.2058e-02,  7.8303e-02, -7.8507e-02]]],\n",
       "\n",
       "\n",
       "        [[[-4.3104e-01, -5.7189e-01, -4.3326e-01],\n",
       "          [-1.0239e-01, -9.3575e-02, -3.3883e-02],\n",
       "          [ 5.6058e-02,  2.9899e-01,  3.4688e-01]],\n",
       "\n",
       "         [[-1.0921e-01, -5.2348e-02, -5.9448e-03],\n",
       "          [-1.2670e-01, -2.6378e-01, -3.2515e-02],\n",
       "          [ 6.8985e-02,  5.3021e-02,  1.4330e-01]],\n",
       "\n",
       "         [[-4.1305e-03,  2.5629e-02,  2.1210e-02],\n",
       "          [-6.6144e-02, -1.6683e-01,  1.2794e-01],\n",
       "          [ 1.5553e-01, -4.3625e-02,  3.0598e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.6633e-02,  5.0078e-02, -9.6742e-02],\n",
       "          [ 3.9312e-02,  1.0999e-01,  7.2712e-02],\n",
       "          [-1.7286e-01, -1.5300e-01,  6.1827e-02]],\n",
       "\n",
       "         [[ 1.3341e-01,  7.1126e-02,  1.1662e-01],\n",
       "          [ 3.8440e-02,  6.4988e-03,  9.7538e-02],\n",
       "          [-1.2438e-02, -1.4038e-01, -1.4952e-01]],\n",
       "\n",
       "         [[-6.5931e-02,  5.8828e-02, -7.3476e-02],\n",
       "          [-1.6636e-01, -9.1345e-02, -1.6597e-01],\n",
       "          [-1.7878e-01, -1.4545e-01, -1.7612e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-3.1890e-01, -8.9355e-02,  4.5131e-01],\n",
       "          [-3.2570e-01, -3.2129e-01,  3.7161e-01],\n",
       "          [-2.5418e-01, -2.5314e-01,  1.7591e-01]],\n",
       "\n",
       "         [[-3.5942e-01, -5.5225e-03,  1.2099e-01],\n",
       "          [-2.6192e-01, -1.6951e-01,  3.6614e-01],\n",
       "          [-1.5769e-01, -3.1891e-01,  8.7825e-02]],\n",
       "\n",
       "         [[ 5.9650e-02,  2.0955e-01, -2.2324e-01],\n",
       "          [-5.1009e-02,  4.3352e-01, -3.2561e-01],\n",
       "          [-2.8634e-01,  2.9024e-01, -1.5285e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1922e-01, -3.9319e-02,  1.6735e-01],\n",
       "          [ 1.5122e-01, -1.9984e-01, -1.8296e-02],\n",
       "          [ 1.8593e-01, -4.6613e-02, -1.4247e-01]],\n",
       "\n",
       "         [[ 1.1678e-01,  1.3745e-01, -3.2050e-01],\n",
       "          [-5.6163e-02,  2.4330e-01, -3.3453e-01],\n",
       "          [-1.3105e-01,  2.6278e-01, -4.5960e-02]],\n",
       "\n",
       "         [[ 8.0161e-02,  1.3905e-03,  2.6035e-02],\n",
       "          [ 8.3491e-02, -4.4419e-02, -4.7975e-02],\n",
       "          [ 1.2470e-01,  3.2736e-03, -4.3810e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.1311e-02,  9.7264e-02,  6.5166e-02],\n",
       "          [-4.0381e-02,  7.2720e-02, -6.6125e-03],\n",
       "          [-9.7801e-02, -1.0985e-01, -1.2949e-01]],\n",
       "\n",
       "         [[-3.6722e-02,  8.9013e-02,  1.0709e-01],\n",
       "          [ 9.5156e-02,  1.1612e-01, -3.9410e-02],\n",
       "          [-3.1924e-02, -4.1518e-02, -2.2182e-02]],\n",
       "\n",
       "         [[-3.4603e-02,  8.4106e-02, -6.1335e-02],\n",
       "          [ 1.7926e-02,  6.6020e-03, -1.4932e-02],\n",
       "          [ 1.1016e-01,  9.5302e-02, -4.9213e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.9794e-02, -8.1157e-02, -1.2325e-02],\n",
       "          [ 7.6162e-02,  2.1607e-01,  1.6998e-01],\n",
       "          [-1.4431e-02, -3.2761e-02,  2.6566e-02]],\n",
       "\n",
       "         [[-6.1009e-02,  5.0737e-03, -6.0558e-02],\n",
       "          [ 4.4122e-04,  8.8493e-02,  4.0241e-02],\n",
       "          [ 5.2854e-02, -7.3236e-03,  5.9698e-03]],\n",
       "\n",
       "         [[-8.9265e-02,  6.0699e-02,  2.1334e-02],\n",
       "          [ 4.4724e-02,  1.1628e-01,  8.0902e-02],\n",
       "          [-3.4038e-02, -6.0316e-02, -4.8586e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.4623e-02, -2.9249e-02,  3.7047e-02],\n",
       "          [-1.4931e-01, -2.2723e-01, -8.0352e-02],\n",
       "          [-2.4241e-01, -2.2822e-01, -1.9583e-01]],\n",
       "\n",
       "         [[-1.2351e-01, -7.9157e-03,  5.7404e-02],\n",
       "          [-1.8471e-01, -6.9618e-02, -3.4416e-02],\n",
       "          [-9.9610e-02, -6.3130e-02,  1.6072e-01]],\n",
       "\n",
       "         [[-1.3727e-01, -1.7437e-01, -2.2587e-01],\n",
       "          [-6.8449e-02, -7.2583e-02, -1.0623e-01],\n",
       "          [ 1.6894e-01,  1.0352e-01,  2.2331e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.3980e-04,  6.8627e-02,  5.1235e-02],\n",
       "          [-1.5039e-01, -1.3974e-01, -9.2186e-02],\n",
       "          [ 9.2180e-02,  6.6301e-02,  5.9281e-02]],\n",
       "\n",
       "         [[-7.3474e-02, -1.2061e-01, -8.8610e-02],\n",
       "          [-3.2279e-02, -6.8353e-02, -8.5987e-02],\n",
       "          [ 9.6560e-02,  4.7846e-02, -1.3998e-02]],\n",
       "\n",
       "         [[-6.5294e-02, -2.0633e-02, -6.9402e-02],\n",
       "          [-2.6595e-01, -2.6203e-01, -2.6884e-01],\n",
       "          [-1.3080e-01, -1.3165e-01, -1.2422e-01]]]], device='mps:0')"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(baseline.avg_weight_history[99]['model.1.0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956fcb00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
