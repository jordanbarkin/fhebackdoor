{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure GPU (change if not M1 mac)\n",
    "mps = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "\n",
    "# Using CIFAR-10 again as in the programming assignments\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving And helpers\n",
    "\n",
    "def save_tracker(tracker, path):\n",
    "  np.savetxt(path, tracker, delimiter=',') \n",
    "\n",
    "def save_trackers(device, filename):\n",
    "  \"\"\"Save all trackers and current total_time to a file.\"\"\"\n",
    "  torch.save((device['train_loss_tracker'], device['train_acc_tracker'], device['test_loss_tracker'], device['test_acc_tracker'], total_time), filename)\n",
    "  print(\"Saved trackers to \" + filename)\n",
    "\n",
    "def moving_average(a, n=100):\n",
    "  '''Helper function used for visualization'''\n",
    "  ret = torch.cumsum(torch.Tensor(a), 0)\n",
    "  ret[n:] = ret[n:] - ret[:-n]\n",
    "  return ret[n - 1:] / n\n",
    "\n",
    "# Plotting helpers! \n",
    "def make_plot(trackers, num_epochs, title, y_axis_lab, should_average=False, legend=True, fix_ax=True):\n",
    "  avg_fn = moving_average if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  if fix_ax:\n",
    "    ax.set_ylim([0, 100])\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "\n",
    "  if legend:\n",
    "    _ = plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def make_plot_better(trackers, num_epochs, title, y_axis_lab, should_average=False, n = 100):\n",
    "  avg_fn = (lambda x : moving_average(x, n)) if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "  _ = plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "def iid_sampler(dataset, num_devices, data_pct):\n",
    "    '''\n",
    "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
    "    num_devices: integer number of devices to create subsets for\n",
    "    data_pct: percentalge of training samples to give each device\n",
    "              e.g., 0.1 represents 10%\n",
    "\n",
    "    return: a dictionary of the following format:\n",
    "      {\n",
    "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
    "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    iid (independent and identically distributed) means that the indexes\n",
    "    should be drawn independently in a uniformly random fashion.\n",
    "    '''\n",
    "    total_samples = len(dataset)\n",
    "    sampled = {}\n",
    "    number_samples = int((data_pct)*(total_samples)) \n",
    "\n",
    "    for i in range(num_devices):\n",
    "      sampled[i] = random.sample(range(total_samples), number_samples)\n",
    "        \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net definitions\n",
    "\n",
    "# Same ConvNet as in Assignment 2 and 3\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated learning helpers\n",
    "\n",
    "# The baseline `average' function. \n",
    "def average_weights(devices, **kwargs):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "#     max_magnitude = 0\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "      w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "#             max_magnitude = max(max_magnitude, abs(torch.max(state_dicts[i][k].type(torch.float32))))\n",
    "            w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "def get_devices_for_round(devices, device_pct):  \n",
    "    return random.sample(devices, int(device_pct * len(devices)))\n",
    "\n",
    "def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n",
    "                  milestones=None, batch_size=128):\n",
    "    if milestones == None:\n",
    "        milestones = [25, 50, 75]\n",
    "\n",
    "    device_net = copy.deepcopy(net)\n",
    "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=milestones,\n",
    "                                                     gamma=0.1)\n",
    "    device_trainset = DatasetSplit(trainset, data_idxs)\n",
    "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "    return {\n",
    "        'net': device_net,\n",
    "        'id': device_id,\n",
    "        'dataloader': device_trainloader, \n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_tracker': [],\n",
    "        'train_acc_tracker': [],\n",
    "        'test_loss_tracker': [],\n",
    "        'test_acc_tracker': [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local device training and testing\n",
    "def train(epoch, device, criterion):\n",
    "    device['net'].train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
    "        inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "        device['optimizer'].zero_grad()\n",
    "        outputs = device['net'](inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        device['optimizer'].step()\n",
    "        train_loss += loss.item()\n",
    "        device['train_loss_tracker'].append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        dev_id = device['id']\n",
    "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "                         f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    device['train_acc_tracker'].append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(epoch, device, criterion, testloader = testloader):\n",
    "    device['net'].eval()\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "            outputs = device['net'](inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            device['test_loss_tracker'].append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()  \n",
    "    acc = 100.*correct/total\n",
    "    device['test_acc_tracker'].append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function tests (the main experiment routine)\n",
    "\n",
    "# Given two different sets of aggregated weights, \n",
    "# gives a value representing the difference between them,\n",
    "# as a way to measure the direct cost of using our aggregated function\n",
    "def diff_aggregated_weights(strat, baseline):\n",
    "    result = 0 \n",
    "    for k in strat.keys():\n",
    "        result += torch.linalg.norm(strat[k] - baseline[k])\n",
    "    return result\n",
    "\n",
    "# A data type for experiment results\n",
    "BackdoorResult = namedtuple(\"BackdoorResult\", [\"scheme_loss\", \"test_accuracy\", \"backdoor_success\", \"devices\", \"avg_weight_history\"])\n",
    "\n",
    "# The main routine! \n",
    "def run_federated_test(agg_fn = average_weights,  # Pass in aggregation function, \n",
    "                                                  # Device list -> aggregated weights \n",
    "                       rounds = 10,               # Rounds of FL\n",
    "                       local_epochs = 4,          # Epochs per device                      \n",
    "                       num_devices = 50,          # Total # devices\n",
    "                       device_pct = 0.1,          # % of devices per round\n",
    "                       data_pct = 0.1,            # % of data each device gets\n",
    "                       net = ConvNet().to(mps),   # Net design; make sure on mps\n",
    "                       evil_round = None,         # If None, no attack; else attacker will mount attack on this round\n",
    "                       attacker_strategy = None,  # device --> void --- set up the local weights on the attacker\n",
    "                       evil_device_id = None,     # Which device attacks? \n",
    "                       evaluate_attack = None,    # Function devices --> <any> measuring how well the attack worked at the end\n",
    "                       output_filename = None, \n",
    "                       snapshot = True, \n",
    "                       resume_from_snap = None):   \n",
    "    def lighten_device(d):\n",
    "        return {\n",
    "            k: d[k] for k in ( 'id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker')\n",
    "        }                                   \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "    devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "    \n",
    "    scheme_loss = []\n",
    "    max_magnitudes = []\n",
    "    avg_weight_history = []\n",
    "    _starting_round_num = 0\n",
    "    \n",
    "    if resume_from_snap: \n",
    "        print(\"Resuming from snapshot!\\n\")\n",
    "        # Load what we can\n",
    "        result = resume_from_snap\n",
    "        scheme_loss = result.scheme_loss\n",
    "        avg_weight_history = result.avg_weight_history\n",
    "        partial_devices = result.devices\n",
    "\n",
    "        # Fresh data and devices\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "        devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "\n",
    "        # Restore the devices with the info from the trackers\n",
    "        def restore_device(old_device, new_device):\n",
    "            for k in ('id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker'):\n",
    "                new_device[k] = old_device[k]\n",
    "            return new_device\n",
    "        \n",
    "        print(\"Restoring devices\\n\")\n",
    "        devices = [restore_device(partial_devices[i], devices[i]) for i in range(len(devices))]\n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(avg_weight_history[-1])\n",
    "        \n",
    "        _starting_round_num = len(result.test_accuracy)\n",
    "        print(\"Finished restoring\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "    ## IID Federated Learning\n",
    "    start_time = time.time()\n",
    "    for round_num in range(_starting_round_num, rounds):\n",
    "        round_start_time = time.time()\n",
    "        # Part 1.3: Implement getting devices for each round here\n",
    "        round_devices = get_devices_for_round(devices, device_pct)\n",
    "\n",
    "        print('Round: ', round_num)\n",
    "        # Train locally \n",
    "        for device in round_devices:\n",
    "            for local_epoch in range(local_epochs):\n",
    "                train(local_epoch, device, criterion)\n",
    "        \n",
    "        # One device becomes evil if required\n",
    "        if (evil_round and round_num == evil_round):\n",
    "            assert (evil_device_id is not None)\n",
    "            assert (attacker_strategy is not None)\n",
    "            print(\"Attacking!\\n\")\n",
    "            \n",
    "            attacker_strategy(devices[evil_device_id])\n",
    "            # Make sure evil guy gets averaged in \n",
    "            if evil_device_id not in round_devices:\n",
    "                round_devices.append(devices[evil_device_id])\n",
    "            print(\"Finished attacking\\n\")\n",
    "            \n",
    "        \n",
    "        # Weight averaging\n",
    "        w_baseline = average_weights(round_devices)\n",
    "        w_avg = agg_fn(round_devices, avg_weight_history[-1] if avg_weight_history != [] else None)\n",
    "        max_magnitudes.append(max_magnitude)\n",
    "        \n",
    "        # Track the difference between the two; should be 0 if straight average\n",
    "        scheme_loss.append((float(diff_aggregated_weights(w_avg, w_baseline))))\n",
    "        \n",
    "        avg_weight_history.append(copy.deepcopy(w_avg))\n",
    "        \n",
    "        # Gradients         \n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(w_avg)\n",
    "            device['optimizer'].zero_grad()\n",
    "            device['optimizer'].step()\n",
    "            device['scheduler'].step()\n",
    "\n",
    "        # test accuracy after aggregation\n",
    "        # device 0 is the unique device with all of the \n",
    "        # test accuracies and losses in its tracker\n",
    "        test(round_num, devices[0], criterion)\n",
    "        \n",
    "        print(f\"\\nDiff: {scheme_loss[-1]}\\n\")\n",
    "        print(f\"Round time: {time.time() - start_time} \\n\")\n",
    "        if snapshot:\n",
    "            intermediate_result = BackdoorResult(\n",
    "                scheme_loss = scheme_loss, \n",
    "                test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "                backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "                devices = [lighten_device(d) for d in devices], \n",
    "                avg_weight_history = avg_weight_history\n",
    "            )\n",
    "\n",
    "\n",
    "            if output_filename is not None: \n",
    "                print(\"Writing snapshot\\n\")\n",
    "                with open(f\"snapshot_{output_filename}\", 'wb') as file: \n",
    "                    pickle.dump(intermediate_result, file)\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    print('Total training time: {} seconds'.format(total_time))\n",
    "    \n",
    "    # Pack up everything we care about and the devices for good measure\n",
    "    result = BackdoorResult(\n",
    "        scheme_loss = scheme_loss, \n",
    "        test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "        backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "        devices = [lighten_device(d) for d in devices], \n",
    "        avg_weight_history = avg_weight_history\n",
    "    )\n",
    "\n",
    "    \n",
    "    if output_filename is not None: \n",
    "        with open(output_filename, 'wb') as file: \n",
    "            print(\"Writing file\\n\")\n",
    "            pickle.dump(result, file)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Load output files back into memory\n",
    "def load_result(filename):\n",
    "    with open(filename, 'rb') as file: \n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 9/Epoch 0) Train Loss: 2.036 | Train Acc: 23.700"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/2923223566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_federated_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"testout.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/2375901062.py\u001b[0m in \u001b[0;36mrun_federated_test\u001b[0;34m(agg_fn, rounds, local_epochs, num_devices, device_pct, data_pct, net, evil_round, attacker_strategy, evil_device_id, evaluate_attack, output_filename, snapshot, resume_from_snap)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Weight averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mw_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_magnitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mw_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_weight_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mavg_weight_history\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mmax_magnitudes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_magnitude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "results = run_federated_test(local_epochs=1, num_devices = 10, rounds = 2, output_filename = \"testout.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Can also load it back\n",
    "# results = load_result(\"snapshot_testout.pickle\")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A silly attack that just sends massive weights all of magnitude 10000\n",
    "def sample_attack(device):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    weights = device[\"net\"].state_dict().copy()\n",
    "    \n",
    "    for w in weights.keys():\n",
    "        weights[w] = torch.full(weights[w].size(), 10000)\n",
    "    \n",
    "    device['net'].load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function that squashes any weights with magnitude over 1000 (except on the first device but whatever)\n",
    "def super_smart_aggregation(devices):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights, excluding huge updates.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            if (torch.max(state_dicts[i][k].type(torch.float32)) <= 1000): \n",
    "                w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 9/Epoch 0) Train Loss: 2.168 | Train Acc: 18.415"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/3893217655.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Here, we carry out the attack but use the ordinary average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m results_straight_avg = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n\u001b[0m\u001b[1;32m      5\u001b[0m                                          \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                          \u001b[0mlocal_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/3626052363.py\u001b[0m in \u001b[0;36mrun_federated_test\u001b[0;34m(agg_fn, rounds, local_epochs, num_devices, device_pct, data_pct, net, evil_round, attacker_strategy, evil_device_id, evaluate_attack, output_filename, snapshot, resume_from_snap)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mround_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlocal_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# One device becomes evil if required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/275447899.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss_tracker'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    198\u001b[0m          \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m          \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of how this works\n",
    "\n",
    "# Here, we carry out the attack but use the ordinary average\n",
    "results_straight_avg = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline_trivial_attack.pickle\")  \n",
    "# Observe that it absolutely destroys our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 7/Epoch 0) Train Loss: 2.018 | Train Acc: 23.180 | Test Loss: 18.179 | Test Acc: 12.360\n",
      "\n",
      "Diff: 84.85118865966797\n",
      "\n",
      "Round:  1\n",
      "(Device 9/Epoch 0) Train Loss: 1.873 | Train Acc: 29.680Attacking!\n",
      "\n",
      " | Test Loss: 3.511 | Test Acc: 17.340\n",
      "\n",
      "Diff: 10114592.0\n",
      "\n",
      "Round:  2\n",
      "(Device 2/Epoch 0) Train Loss: 1.789 | Train Acc: 32.900 | Test Loss: 2.722 | Test Acc: 23.900\n",
      "\n",
      "Diff: 46.39040756225586\n",
      "\n",
      "Total training time: 56.98331904411316 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now we do it again but with the aggregation that rejects huge updates\n",
    "results_reject_huge = run_federated_test(agg_fn = super_smart_aggregation,                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None,\n",
    "                                         output_filename = \"simple_attack_and_defense.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 2/Epoch 3) Train Loss: 1.727 | Train Acc: 34.8200 | Test Loss: 2.821 | Test Acc: 10.200\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  1\n",
      "(Device 24/Epoch 3) Train Loss: 1.562 | Train Acc: 41.420 | Test Loss: 1.498 | Test Acc: 44.420\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  2\n",
      "(Device 21/Epoch 3) Train Loss: 1.314 | Train Acc: 50.700 | Test Loss: 1.242 | Test Acc: 54.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  3\n",
      "(Device 9/Epoch 3) Train Loss: 1.142 | Train Acc: 59.3200 | Test Loss: 1.086 | Test Acc: 60.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  4\n",
      "(Device 15/Epoch 3) Train Loss: 1.108 | Train Acc: 59.800 | Test Loss: 0.965 | Test Acc: 65.690\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  5\n",
      "(Device 1/Epoch 3) Train Loss: 1.012 | Train Acc: 63.1400 | Test Loss: 0.869 | Test Acc: 69.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  6\n",
      "(Device 28/Epoch 3) Train Loss: 0.883 | Train Acc: 68.640 | Test Loss: 0.815 | Test Acc: 71.450\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  7\n",
      "(Device 10/Epoch 3) Train Loss: 0.804 | Train Acc: 72.000 | Test Loss: 0.757 | Test Acc: 73.380\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  8\n",
      "(Device 41/Epoch 3) Train Loss: 0.766 | Train Acc: 73.160 | Test Loss: 0.683 | Test Acc: 76.610\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  9\n",
      "(Device 16/Epoch 3) Train Loss: 0.682 | Train Acc: 75.940 | Test Loss: 0.648 | Test Acc: 77.530\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  10\n",
      "(Device 24/Epoch 3) Train Loss: 0.643 | Train Acc: 77.680 | Test Loss: 0.602 | Test Acc: 79.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  11\n",
      "(Device 27/Epoch 3) Train Loss: 0.578 | Train Acc: 79.520 | Test Loss: 0.647 | Test Acc: 77.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  12\n",
      "(Device 27/Epoch 3) Train Loss: 0.583 | Train Acc: 79.420 | Test Loss: 0.595 | Test Acc: 79.950\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  13\n",
      "(Device 23/Epoch 3) Train Loss: 0.551 | Train Acc: 80.640 | Test Loss: 0.548 | Test Acc: 81.380\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  14\n",
      "(Device 24/Epoch 3) Train Loss: 0.537 | Train Acc: 81.080 | Test Loss: 0.521 | Test Acc: 82.400\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  15\n",
      "(Device 13/Epoch 3) Train Loss: 0.447 | Train Acc: 84.500 | Test Loss: 0.510 | Test Acc: 82.860\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  16\n",
      "(Device 41/Epoch 3) Train Loss: 0.539 | Train Acc: 80.840 | Test Loss: 0.504 | Test Acc: 83.430\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  17\n",
      "(Device 41/Epoch 3) Train Loss: 0.435 | Train Acc: 85.920 | Test Loss: 0.505 | Test Acc: 83.600\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  18\n",
      "(Device 36/Epoch 3) Train Loss: 0.414 | Train Acc: 85.340 | Test Loss: 0.469 | Test Acc: 84.710\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  19\n",
      "(Device 19/Epoch 3) Train Loss: 0.429 | Train Acc: 85.720 | Test Loss: 0.461 | Test Acc: 85.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  20\n",
      "(Device 15/Epoch 3) Train Loss: 0.356 | Train Acc: 87.620 | Test Loss: 0.449 | Test Acc: 85.460\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  21\n",
      "(Device 12/Epoch 3) Train Loss: 0.379 | Train Acc: 86.120 | Test Loss: 0.431 | Test Acc: 85.900\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  22\n",
      "(Device 28/Epoch 3) Train Loss: 0.382 | Train Acc: 87.000 | Test Loss: 0.442 | Test Acc: 85.790\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  23\n",
      "(Device 20/Epoch 3) Train Loss: 0.321 | Train Acc: 88.840 | Test Loss: 0.413 | Test Acc: 86.540\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  24\n",
      "(Device 15/Epoch 3) Train Loss: 0.270 | Train Acc: 90.920 | Test Loss: 0.419 | Test Acc: 86.480\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  25\n",
      "(Device 39/Epoch 3) Train Loss: 0.236 | Train Acc: 91.420 | Test Loss: 0.391 | Test Acc: 87.190\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  26\n",
      "(Device 36/Epoch 3) Train Loss: 0.245 | Train Acc: 92.040 | Test Loss: 0.390 | Test Acc: 87.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  27\n",
      "(Device 19/Epoch 3) Train Loss: 0.221 | Train Acc: 92.440 | Test Loss: 0.388 | Test Acc: 87.520\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  28\n",
      "(Device 19/Epoch 3) Train Loss: 0.235 | Train Acc: 92.680 | Test Loss: 0.386 | Test Acc: 87.520\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  29\n",
      "(Device 30/Epoch 3) Train Loss: 0.221 | Train Acc: 92.260 | Test Loss: 0.387 | Test Acc: 87.370\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  30\n",
      "(Device 26/Epoch 3) Train Loss: 0.221 | Train Acc: 92.580 | Test Loss: 0.386 | Test Acc: 87.480\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  31\n",
      "(Device 49/Epoch 3) Train Loss: 0.195 | Train Acc: 93.360 | Test Loss: 0.386 | Test Acc: 87.650\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  32\n",
      "(Device 34/Epoch 3) Train Loss: 0.216 | Train Acc: 93.480 | Test Loss: 0.389 | Test Acc: 87.620\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  33\n",
      "(Device 7/Epoch 3) Train Loss: 0.246 | Train Acc: 91.8800 | Test Loss: 0.384 | Test Acc: 87.560\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  34\n",
      "(Device 38/Epoch 3) Train Loss: 0.186 | Train Acc: 94.140 | Test Loss: 0.385 | Test Acc: 87.580\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  35\n",
      "(Device 44/Epoch 3) Train Loss: 0.189 | Train Acc: 93.860 | Test Loss: 0.387 | Test Acc: 87.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  36\n",
      "(Device 25/Epoch 3) Train Loss: 0.210 | Train Acc: 93.400 | Test Loss: 0.387 | Test Acc: 87.630\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  37\n",
      "(Device 38/Epoch 3) Train Loss: 0.184 | Train Acc: 94.600 | Test Loss: 0.389 | Test Acc: 87.620\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  38\n",
      "(Device 10/Epoch 3) Train Loss: 0.209 | Train Acc: 92.800 | Test Loss: 0.388 | Test Acc: 87.800\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  39\n",
      "(Device 47/Epoch 3) Train Loss: 0.180 | Train Acc: 93.820 | Test Loss: 0.388 | Test Acc: 87.760\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  40\n",
      "(Device 2/Epoch 3) Train Loss: 0.192 | Train Acc: 93.3200 | Test Loss: 0.390 | Test Acc: 87.920\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  41\n",
      "(Device 11/Epoch 3) Train Loss: 0.198 | Train Acc: 93.520 | Test Loss: 0.388 | Test Acc: 87.820\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  42\n",
      "(Device 11/Epoch 3) Train Loss: 0.218 | Train Acc: 93.460 | Test Loss: 0.391 | Test Acc: 87.870\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  43\n",
      "(Device 48/Epoch 3) Train Loss: 0.186 | Train Acc: 94.340 | Test Loss: 0.389 | Test Acc: 87.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  44\n",
      "(Device 28/Epoch 3) Train Loss: 0.175 | Train Acc: 93.680 | Test Loss: 0.390 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  45\n",
      "(Device 26/Epoch 3) Train Loss: 0.187 | Train Acc: 93.960 | Test Loss: 0.391 | Test Acc: 87.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  46\n",
      "(Device 29/Epoch 3) Train Loss: 0.171 | Train Acc: 93.880 | Test Loss: 0.388 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  47\n",
      "(Device 34/Epoch 3) Train Loss: 0.164 | Train Acc: 94.000 | Test Loss: 0.393 | Test Acc: 87.850\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  48\n",
      "(Device 40/Epoch 3) Train Loss: 0.191 | Train Acc: 94.680 | Test Loss: 0.393 | Test Acc: 87.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  49\n",
      "(Device 30/Epoch 3) Train Loss: 0.162 | Train Acc: 94.740 | Test Loss: 0.392 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  50\n",
      "(Device 48/Epoch 3) Train Loss: 0.177 | Train Acc: 93.800 | Test Loss: 0.391 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  51\n",
      "(Device 9/Epoch 3) Train Loss: 0.173 | Train Acc: 94.1000 | Test Loss: 0.390 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  52\n",
      "(Device 24/Epoch 3) Train Loss: 0.175 | Train Acc: 93.900 | Test Loss: 0.390 | Test Acc: 88.010\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  53\n",
      "(Device 49/Epoch 3) Train Loss: 0.174 | Train Acc: 94.080 | Test Loss: 0.390 | Test Acc: 87.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  54\n",
      "(Device 7/Epoch 3) Train Loss: 0.185 | Train Acc: 93.3800 | Test Loss: 0.389 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  55\n",
      "(Device 28/Epoch 3) Train Loss: 0.190 | Train Acc: 94.320 | Test Loss: 0.389 | Test Acc: 88.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  56\n",
      "(Device 37/Epoch 3) Train Loss: 0.165 | Train Acc: 94.220 | Test Loss: 0.388 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  57\n",
      "(Device 17/Epoch 3) Train Loss: 0.199 | Train Acc: 93.800 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  58\n",
      "(Device 37/Epoch 3) Train Loss: 0.170 | Train Acc: 94.200 | Test Loss: 0.387 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  59\n",
      "(Device 18/Epoch 3) Train Loss: 0.162 | Train Acc: 94.760 | Test Loss: 0.387 | Test Acc: 88.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  60\n",
      "(Device 7/Epoch 3) Train Loss: 0.184 | Train Acc: 93.3600 | Test Loss: 0.388 | Test Acc: 88.170\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  61\n",
      "(Device 40/Epoch 3) Train Loss: 0.170 | Train Acc: 94.060 | Test Loss: 0.387 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  62\n",
      "(Device 30/Epoch 3) Train Loss: 0.173 | Train Acc: 94.480 | Test Loss: 0.389 | Test Acc: 88.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  63\n",
      "(Device 18/Epoch 3) Train Loss: 0.164 | Train Acc: 94.420 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  64\n",
      "(Device 23/Epoch 3) Train Loss: 0.166 | Train Acc: 94.000 | Test Loss: 0.388 | Test Acc: 88.080\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  65\n",
      "(Device 46/Epoch 3) Train Loss: 0.162 | Train Acc: 94.620 | Test Loss: 0.388 | Test Acc: 87.990\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  66\n",
      "(Device 22/Epoch 3) Train Loss: 0.159 | Train Acc: 94.440 | Test Loss: 0.389 | Test Acc: 88.100\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  67\n",
      "(Device 27/Epoch 3) Train Loss: 0.180 | Train Acc: 95.020 | Test Loss: 0.387 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  68\n",
      "(Device 25/Epoch 3) Train Loss: 0.182 | Train Acc: 93.740 | Test Loss: 0.387 | Test Acc: 88.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Device 11/Epoch 3) Train Loss: 0.174 | Train Acc: 94.300 | Test Loss: 0.386 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  70\n",
      "(Device 0/Epoch 3) Train Loss: 0.190 | Train Acc: 94.0800 | Test Loss: 0.386 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  71\n",
      "(Device 31/Epoch 3) Train Loss: 0.166 | Train Acc: 94.200 | Test Loss: 0.386 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  72\n",
      "(Device 1/Epoch 1) Train Loss: 0.164 | Train Acc: 94.3820"
     ]
    }
   ],
   "source": [
    "# Here, we compute a real baseline over 100 epochs and typical averaging. Good starting point for future \n",
    "# tests; can resume from this snapshot.\n",
    "baseline = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 100,              \n",
    "                                         local_epochs = 4,                             \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = load_result(\"baseline.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, now can get things like\n",
    "# baseline.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 27/Epoch 3) Train Loss: 0.324 | Train Acc: 88.320 | Test Loss: 0.409 | Test Acc: 86.340\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 664.8843719959259 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 667.9921019077301 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can resume from a snapshot:\n",
    "# NOTE : keep all the basic params (local epochs, num devices, net, etc) the same\n",
    "# when resuming from a snapshot \n",
    "baseline2 = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 101,              \n",
    "                                         local_epochs = 4,                             \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline2.pickle\", \n",
    "                                         resume_from_snap = baseline )  # we pass in a results object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline2_loaded = load_result(\"baseline2.pickle\")\n",
    "baseline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can move on to a real defense. \n",
    "\n",
    "# We create factory functions that let us parametrize a double sigmoid using threshholds\n",
    "\n",
    "def torch_scaled_sigmoid_factory(a, c = 0.005, left = False):\n",
    "    \n",
    "    def f(x):\n",
    "        scaled = torch.sub(x, a)\n",
    "\n",
    "        divisor = c if left else -1*c\n",
    "\n",
    "        scaled = torch.div(scaled, divisor)\n",
    "\n",
    "        return torch.sigmoid(scaled)\n",
    "    return f\n",
    "\n",
    "def torch_double_sigmoid_factory(a,b,c = 0.005):\n",
    "    left = torch_scaled_sigmoid_factory(a, left = True)\n",
    "    right = torch_scaled_sigmoid_factory(b, left = False) \n",
    "    def f(x):\n",
    "        return (left(x)*right(x))\n",
    "    return f\n",
    "\n",
    "my_sigmoid = torch_double_sigmoid_factory(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it works\n",
    "# torch_double_sigmoid_factory(-2,2)(torch.Tensor([-5,-4,-3,-2,-1.5,-1.4,1.3,-1,0,1,1.5,1.6,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a sigmoid defense,\n",
    "#     keys_to_range_mapping maps each layer name in the state dict to the sigmoid bounds for processing it\n",
    "#     All other layers just get identity function-ed\n",
    "def make_sigmoid_defense(keys_to_range_mapping, stickiness = 0):\n",
    "    \n",
    "    sigmoid_dict = defaultdict(lambda : (lambda x: 1))\n",
    "    \n",
    "    for k,(a,b) in keys_to_range_mapping.items():\n",
    "        sigmoid_dict[k] = torch_double_sigmoid_factory(a,b)\n",
    "    \n",
    "    def f(devices, previous_round_weights):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights preprocessed by sigmoid\n",
    "        '''\n",
    "        state_dicts = [device['net'].state_dict() for device in devices]\n",
    "        # initialize w_avg to tensors from device 0\n",
    "        w_avg = copy.deepcopy(state_dicts[0])\n",
    "        for k in w_avg.keys():\n",
    "            sig = sigmoid_dict[k]\n",
    "            x = w_avg[k].type(torch.float32)\n",
    "            prev_round_x = previous_round_weights[k].type(torch.float32) \n",
    "            print(\"Diff\\n\")\n",
    "            print((x - prev_round_x))\n",
    "            w_avg[k] = x*sig(x - prev_round_x)\n",
    "            for i in range(stickiness):\n",
    "                w_avg[k] += x*sig(x - prev_round_x)                \n",
    "\n",
    "        # for each model param\n",
    "        for k in w_avg.keys():\n",
    "            sig = sigmoid_dict[k]\n",
    "            # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "            prev_round_x = previous_round_weights[k].type(torch.float32) \n",
    "            for i in range(1, len(devices)):\n",
    "                x = (state_dicts[i][k].type(torch.float32))\n",
    "                w_avg[k] += x*sig(x - prev_round_x)\n",
    "            # compute average\n",
    "            w_avg[k] /= float(len(devices) + stickiness)\n",
    "        return w_avg\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.0.0.weight',\n",
       " 'model.1.0.weight',\n",
       " 'model.2.0.weight',\n",
       " 'model.3.0.weight',\n",
       " 'model.4.0.weight',\n",
       " 'model.5.0.weight',\n",
       " 'model.6.0.weight',\n",
       " 'model.7.0.weight',\n",
       " 'model.8.0.weight']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the actual layer weights that we care about\n",
    "[x for x in baseline.avg_weight_history[-1].keys() if '.0.weight' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets set up a potential sigmoid defense\n",
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-2,2),\n",
    "    'model.1.0.weight' : (-2,2),\n",
    "    'model.2.0.weight' : (-.7,.7),\n",
    "    'model.3.0.weight' : (-.6,.6),\n",
    "    'model.4.0.weight' : (-0.7,0.7),\n",
    "    'model.5.0.weight' : (-0.5,0.5),\n",
    "    'model.6.0.weight' : (-0.3,0.3),\n",
    "    'model.7.0.weight' : (-0.2,0.2),\n",
    "    'model.8.0.weight' : (-0.2,0.2),\n",
    "}\n",
    "sigmoid_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid_resumption = run_federated_test(agg_fn = sigmoid_accuracy,                    \n",
    "#                                          rounds = 110,              \n",
    "#                                          local_epochs = 1,                             \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.05,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evaluate_attack = None, \n",
    "#                                          output_filename = None,\n",
    "#                                          snapshot = False,\n",
    "#                                          resume_from_snap = baseline )  # we pass in a results object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make an attack\n",
    "# Makes an attack that puts noise in all of target layers. \n",
    "# Noise is uniform over a,b\n",
    "def noise_attack_factory(target_layers, a,b):\n",
    "    def attack(device):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights.\n",
    "        '''\n",
    "        weights = device[\"net\"].state_dict().copy()\n",
    "\n",
    "        for w in weights.keys():\n",
    "            if w in target_layers:\n",
    "                weights[w] = (a - b) * torch.rand(weights[w].size()) + b\n",
    "\n",
    "        device['net'].load_state_dict(weights)\n",
    "    return attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We attack these layers with weights ranging from -4,4\n",
    "target_layers = ['model.0.0.weight',\n",
    " 'model.1.0.weight',\n",
    " 'model.2.0.weight',\n",
    " 'model.3.0.weight',\n",
    " 'model.4.0.weight',\n",
    " 'model.5.0.weight',\n",
    " 'model.6.0.weight',\n",
    " 'model.7.0.weight',\n",
    " 'model.8.0.weight']\n",
    "big_noise_attack = noise_attack_factory(target_layers,-3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll resume from our baseline model\n",
    "baseline = load_result(\"baseline.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 43/Epoch 3) Train Loss: 0.293 | Train Acc: 89.860 | Test Loss: 0.589 | Test Acc: 82.010\n",
      "\n",
      "Diff: 87.77852630615234\n",
      "\n",
      "Round time: 668.7943501472473 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  101\n",
      "(Device 9/Epoch 3) Train Loss: 0.320 | Train Acc: 88.5200 | Test Loss: 0.493 | Test Acc: 84.380\n",
      "\n",
      "Diff: 70.42446899414062\n",
      "\n",
      "Round time: 1265.7698509693146 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  102\n",
      "(Device 40/Epoch 3) Train Loss: 0.216 | Train Acc: 92.800 | Test Loss: 0.565 | Test Acc: 83.180\n",
      "\n",
      "Diff: 77.08517456054688\n",
      "\n",
      "Round time: 1843.9197390079498 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  103\n",
      "(Device 43/Epoch 3) Train Loss: 0.384 | Train Acc: 86.640 | Test Loss: 0.663 | Test Acc: 79.490\n",
      "\n",
      "Diff: 66.0071029663086\n",
      "\n",
      "Round time: 2419.1470758914948 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  104\n",
      "(Device 48/Epoch 3) Train Loss: 0.181 | Train Acc: 93.620 | Test Loss: 0.593 | Test Acc: 82.430\n",
      "\n",
      "Diff: 65.98039245605469\n",
      "\n",
      "Round time: 2990.5921189785004 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  105\n",
      "(Device 4/Epoch 3) Train Loss: 0.351 | Train Acc: 88.2400 | Test Loss: 0.513 | Test Acc: 84.850\n",
      "\n",
      "Diff: 69.80987548828125\n",
      "\n",
      "Round time: 3561.174861907959 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  106\n",
      "(Device 26/Epoch 3) Train Loss: 0.352 | Train Acc: 87.560 | Test Loss: 0.445 | Test Acc: 86.160\n",
      "\n",
      "Diff: 72.47367095947266\n",
      "\n",
      "Round time: 4138.866477012634 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  107\n",
      "(Device 46/Epoch 3) Train Loss: 0.262 | Train Acc: 91.640 | Test Loss: 0.453 | Test Acc: 85.770\n",
      "\n",
      "Diff: 86.12258911132812\n",
      "\n",
      "Round time: 4723.356612920761 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  108\n",
      "(Device 17/Epoch 3) Train Loss: 0.234 | Train Acc: 92.420 | Test Loss: 0.472 | Test Acc: 85.410\n",
      "\n",
      "Diff: 85.34950256347656\n",
      "\n",
      "Round time: 5298.934043884277 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  109\n",
      "(Device 22/Epoch 3) Train Loss: 0.294 | Train Acc: 89.760 | Test Loss: 0.439 | Test Acc: 86.280\n",
      "\n",
      "Diff: 71.52302551269531\n",
      "\n",
      "Round time: 5924.683141946793 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  110\n",
      "(Device 48/Epoch 3) Train Loss: 0.173 | Train Acc: 93.680Attacking!\n",
      "\n",
      " | Test Loss: 0.471 | Test Acc: 84.590\n",
      "\n",
      "Diff: 560.0950317382812\n",
      "\n",
      "Round time: 6654.875817060471 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  111\n",
      "(Device 49/Epoch 3) Train Loss: 0.224 | Train Acc: 92.640 | Test Loss: 0.456 | Test Acc: 85.930\n",
      "\n",
      "Diff: 83.57064056396484\n",
      "\n",
      "Round time: 7395.187988042831 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  112\n",
      "(Device 39/Epoch 3) Train Loss: 0.212 | Train Acc: 92.720 | Test Loss: 0.449 | Test Acc: 86.400\n",
      "\n",
      "Diff: 67.50779724121094\n",
      "\n",
      "Round time: 13916.307879924774 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  113\n",
      "(Device 8/Epoch 3) Train Loss: 0.189 | Train Acc: 93.4000 | Test Loss: 0.450 | Test Acc: 86.550\n",
      "\n",
      "Diff: 60.869834899902344\n",
      "\n",
      "Round time: 14504.004343032837 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  114\n",
      "(Device 30/Epoch 3) Train Loss: 0.239 | Train Acc: 91.340 | Test Loss: 0.474 | Test Acc: 85.770\n",
      "\n",
      "Diff: 82.01603698730469\n",
      "\n",
      "Round time: 15075.272027015686 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  115\n",
      "(Device 19/Epoch 3) Train Loss: 0.249 | Train Acc: 91.320 | Test Loss: 0.473 | Test Acc: 85.680\n",
      "\n",
      "Diff: 76.22505950927734\n",
      "\n",
      "Round time: 15662.505131959915 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  116\n",
      "(Device 24/Epoch 3) Train Loss: 0.196 | Train Acc: 93.300 | Test Loss: 0.423 | Test Acc: 87.220\n",
      "\n",
      "Diff: 67.48401641845703\n",
      "\n",
      "Round time: 16301.689856052399 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  117\n",
      "(Device 43/Epoch 3) Train Loss: 0.199 | Train Acc: 92.960 | Test Loss: 0.489 | Test Acc: 84.760\n",
      "\n",
      "Diff: 93.21798706054688\n",
      "\n",
      "Round time: 17041.293228149414 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  118\n",
      "(Device 18/Epoch 3) Train Loss: 0.189 | Train Acc: 92.960 | Test Loss: 0.471 | Test Acc: 86.080\n",
      "\n",
      "Diff: 68.53842163085938\n",
      "\n",
      "Round time: 19962.124579906464 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  119\n",
      "(Device 29/Epoch 3) Train Loss: 0.241 | Train Acc: 91.660 | Test Loss: 0.479 | Test Acc: 86.490\n",
      "\n",
      "Diff: 95.97039031982422\n",
      "\n",
      "Round time: 20573.600088834763 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  120\n",
      "(Device 9/Epoch 3) Train Loss: 0.239 | Train Acc: 91.9000 | Test Loss: 0.464 | Test Acc: 86.320\n",
      "\n",
      "Diff: 108.36822509765625\n",
      "\n",
      "Round time: 21168.965790987015 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  121\n",
      "(Device 46/Epoch 3) Train Loss: 0.342 | Train Acc: 88.540 | Test Loss: 0.449 | Test Acc: 86.450\n",
      "\n",
      "Diff: 110.08718872070312\n",
      "\n",
      "Round time: 21768.884684085846 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  122\n",
      "(Device 33/Epoch 3) Train Loss: 0.154 | Train Acc: 94.960 | Test Loss: 0.427 | Test Acc: 87.520\n",
      "\n",
      "Diff: 84.21282958984375\n",
      "\n",
      "Round time: 22361.200109004974 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  123\n",
      "(Device 14/Epoch 3) Train Loss: 0.159 | Train Acc: 94.340 | Test Loss: 0.455 | Test Acc: 87.290\n",
      "\n",
      "Diff: 89.75833892822266\n",
      "\n",
      "Round time: 24335.490816116333 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  124\n",
      "(Device 2/Epoch 3) Train Loss: 0.158 | Train Acc: 94.7000 | Test Loss: 0.490 | Test Acc: 86.200\n",
      "\n",
      "Diff: 85.81427764892578\n",
      "\n",
      "Round time: 24922.286353826523 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  125\n",
      "(Device 16/Epoch 3) Train Loss: 0.191 | Train Acc: 94.200 | Test Loss: 0.390 | Test Acc: 88.490\n",
      "\n",
      "Diff: 20.6744384765625\n",
      "\n",
      "Round time: 26527.688716173172 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  126\n",
      "(Device 13/Epoch 3) Train Loss: 0.153 | Train Acc: 95.120 | Test Loss: 0.377 | Test Acc: 88.880\n",
      "\n",
      "Diff: 20.290822982788086\n",
      "\n",
      "Round time: 32456.793838977814 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  127\n",
      "(Device 42/Epoch 3) Train Loss: 0.161 | Train Acc: 94.800 | Test Loss: 0.372 | Test Acc: 88.670\n",
      "\n",
      "Diff: 18.645763397216797\n",
      "\n",
      "Round time: 40393.617008924484 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  128\n",
      "(Device 27/Epoch 3) Train Loss: 0.145 | Train Acc: 95.000 | Test Loss: 0.374 | Test Acc: 88.830\n",
      "\n",
      "Diff: 18.296842575073242\n",
      "\n",
      "Round time: 40989.612004995346 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  129\n",
      "(Device 1/Epoch 3) Train Loss: 0.140 | Train Acc: 94.98000 | Test Loss: 0.375 | Test Acc: 88.680\n",
      "\n",
      "Diff: 25.800113677978516\n",
      "\n",
      "Round time: 41589.1444401741 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 41592.47633099556 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoid_against_noise_attack = run_federated_test(agg_fn = sigmoid_aggregation,                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = big_noise_attack, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"sigmoid_against_noise_attack_1.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 46/Epoch 0) Train Loss: 0.334 | Train Acc: 88.189"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/2894354327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m determine_cutoffs = run_federated_test(agg_fn = sigmoid_aggregation,                    \n\u001b[0m\u001b[1;32m      2\u001b[0m                                          \u001b[0mrounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# go 30 pounds past where the baseline left off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                          \u001b[0mlocal_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# all else the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          \u001b[0mnum_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0mdevice_pct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/2375901062.py\u001b[0m in \u001b[0;36mrun_federated_test\u001b[0;34m(agg_fn, rounds, local_epochs, num_devices, device_pct, data_pct, net, evil_round, attacker_strategy, evil_device_id, evaluate_attack, output_filename, snapshot, resume_from_snap)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mround_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlocal_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# One device becomes evil if required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_26277/275447899.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, device, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/cs187/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "determine_cutoffs = run_federated_test(agg_fn = sigmoid_aggregation,                    \n",
    "                                         rounds = 101, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = None, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = None, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = None,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"sigmoid_against_noise_attack_1.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_sigmoid_on_cutoff(a,c=0.005, left = False):\n",
    "#     def f(x):\n",
    "#         if(not left and x > (a + 2)):\n",
    "#             return 0\n",
    "#         if(left and x < (a - 2)):\n",
    "#             return 0\n",
    "#         exponent = (-(x-a))/(c if left else -1*c)\n",
    "#         return 1/(1 + (math.e**(exponent)))\n",
    "#     return f \n",
    "\n",
    "# def make_double_sigmoid(a,b):\n",
    "#     left = make_sigmoid_on_cutoff(a, left = True)\n",
    "#     right = make_sigmoid_on_cutoff(b, left = False)\n",
    "#     def f(x):\n",
    "#         return left(x)*right(x)\n",
    "#     return f\n",
    "# baseline2.devices[0]['net']\n",
    "\n",
    "def make_test_device(trainset):\n",
    "    data_idxs = iid_sampler(trainset, 1, .1)\n",
    "    return create_device(ConvNet().to(mps), 0, trainset, data_idxs[0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_noise_attack(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device['net'].state_dict()['model.0.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [x[1] for x in trainset if x[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "swapping\n",
      "\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def swap_classes_dataset(s, a, b):\n",
    "    s = list(s)\n",
    "    for i in range(len(s)):\n",
    "        if s[i][1] == a:\n",
    "            s[i] = s[i][0],b\n",
    "        if s[i][1] == b:\n",
    "            s[i] = s[i][0],a \n",
    "    \n",
    "    return s\n",
    "\n",
    "# Make switch classes attack\n",
    "def switch_classes_attack(a,b, train_epochs):\n",
    "    transform_train = transforms.Compose([                                   \n",
    "        transforms.RandomCrop(32, padding=4),                                       \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    evil_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "    evil_trainset = swap_classes_dataset(evil_trainset, a,b)\n",
    "    print(\"swapping\\n\")\n",
    "    \n",
    "    # Load testing data\n",
    "    transform_test = transforms.Compose([                                           \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    evil_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True,\n",
    "                                           transform=transform_test)\n",
    "    \n",
    "    evil_testset = swap_classes_dataset(evil_testset, a,b)\n",
    "    \n",
    "    evil_testloader = torch.utils.data.DataLoader(evil_testset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    attack_idx = iid_sampler(evil_trainset, 1, 1)[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    evil_device_trainset = DatasetSplit(evil_trainset, attack_idx)\n",
    "    evil_device_trainloader = torch.utils.data.DataLoader(evil_device_trainset,\n",
    "                                                     batch_size=128,\n",
    "                                                     shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def attack(device):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights.\n",
    "        '''\n",
    "        device['dataloader'] = evil_device_trainloader\n",
    "\n",
    "        for local_epoch in range(train_epochs):\n",
    "            train(local_epoch, device, criterion)\n",
    "        \n",
    "        print(\"Confirm the attack worked. (This should be high)\")  \n",
    "        test(0, device, criterion,testloader =  evil_testloader)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    return attack\n",
    "\n",
    "sca = switch_classes_attack(0,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "swapping\n",
      "\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Now let's make an instance of this attack that performs 25 epochs of training on the evil device with the misclassified data.\n",
    "# Specifically, we switch cars and airplanes\n",
    "sca2 = switch_classes_attack(0,1,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 30/Epoch 3) Train Loss: 0.448 | Train Acc: 84.700 | Test Loss: 0.389 | Test Acc: 87.390\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 218.5696198940277 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  101\n",
      "(Device 3/Epoch 3) Train Loss: 0.327 | Train Acc: 88.8800 | Test Loss: 0.382 | Test Acc: 87.490\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 425.6108150482178 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  102\n",
      "(Device 14/Epoch 3) Train Loss: 0.222 | Train Acc: 92.600 | Test Loss: 0.408 | Test Acc: 86.950\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 632.2491769790649 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  103\n",
      "(Device 25/Epoch 3) Train Loss: 0.335 | Train Acc: 88.440 | Test Loss: 0.419 | Test Acc: 86.930\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 830.303952217102 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  104\n",
      "(Device 40/Epoch 3) Train Loss: 0.231 | Train Acc: 93.040 | Test Loss: 0.383 | Test Acc: 87.780\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1036.1185810565948 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  105\n",
      "(Device 42/Epoch 3) Train Loss: 0.255 | Train Acc: 91.180 | Test Loss: 0.392 | Test Acc: 87.840\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1244.9114549160004 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  106\n",
      "(Device 38/Epoch 3) Train Loss: 0.280 | Train Acc: 90.120 | Test Loss: 0.390 | Test Acc: 87.540\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1455.037827014923 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  107\n",
      "(Device 48/Epoch 3) Train Loss: 0.292 | Train Acc: 89.680 | Test Loss: 0.370 | Test Acc: 88.290\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1667.519868850708 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  108\n",
      "(Device 42/Epoch 3) Train Loss: 0.345 | Train Acc: 87.840 | Test Loss: 0.369 | Test Acc: 88.460\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1876.9336681365967 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  109\n",
      "(Device 18/Epoch 3) Train Loss: 0.186 | Train Acc: 93.520 | Test Loss: 0.377 | Test Acc: 88.280\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2105.277766227722 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  110\n",
      "(Device 14/Epoch 3) Train Loss: 0.225 | Train Acc: 92.440Attacking!\n",
      "\n",
      "(Device 2/Epoch 24) Train Loss: 0.013 | Train Acc: 99.5780Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.855 | Test Acc: 85.830\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.376 | Test Acc: 88.340\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3514.256497144699 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  111\n",
      "(Device 49/Epoch 3) Train Loss: 0.228 | Train Acc: 92.280 | Test Loss: 0.363 | Test Acc: 88.500\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3872.9786880016327 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  112\n",
      "(Device 19/Epoch 3) Train Loss: 0.176 | Train Acc: 94.480 | Test Loss: 0.369 | Test Acc: 88.470\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4388.051183223724 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  113\n",
      "(Device 12/Epoch 3) Train Loss: 0.181 | Train Acc: 93.920 | Test Loss: 0.368 | Test Acc: 88.760\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4672.941226005554 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  114\n",
      "(Device 38/Epoch 3) Train Loss: 0.166 | Train Acc: 94.120 | Test Loss: 0.368 | Test Acc: 88.920\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4955.253675937653 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  115\n",
      "(Device 34/Epoch 3) Train Loss: 0.158 | Train Acc: 94.380 | Test Loss: 0.365 | Test Acc: 89.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5454.384433269501 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  116\n",
      "(Device 42/Epoch 3) Train Loss: 0.237 | Train Acc: 93.120 | Test Loss: 0.349 | Test Acc: 89.640\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5803.5099267959595 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  117\n",
      "(Device 40/Epoch 3) Train Loss: 0.209 | Train Acc: 92.960 | Test Loss: 0.355 | Test Acc: 88.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6158.822886943817 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  118\n",
      "(Device 12/Epoch 3) Train Loss: 0.298 | Train Acc: 90.6200 | Test Loss: 0.376 | Test Acc: 89.040\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6527.806505918503 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  119\n",
      "(Device 46/Epoch 3) Train Loss: 0.197 | Train Acc: 93.1000 | Test Loss: 0.364 | Test Acc: 89.170\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6739.291934967041 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  120\n",
      "(Device 13/Epoch 3) Train Loss: 0.243 | Train Acc: 91.7600 | Test Loss: 0.358 | Test Acc: 89.550\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6952.29815196991 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  121\n",
      "(Device 15/Epoch 3) Train Loss: 0.188 | Train Acc: 93.9600 | Test Loss: 0.350 | Test Acc: 89.550\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7167.842609167099 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  122\n",
      "(Device 7/Epoch 3) Train Loss: 0.195 | Train Acc: 94.0000 | Test Loss: 0.347 | Test Acc: 89.590\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7389.792389154434 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  123\n",
      "(Device 34/Epoch 3) Train Loss: 0.107 | Train Acc: 96.5800 | Test Loss: 0.354 | Test Acc: 89.690\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7609.542908191681 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  124\n",
      "(Device 0/Epoch 3) Train Loss: 0.172 | Train Acc: 94.1200 | Test Loss: 0.381 | Test Acc: 89.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7972.639851093292 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  125\n",
      "(Device 37/Epoch 3) Train Loss: 0.071 | Train Acc: 97.7000 | Test Loss: 0.347 | Test Acc: 90.250\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 8186.583430051804 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  126\n",
      "(Device 49/Epoch 3) Train Loss: 0.076 | Train Acc: 97.6600 | Test Loss: 0.347 | Test Acc: 90.240\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 8406.754409074783 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  127\n",
      "(Device 13/Epoch 3) Train Loss: 0.071 | Train Acc: 98.1800 | Test Loss: 0.349 | Test Acc: 90.220\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 8638.34443116188 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  128\n",
      "(Device 26/Epoch 3) Train Loss: 0.067 | Train Acc: 97.8200 | Test Loss: 0.350 | Test Acc: 90.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 8878.929777145386 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  129\n",
      "(Device 24/Epoch 3) Train Loss: 0.071 | Train Acc: 98.1400 | Test Loss: 0.352 | Test Acc: 90.350\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 9122.999819993973 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 9140.565530061722 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_no_defense = run_federated_test(                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca2, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_no_defense.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_classes_no_defense = load_result(\"switch_classes_no_defense.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.2,\n",
       " 44.42,\n",
       " 54.68,\n",
       " 60.89,\n",
       " 65.69,\n",
       " 69.14,\n",
       " 71.45,\n",
       " 73.38,\n",
       " 76.61,\n",
       " 77.53,\n",
       " 79.7,\n",
       " 77.91,\n",
       " 79.95,\n",
       " 81.38,\n",
       " 82.4,\n",
       " 82.86,\n",
       " 83.43,\n",
       " 83.6,\n",
       " 84.71,\n",
       " 85.13,\n",
       " 85.46,\n",
       " 85.9,\n",
       " 85.79,\n",
       " 86.54,\n",
       " 86.48,\n",
       " 87.19,\n",
       " 87.33,\n",
       " 87.52,\n",
       " 87.52,\n",
       " 87.37,\n",
       " 87.48,\n",
       " 87.65,\n",
       " 87.62,\n",
       " 87.56,\n",
       " 87.58,\n",
       " 87.7,\n",
       " 87.63,\n",
       " 87.62,\n",
       " 87.8,\n",
       " 87.76,\n",
       " 87.92,\n",
       " 87.82,\n",
       " 87.87,\n",
       " 87.91,\n",
       " 87.96,\n",
       " 87.89,\n",
       " 88.11,\n",
       " 87.85,\n",
       " 87.89,\n",
       " 88.12,\n",
       " 88.09,\n",
       " 87.96,\n",
       " 88.01,\n",
       " 87.94,\n",
       " 88.05,\n",
       " 88.0,\n",
       " 87.96,\n",
       " 88.07,\n",
       " 88.05,\n",
       " 88.0,\n",
       " 88.17,\n",
       " 88.07,\n",
       " 88.23,\n",
       " 88.07,\n",
       " 88.08,\n",
       " 87.99,\n",
       " 88.1,\n",
       " 88.11,\n",
       " 88.13,\n",
       " 88.12,\n",
       " 88.09,\n",
       " 88.12,\n",
       " 88.08,\n",
       " 88.12,\n",
       " 88.13,\n",
       " 88.11,\n",
       " 88.06,\n",
       " 88.12,\n",
       " 88.07,\n",
       " 88.15,\n",
       " 88.1,\n",
       " 88.06,\n",
       " 88.16,\n",
       " 88.12,\n",
       " 88.14,\n",
       " 88.14,\n",
       " 88.18,\n",
       " 88.12,\n",
       " 88.11,\n",
       " 88.09,\n",
       " 88.07,\n",
       " 88.08,\n",
       " 88.16,\n",
       " 88.09,\n",
       " 88.23,\n",
       " 88.22,\n",
       " 88.13,\n",
       " 88.04,\n",
       " 88.13,\n",
       " 88.05,\n",
       " 87.39,\n",
       " 87.49,\n",
       " 86.95,\n",
       " 86.93,\n",
       " 87.78,\n",
       " 87.84,\n",
       " 87.54,\n",
       " 88.29,\n",
       " 88.46,\n",
       " 88.28,\n",
       " 88.34,\n",
       " 88.5,\n",
       " 88.47,\n",
       " 88.76,\n",
       " 88.92,\n",
       " 89.23,\n",
       " 89.64,\n",
       " 88.94,\n",
       " 89.04,\n",
       " 89.17,\n",
       " 89.55,\n",
       " 89.55,\n",
       " 89.59,\n",
       " 89.69,\n",
       " 89.05,\n",
       " 90.25,\n",
       " 90.24,\n",
       " 90.22,\n",
       " 90.33,\n",
       " 90.35]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_classes_no_defense.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_device = make_test_device(trainset)\n",
    "test_device['net'].load_state_dict(switch_classes_no_defense.avg_weight_history[112])\n",
    "# def test(epoch, device, criterion, testloader = testloader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 0.368 | Test Acc: 88.560\n"
     ]
    }
   ],
   "source": [
    "test(0, test_device, nn.CrossEntropyLoss(), testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_test_set(s, labels):\n",
    "    s = list(s)\n",
    "    s = [s for s in s if s[1] in labels]\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def make_testloader_subset(labels):\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "    \n",
    "    restricted_test_set = limit_test_set(testset, labels)\n",
    "    print(len(restricted_test_set))\n",
    "#     restricted_test_set = swap_classes_dataset(restricted_test_set, 0, 1)\n",
    "    \n",
    "    restricted_testloader = torch.utils.data.DataLoader(restricted_test_set, batch_size=128, shuffle=False)\n",
    "    return restricted_testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "checker_test_set = make_testloader_subset([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 0.392 | Test Acc: 89.400\n"
     ]
    }
   ],
   "source": [
    "test(0, test_device, nn.CrossEntropyLoss(), checker_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs187",
   "language": "python",
   "name": "cs187"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
