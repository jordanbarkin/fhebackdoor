{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "from mpmath import mp\n",
    "from scipy import optimize as opt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm # for 3d poltting\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D # for 3d poltting\n",
    "from sympy import symbols, cos, pi\n",
    "from sympy.core.numbers import One\n",
    "from sympy.utilities.lambdify import lambdify\n",
    "from math import sin, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chebyshev Approximations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicSigmoids:\n",
    "    @staticmethod\n",
    "    def sigmoid(x, a, c):\n",
    "        exponent = (-(x-a))/c\n",
    "        denom = 1 + (math.e**(exponent))\n",
    "        return 1/denom\n",
    "    @staticmethod\n",
    "    def square(x,a,b,c):\n",
    "        s1=BasicSigmoids.sigmoid(x, a, c)\n",
    "        s2=BasicSigmoids.sigmoid(x, b, -c)\n",
    "        return s1*s2\n",
    "    @staticmethod\n",
    "    def scale_up(z,x_min,x_max):\n",
    "        \"\"\"\n",
    "        Scales up z \\in [-1,1] to x \\in [x_min,x_max]\n",
    "        where z = (2 * (x - x_min) / (x_max - x_min)) - 1\n",
    "        \"\"\"\n",
    "\n",
    "        return x_min + (z + 1) * (x_max - x_min) / 2\n",
    "    @staticmethod\n",
    "    def scale_down(x,x_min,x_max):\n",
    "        \"\"\"\n",
    "        Scales down x \\in [x_min,x_max] to z \\in [-1,1]\n",
    "        where z = f(x) = (2 * (x - x_min) / (x_max - x_min)) - 1\n",
    "        \"\"\"    \n",
    "\n",
    "        return (2 * (x - x_min) / (x_max - x_min)) - 1\n",
    "\n",
    "class ChebyshevApproximator:\n",
    "    def __init__(self, function, intr, deg, prec = 15, extra_shift=1.0):\n",
    "        self.extra_shift = extra_shift\n",
    "        n = deg + 1\n",
    "\n",
    "        x, u = symbols('x u')\n",
    "\n",
    "        a, b = intr\n",
    "        x_to_u = (2 * x - a - b) / (b - a)\n",
    "        u_to_x = (b - a) / 2 * u + (a + b) / 2\n",
    "        chebyshev_nodes = cos((symbols('i') + 0.5) / n * pi)\n",
    "\n",
    "        result_u = [chebyshev_nodes.evalf(prec, subs={'i': i}) for i in range(n)]\n",
    "        result_x = [u_to_x.evalf(prec, subs={u: i}) for i in result_u]\n",
    "        result_y = [function(i) for i in result_x]\n",
    "\n",
    "        t = [One(), u]\n",
    "\n",
    "        for _ in range(n - 2):\n",
    "            t.append(2 * u * t[-1] - t[-2])\n",
    "\n",
    "        c = [sum(result_y) / n]\n",
    "\n",
    "        for index in range(1, n):\n",
    "            c.append(2 * sum(t[index].evalf(prec, subs={u: i}) * j for i, j in zip(result_u, result_y)) / n)\n",
    "\n",
    "        y = 1 * c[0]\n",
    "\n",
    "        for i in range(1, n):\n",
    "            y += t[i] * c[i]\n",
    "\n",
    "        y = y.subs({u: x_to_u}).simplify()\n",
    "\n",
    "        self.y = y        \n",
    "        x, u = symbols('x u')    \n",
    "        self.f = lambdify(x, self.y, \"numpy\")\n",
    "        self.f.formula = y\n",
    "    \n",
    "    def _get_y(self):\n",
    "        return self.y\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.f(x)*self.extra_shift\n",
    "    \n",
    "    def get_coeffs(self):\n",
    "        def cheby_to_coeff_list(s): \n",
    "            s_coeffs =  [((t.args[1].args[1] if (len(t.args[1].args) != 0)else 1), t.args[0]) for t in s.args[1:]] + [(0, s.args[0])]\n",
    "            return [x[1] for x in sorted(s_coeffs)]\n",
    "\n",
    "        return cheby_to_coeff_list(self.y)\n",
    "\n",
    "class HatChebyshevApproximator(ChebyshevApproximator):\n",
    "    def __init__(self,a,b,c = 0.005,deg = 10,left = -5,right = 5, extra_shift = 1.001):\n",
    "        def hat(x):\n",
    "            return BasicSigmoids.square(x,a,b,c)\n",
    "        \n",
    "        super().__init__(hat, (left,right),deg, prec=15, extra_shift=extra_shift)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure GPU (change if not M1 mac)\n",
    "mps = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "\n",
    "# Using CIFAR-10 again as in the programming assignments\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving And helpers\n",
    "\n",
    "def save_tracker(tracker, path):\n",
    "  np.savetxt(path, tracker, delimiter=',') \n",
    "\n",
    "def save_trackers(device, filename):\n",
    "  \"\"\"Save all trackers and current total_time to a file.\"\"\"\n",
    "  torch.save((device['train_loss_tracker'], device['train_acc_tracker'], device['test_loss_tracker'], device['test_acc_tracker'], total_time), filename)\n",
    "  print(\"Saved trackers to \" + filename)\n",
    "\n",
    "def moving_average(a, n=100):\n",
    "  '''Helper function used for visualization'''\n",
    "  ret = torch.cumsum(torch.Tensor(a), 0)\n",
    "  ret[n:] = ret[n:] - ret[:-n]\n",
    "  return ret[n - 1:] / n\n",
    "    \n",
    "def make_plot_better(trackers, num_epochs, title, y_axis_lab, should_average=False, n = 100, add_attack_region = None, alpha = .2):\n",
    "  avg_fn = (lambda x : moving_average(x, n)) if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "  _ = plt.legend()\n",
    "  sample_tracker = list(trackers.items())[0][1]\n",
    "  if add_attack_region is not None: \n",
    "      ax.fill_between(range(add_attack_region[0], add_attack_region[1]), 0, 100, alpha=alpha,  color='red')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML / FL Utils (Largely borrowed from programming assigment 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "def iid_sampler(dataset, num_devices, data_pct):\n",
    "    total_samples = len(dataset)\n",
    "    sampled = {}\n",
    "    number_samples = int((data_pct)*(total_samples)) \n",
    "\n",
    "    for i in range(num_devices):\n",
    "      sampled[i] = random.sample(range(total_samples), number_samples)\n",
    "        \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net definitions\n",
    "\n",
    "# Same ConvNet as in Assignment 2 and 3\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_devices_for_round(devices, device_pct):  \n",
    "    return random.sample(devices, int(device_pct * len(devices)))\n",
    "\n",
    "def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n",
    "                  milestones=None, batch_size=128):\n",
    "    if milestones == None:\n",
    "        milestones = [25, 50, 75]\n",
    "\n",
    "    device_net = copy.deepcopy(net)\n",
    "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=milestones,\n",
    "                                                     gamma=0.1)\n",
    "    device_trainset = DatasetSplit(trainset, data_idxs)\n",
    "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "    return {\n",
    "        'net': device_net,\n",
    "        'id': device_id,\n",
    "        'dataloader': device_trainloader, \n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_tracker': [],\n",
    "        'train_acc_tracker': [],\n",
    "        'test_loss_tracker': [],\n",
    "        'test_acc_tracker': [],\n",
    "        }\n",
    "\n",
    "def make_test_device(trainset):\n",
    "    data_idxs = iid_sampler(trainset, 1, .1)\n",
    "    return create_device(ConvNet().to(mps), 0, trainset, data_idxs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local device training and testing\n",
    "def train(epoch, device, criterion):\n",
    "    device['net'].train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
    "        inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "        device['optimizer'].zero_grad()\n",
    "        outputs = device['net'](inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        device['optimizer'].step()\n",
    "        train_loss += loss.item()\n",
    "        device['train_loss_tracker'].append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        dev_id = device['id']\n",
    "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "                         f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    device['train_acc_tracker'].append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(epoch, device, criterion, testloader = testloader):\n",
    "    device['net'].eval()\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "            outputs = device['net'](inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            device['test_loss_tracker'].append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()  \n",
    "    acc = 100.*correct/total\n",
    "    device['test_acc_tracker'].append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FL Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline `average' function. \n",
    "def average_weights(devices,*args, **kwargs):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function tests (the main experiment routine)\n",
    "\n",
    "# Given two different sets of aggregated weights, \n",
    "# gives a value representing the difference between them,\n",
    "# as a way to measure the direct cost of using our aggregated function\n",
    "def diff_aggregated_weights(strat, baseline):\n",
    "    result = 0 \n",
    "    for k in strat.keys():\n",
    "        result += torch.linalg.norm(strat[k] - baseline[k])\n",
    "    return result\n",
    "\n",
    "# A data type for experiment results\n",
    "BackdoorResult = namedtuple(\"BackdoorResult\", [\"scheme_loss\", \"test_accuracy\", \"backdoor_success\", \"devices\", \"avg_weight_history\"])\n",
    "\n",
    "def run_federated_test(agg_fn = average_weights,   # Pass in aggregation function, \n",
    "                                                   #     device list -> aggregated weights \n",
    "                       rounds = 10,                # Rounds of FL\n",
    "                       local_epochs = 4,           # Epochs per device                      \n",
    "                       num_devices = 50,           # Total # devices\n",
    "                       device_pct = 0.2,           # % of devices per round\n",
    "                       data_pct = 0.1,             # % of data each device gets\n",
    "                       net = ConvNet().to(mps),    # Network object; make sure on mps backend\n",
    "                       evil_round = None,          # integer list option; attacker will mount attack during these rounds\n",
    "                       attacker_strategy = None,   # device -> void; Sets up attacker weight update on device \n",
    "                       evil_device_id = None,      # Which device id should perform the attack\n",
    "                       evaluate_attack = None,     # Evaluated on the set of devices after each round and logged.\n",
    "                       output_filename = None,     # Write results to disk at this path\n",
    "                       snapshot = True,            # Snapshot to disk after each round\n",
    "                       resume_from_snap = None,    # Optionally resume from a prior snapshot.\n",
    "                       multiple_attack_rounds = [] # Specify multiple attack rounds instead of one, as above.\n",
    "                      ):   \n",
    "    '''\n",
    "    Returns instance of BackdoorResult, with following state: \n",
    "        scheme_loss        : difference between a straight average and aggregated weights after each round\n",
    "        test_accuracy      : classification accuracy after each round\n",
    "        backdoor_success   : result of evaluate_attack(devices) after each round\n",
    "        devices            : snapshot of devices after each round\n",
    "        avg_weight_history : snapshot of central server weights after each round\n",
    "    '''\n",
    "    \n",
    "    def lighten_device(d):\n",
    "        return {\n",
    "            k: d[k] for k in ( 'id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker')\n",
    "        }                                   \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "    devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "    \n",
    "    scheme_loss = []\n",
    "    avg_weight_history = []\n",
    "    _starting_round_num = 0\n",
    "    \n",
    "    if resume_from_snap: \n",
    "        print(\"Resuming from snapshot!\\n\")\n",
    "        # Load what we can\n",
    "        result = resume_from_snap\n",
    "        scheme_loss = result.scheme_loss\n",
    "        avg_weight_history = result.avg_weight_history\n",
    "        partial_devices = result.devices\n",
    "\n",
    "        # Fresh data and devices\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "        devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "\n",
    "        # Restore the devices with the info from the trackers\n",
    "        def restore_device(old_device, new_device):\n",
    "            for k in ('id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker'):\n",
    "                new_device[k] = old_device[k]\n",
    "            return new_device\n",
    "        \n",
    "        print(\"Restoring devices\\n\")\n",
    "        devices = [restore_device(partial_devices[i], devices[i]) for i in range(len(devices))]\n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(avg_weight_history[-1])\n",
    "        \n",
    "        _starting_round_num = len(result.test_accuracy)\n",
    "        print(\"Finished restoring\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "    ## IID Federated Learning\n",
    "    start_time = time.time()\n",
    "    for round_num in range(_starting_round_num, rounds):\n",
    "        round_start_time = time.time()\n",
    "        # Part 1.3: Implement getting devices for each round here\n",
    "        round_devices = get_devices_for_round(devices, device_pct)\n",
    "\n",
    "        print('Round: ', round_num)\n",
    "        # Train locally \n",
    "        for device in round_devices:\n",
    "            for local_epoch in range(local_epochs):\n",
    "                train(local_epoch, device, criterion)\n",
    "        \n",
    "        # One device becomes evil if required\n",
    "        if ((evil_round and round_num == evil_round) or round_num in multiple_attack_rounds):\n",
    "            assert (evil_device_id is not None)\n",
    "            assert (attacker_strategy is not None)\n",
    "            print(\"Attacking!\\n\")\n",
    "            \n",
    "            attacker_strategy(devices[evil_device_id])\n",
    "            # Make sure evil guy gets averaged in \n",
    "            if evil_device_id not in round_devices:\n",
    "                round_devices.append(devices[evil_device_id])\n",
    "            print(\"Finished attacking\\n\")\n",
    "            \n",
    "        \n",
    "        # Weight averaging\n",
    "        w_baseline = average_weights(round_devices)\n",
    "        w_avg = agg_fn(round_devices, (avg_weight_history[-1] if avg_weight_history != [] else None))\n",
    "        \n",
    "        # Track the difference between the two; should be 0 if straight average\n",
    "        scheme_loss.append((float(diff_aggregated_weights(w_avg, w_baseline))))\n",
    "        \n",
    "        avg_weight_history.append(copy.deepcopy(w_avg))\n",
    "        \n",
    "        # Gradients         \n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(w_avg)\n",
    "            device['optimizer'].zero_grad()\n",
    "            device['optimizer'].step()\n",
    "            device['scheduler'].step()\n",
    "\n",
    "        # test accuracy after aggregation\n",
    "        # device 0 is the unique device with all of the \n",
    "        # test accuracies and losses in its tracker\n",
    "        test(round_num, devices[0], criterion)\n",
    "        \n",
    "        print(f\"\\nDiff: {scheme_loss[-1]}\\n\")\n",
    "        print(f\"Round time: {time.time() - start_time} \\n\")\n",
    "        if snapshot:\n",
    "            intermediate_result = BackdoorResult(\n",
    "                scheme_loss = scheme_loss, \n",
    "                test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "                backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "                devices = [lighten_device(d) for d in devices], \n",
    "                avg_weight_history = avg_weight_history\n",
    "            )\n",
    "\n",
    "\n",
    "            if output_filename is not None: \n",
    "                print(\"Writing snapshot\\n\")\n",
    "                with open(f\"snapshot_{output_filename}\", 'wb') as file: \n",
    "                    pickle.dump(intermediate_result, file)\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    print('Total training time: {} seconds'.format(total_time))\n",
    "    \n",
    "    # Pack up everything we care about and the devices for good measure\n",
    "    result = BackdoorResult(\n",
    "        scheme_loss = scheme_loss, \n",
    "        test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "        backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "        devices = [lighten_device(d) for d in devices], \n",
    "        avg_weight_history = avg_weight_history\n",
    "    )\n",
    "\n",
    "    \n",
    "    if output_filename is not None: \n",
    "        with open(output_filename, 'wb') as file: \n",
    "            print(\"Writing file\\n\")\n",
    "            pickle.dump(result, file)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Load output files back into memory\n",
    "def load_result(filename):\n",
    "    with open(filename, 'rb') as file: \n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# results = run_federated_test(local_epochs=1, num_devices = 10, rounds = 2, output_filename = \"testout.pickle\")\n",
    "# # Can also load it back\n",
    "# results = load_result(\"snapshot_testout.pickle\")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1 (Not real experiments; just to demonstrate usage)\n",
    "# A silly attack that just sends massive weights all of magnitude 10000\n",
    "def sample_attack(device):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    weights = device[\"net\"].state_dict().copy()\n",
    "    \n",
    "    for w in weights.keys():\n",
    "        weights[w] = torch.full(weights[w].size(), 10000)\n",
    "    \n",
    "    device['net'].load_state_dict(weights)\n",
    "    \n",
    "# A demo aggregation function that squashes any weights with magnitude over 1000 (except on the first device but whatever)\n",
    "def super_smart_aggregation(devices):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights, excluding huge updates.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            if (torch.max(state_dicts[i][k].type(torch.float32)) <= 1000): \n",
    "                w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg\n",
    "\n",
    "# Here, we carry out the attack but use the ordinary average\n",
    "results_straight_avg = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline_trivial_attack.pickle\")  \n",
    "# Observe that it absolutely destroys our model\n",
    "# Now we do it again but with the aggregation that rejects huge updates\n",
    "results_reject_huge = run_federated_test(agg_fn = super_smart_aggregation,                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None,\n",
    "                                         output_filename = \"simple_attack_and_defense.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here, we compute a real baseline over 100 epochs and typical averaging. Good starting point for future \n",
    "# tests; can resume from this snapshot.\n",
    "# baseline = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "#                                          rounds = 100,              \n",
    "#                                          local_epochs = 4,                             \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.2,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evaluate_attack = None, \n",
    "#                                          output_filename = \"baseline.pickle\")  \n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "# For example, now can get things like\n",
    "# baseline.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can resume from a snapshot:\n",
    "# NOTE : keep all the basic params (local epochs, num devices, net, etc) the same\n",
    "# when resuming from a snapshot \n",
    "baseline2 = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 101,              \n",
    "                                         local_epochs = 4,                             \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline2.pickle\", \n",
    "                                         resume_from_snap = baseline )  # we pass in a results object here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defenses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can move on to a real defense. \n",
    "\n",
    "# We create factory functions that let us parametrize a double sigmoid using threshholds\n",
    "def torch_scaled_sigmoid_factory(a, c = 0.005, left = False):\n",
    "    \n",
    "    def f(x):\n",
    "        scaled = torch.sub(x, a)\n",
    "        divisor = c if left else -1*c\n",
    "        scaled = torch.div(scaled, divisor)\n",
    "        return torch.sigmoid(scaled)\n",
    "    \n",
    "    return f\n",
    "\n",
    "# This is our paper's ``ideal'' hat function\n",
    "def torch_double_sigmoid_factory(a,b,c = 0.005):\n",
    "    left = torch_scaled_sigmoid_factory(a, left = True)\n",
    "    right = torch_scaled_sigmoid_factory(b, left = False) \n",
    "    def f(x):\n",
    "        return (left(x)*right(x))\n",
    "    return f\n",
    "\n",
    "my_sigmoid = torch_double_sigmoid_factory(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_double_sigmoid_factory(-1,6)(torch.tensor(list(range(-5,10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it works\n",
    "torch_double_sigmoid_factory(-2,2)(torch.Tensor([-5,-4,-3,-2,-1.5,-1.4,1.3,-1,0,1,1.5,1.6,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a sigmoid defense,\n",
    "#     keys_to_range_mapping maps each layer name in the state dict to the sigmoid bounds for processing it\n",
    "#     All other layers just get identity function-ed\n",
    "#     Can provide the sigmoid function of your choice; this lets us switch to Cheby/minmax later\n",
    "def make_sigmoid_defense(keys_to_range_mapping, stickiness = 0, sigmoid_function_on_ab = torch_double_sigmoid_factory):\n",
    "    \n",
    "    sigmoid_dict = defaultdict(lambda : (lambda x: 1))\n",
    "    \n",
    "    for k,(a,b) in keys_to_range_mapping.items():\n",
    "        sigmoid_dict[k] = sigmoid_function_on_ab(a,b)\n",
    "    \n",
    "    def f(devices, previous_round_weights):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights preprocessed by sigmoid\n",
    "        '''\n",
    "        \n",
    "        state_dicts = [device['net'].state_dict() for device in devices]\n",
    "        sigmoid_result_per_device = [1]*len(devices)\n",
    "        # initialize w_avg to tensors from device 0\n",
    "        w_avg = copy.deepcopy(state_dicts[0])\n",
    "\n",
    "        # get the sigmoids\n",
    "        for i in range(len(devices)):\n",
    "            for k in w_avg.keys():\n",
    "                sig = sigmoid_dict[k]\n",
    "                new_weight = (state_dicts[i][k].type(torch.float32))\n",
    "                prev_round_x = previous_round_weights[k].type(torch.float32)  if (previous_round_weights is not None) else new_weight\n",
    "                r = sig(torch.sub(new_weight, prev_round_x))\n",
    "                sigmoid_result_per_device[i] *= ((torch.mean(r) if (type(r) not in [int, float]) else r)**5)\n",
    "        \n",
    "        for k in w_avg.keys():\n",
    "            sig = sigmoid_dict[k]\n",
    "            new_weight = w_avg[k].type(torch.float32)\n",
    "            w_avg[k] = new_weight*sigmoid_result_per_device[0]\n",
    "            \n",
    "            prev_round_x = previous_round_weights[k].type(torch.float32)  if (previous_round_weights is not None) else new_weight\n",
    "            for i in range(stickiness):\n",
    "                w_avg[k] += prev_round_x            \n",
    "\n",
    "        # for each model param\n",
    "        for k in w_avg.keys():\n",
    "            # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k], scaling by sig\n",
    "            for i in range(1, len(devices)):\n",
    "                new_weight = (state_dicts[i][k].type(torch.float32))\n",
    "                w_avg[k] += new_weight*sigmoid_result_per_device[i]\n",
    "                \n",
    "            # compute average\n",
    "            w_avg[k] /= float(sum(sigmoid_result_per_device) + stickiness)\n",
    "        return w_avg\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.0.0.weight',\n",
       " 'model.1.0.weight',\n",
       " 'model.2.0.weight',\n",
       " 'model.3.0.weight',\n",
       " 'model.4.0.weight',\n",
       " 'model.5.0.weight',\n",
       " 'model.6.0.weight',\n",
       " 'model.7.0.weight',\n",
       " 'model.8.0.weight']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the actual layer weights that we care about\n",
    "[x for x in baseline.avg_weight_history[-1].keys() if '.0.weight' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets set up a potential sigmoid defense\n",
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-1,1),\n",
    "    'model.1.0.weight' : (-0.3,0.3),\n",
    "    'model.2.0.weight' : (-0.3,0.3),\n",
    "    'model.3.0.weight' : (-0.3,0.3),\n",
    "    'model.4.0.weight' : (-0.3,0.3),\n",
    "    'model.5.0.weight' : (-0.3,0.3),\n",
    "    'model.6.0.weight' : (-0.3,0.3),\n",
    "    'model.7.0.weight' : (-0.1,0.1),\n",
    "    'model.8.0.weight' : (-0.1,0.1),\n",
    "}\n",
    "sigmoid_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=0, sigmoid_function_on_ab=HatChebyshevApproximator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make an real attack\n",
    "# Makes an attack that puts noise in all of target layers. \n",
    "# Noise is uniform over a,b\n",
    "def noise_attack_factory(target_layers, a,b):\n",
    "    def attack(device):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights.\n",
    "        '''\n",
    "        weights = device[\"net\"].state_dict().copy()\n",
    "\n",
    "        for w in weights.keys():\n",
    "            if w in target_layers:\n",
    "                weights[w] = (a - b) * torch.rand(weights[w].size()) + b\n",
    "\n",
    "        device['net'].load_state_dict(weights)\n",
    "    return attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for the swap classes attack\n",
    "def swap_classes_dataset(s, a, b):\n",
    "    s = list(s)\n",
    "    for i in range(len(s)):\n",
    "        if s[i][1] == a:\n",
    "            s[i] = s[i][0],b\n",
    "        elif s[i][1] == b:\n",
    "            s[i] = s[i][0],a \n",
    "    \n",
    "    return s\n",
    "\n",
    "def limit_test_set(s, labels):\n",
    "    s = list(s)\n",
    "    s = [s for s in s if s[1] in labels]\n",
    "    return s\n",
    "\n",
    "\n",
    "# Make switch classes attack\n",
    "def switch_classes_attack(a,b, train_epochs):\n",
    "    transform_train = transforms.Compose([                                   \n",
    "        transforms.RandomCrop(32, padding=4),                                       \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    evil_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "    print(len(evil_trainset))\n",
    "\n",
    "\n",
    "    evil_trainset = swap_classes_dataset(evil_trainset, a,b)\n",
    "    print(len(evil_trainset))\n",
    "\n",
    "\n",
    "    print(\"swapping and limiting\\n\")\n",
    "    \n",
    "    # Load testing data\n",
    "    transform_test = transforms.Compose([                                           \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    evil_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True,\n",
    "                                           transform=transform_test)\n",
    "    evil_testset = limit_test_set(evil_testset, [a,b])\n",
    "\n",
    "    \n",
    "    evil_testset = swap_classes_dataset(evil_testset, a,b)\n",
    "    \n",
    "    evil_testloader = torch.utils.data.DataLoader(evil_testset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    attack_idx = iid_sampler(evil_trainset, 1, 1)[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    evil_device_trainset = DatasetSplit(evil_trainset, attack_idx)\n",
    "    evil_device_trainloader = torch.utils.data.DataLoader(evil_device_trainset,\n",
    "                                                     batch_size=128,\n",
    "                                                     shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    logged_state_dict = None\n",
    "\n",
    "    class attack_class:\n",
    "        def __init__(self):\n",
    "            self.logged_state_dict = None\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights.\n",
    "        '''\n",
    "        def __call__(self, device):\n",
    "            device['dataloader'] = evil_device_trainloader\n",
    "\n",
    "            if self.logged_state_dict is not None:\n",
    "                print(\"Using memoized attack\\n\")\n",
    "                device['net'].load_state_dict(self.logged_state_dict)\n",
    "            else:\n",
    "                for local_epoch in range(train_epochs):\n",
    "                    train(local_epoch, device, criterion)\n",
    "                    test(0, device, criterion,testloader =  evil_testloader)\n",
    "\n",
    "\n",
    "            print(\"Confirm the attack worked. (This should be high)\")  \n",
    "            test(0, device, criterion,testloader =  evil_testloader)\n",
    "\n",
    "\n",
    "            self.logged_state_dict = copy.deepcopy(device['net'].state_dict())\n",
    "\n",
    "    return attack_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make an instance of this attack that performs 25 epochs of training on the evil device with the misclassified data.\n",
    "# Specifically, we switch cars and airplanes\n",
    "sca_01 = switch_classes_attack(0,1,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, time for the experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def limit_test_set(s, labels):\n",
    "    s = list(s)\n",
    "    s = [s for s in s if s[1] in labels]\n",
    "    return s\n",
    "\n",
    "\n",
    "def make_testloader_subset(labels, a, b):\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "    \n",
    "    restricted_test_set = limit_test_set(testset, labels)\n",
    "    restricted_test_set = swap_classes_dataset(restricted_test_set, a, b)\n",
    "    \n",
    "    restricted_testloader = torch.utils.data.DataLoader(restricted_test_set, batch_size=128, shuffle=False)\n",
    "    \n",
    "    return restricted_testloader\n",
    "\n",
    "# A tool used to help us find the empirical cutoffs (experimenting around not present in this notebook. )\n",
    "def weight_history_to_layer_max_magnitude_and_means(w_avg, include_batch_norm = True):\n",
    "    weight_keys = [x for x in w_avg.keys() if \".0.weight\" in x]\n",
    "    if include_batch_norm:\n",
    "        weight_keys += [x for x in w_avg.keys() if \".1.weight\" in x]\n",
    "    def max_magnitude(t):\n",
    "        return torch.max(torch.abs(t))\n",
    "    def mean_magnitude(t):\n",
    "        return torch.mean(torch.abs(t))\n",
    "    all_means = [mean_magnitude(w_avg[w]) for w in weight_keys]\n",
    "    all_max = [max_magnitude(w_avg[w]) for w in weight_keys]\n",
    "    return all_means, all_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline = load_result(\"baseline.pickle\")\n",
    "# switch_classes_no_defense = run_federated_test(                    \n",
    "#                                          rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "#                                          local_epochs = 4,  # all else the same                     \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.2,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "#                                          attacker_strategy = sca2, # device 2 will carry out big noise attack  \n",
    "#                                          evil_device_id = 2,     \n",
    "#                                          evaluate_attack = None, # we will evaluate manually afterwards \n",
    "#                                          output_filename = \"switch_classes_no_defense.pickle\",\n",
    "#                                          resume_from_snap = baseline, # pick up where baseline left off  \n",
    "#                                          snapshot = True ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "swapping\n",
      "\n",
      "Files already downloaded and verified\n",
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 44/Epoch 3) Train Loss: 0.285 | Train Acc: 90.480Attacking!\n",
      "\n",
      "(Device 2/Epoch 24) Train Loss: 0.014 | Train Acc: 99.5660Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.963 | Test Acc: 85.330\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.411 | Test Acc: 86.850\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1250.4205939769745 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  101\n",
      "(Device 15/Epoch 3) Train Loss: 0.311 | Train Acc: 89.640Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.255 | Test Acc: 78.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.404 | Test Acc: 86.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1435.8249740600586 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  102\n",
      "(Device 39/Epoch 3) Train Loss: 0.269 | Train Acc: 90.700Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.810 | Test Acc: 78.590\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.395 | Test Acc: 87.440\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1619.1456139087677 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  103\n",
      "(Device 26/Epoch 3) Train Loss: 0.269 | Train Acc: 90.600Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.669 | Test Acc: 82.810\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.433 | Test Acc: 86.750\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1956.4879717826843 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  104\n",
      "(Device 28/Epoch 3) Train Loss: 0.262 | Train Acc: 90.780Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.872 | Test Acc: 79.930\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.454 | Test Acc: 86.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2142.7575058937073 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  105\n",
      "(Device 49/Epoch 3) Train Loss: 0.208 | Train Acc: 92.940Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.681 | Test Acc: 84.050\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.457 | Test Acc: 86.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2479.3429250717163 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  106\n",
      "(Device 33/Epoch 3) Train Loss: 0.357 | Train Acc: 88.180Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.893 | Test Acc: 79.500\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.391 | Test Acc: 87.650\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2666.9391298294067 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  107\n",
      "(Device 43/Epoch 3) Train Loss: 0.242 | Train Acc: 91.540Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.757 | Test Acc: 78.930\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.380 | Test Acc: 88.490\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2851.377774000168 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  108\n",
      "(Device 7/Epoch 3) Train Loss: 0.245 | Train Acc: 91.6000Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.944 | Test Acc: 79.410\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.366 | Test Acc: 88.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3037.1944308280945 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  109\n",
      "(Device 39/Epoch 3) Train Loss: 0.277 | Train Acc: 90.760Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 2.009 | Test Acc: 79.570\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.359 | Test Acc: 88.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3224.3572149276733 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  110\n",
      "(Device 29/Epoch 3) Train Loss: 0.188 | Train Acc: 93.160Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.961 | Test Acc: 79.520\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.374 | Test Acc: 88.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3419.0711059570312 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  111\n",
      "(Device 36/Epoch 3) Train Loss: 0.257 | Train Acc: 91.040 | Test Loss: 0.355 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3606.8760719299316 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  112\n",
      "(Device 4/Epoch 3) Train Loss: 0.280 | Train Acc: 90.54000 | Test Loss: 0.361 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3792.78076171875 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  113\n",
      "(Device 29/Epoch 3) Train Loss: 0.178 | Train Acc: 93.660 | Test Loss: 0.357 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3987.803965806961 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  114\n",
      "(Device 44/Epoch 3) Train Loss: 0.172 | Train Acc: 94.860 | Test Loss: 0.370 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4351.923414945602 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  115\n",
      "(Device 39/Epoch 3) Train Loss: 0.117 | Train Acc: 96.060 | Test Loss: 0.359 | Test Acc: 89.340\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4539.46182179451 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  116\n",
      "(Device 9/Epoch 3) Train Loss: 0.147 | Train Acc: 95.0600 | Test Loss: 0.351 | Test Acc: 89.390\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4729.595947980881 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  117\n",
      "(Device 31/Epoch 3) Train Loss: 0.133 | Train Acc: 96.1200 | Test Loss: 0.356 | Test Acc: 89.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4929.9669868946075 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  118\n",
      "(Device 10/Epoch 3) Train Loss: 0.194 | Train Acc: 93.400 | Test Loss: 0.366 | Test Acc: 89.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5118.179821968079 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  119\n",
      "(Device 25/Epoch 3) Train Loss: 0.228 | Train Acc: 92.120 | Test Loss: 0.351 | Test Acc: 89.900\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5306.937247037888 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  120\n",
      "(Device 17/Epoch 3) Train Loss: 0.160 | Train Acc: 95.120 | Test Loss: 0.366 | Test Acc: 89.460\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5502.8072509765625 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  121\n",
      "(Device 7/Epoch 3) Train Loss: 0.115 | Train Acc: 95.8200 | Test Loss: 0.345 | Test Acc: 89.970\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5692.020018815994 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  122\n",
      "(Device 9/Epoch 3) Train Loss: 0.223 | Train Acc: 92.6000 | Test Loss: 0.361 | Test Acc: 89.340\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6040.297296762466 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  123\n",
      "(Device 42/Epoch 3) Train Loss: 0.085 | Train Acc: 97.3400 | Test Loss: 0.382 | Test Acc: 89.360\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6230.666080951691 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  124\n",
      "(Device 18/Epoch 3) Train Loss: 0.162 | Train Acc: 94.6400 | Test Loss: 0.349 | Test Acc: 89.790\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6418.991494894028 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  125\n",
      "(Device 1/Epoch 3) Train Loss: 0.096 | Train Acc: 96.9400 | Test Loss: 0.347 | Test Acc: 90.320\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6606.251834869385 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  126\n",
      "(Device 17/Epoch 3) Train Loss: 0.065 | Train Acc: 97.9600 | Test Loss: 0.349 | Test Acc: 90.280\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6948.046301841736 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  127\n",
      "(Device 19/Epoch 3) Train Loss: 0.073 | Train Acc: 97.740 | Test Loss: 0.353 | Test Acc: 90.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7138.234809875488 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  128\n",
      "(Device 34/Epoch 3) Train Loss: 0.074 | Train Acc: 97.8000 | Test Loss: 0.355 | Test Acc: 90.170\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7327.037197113037 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  129\n",
      "(Device 18/Epoch 3) Train Loss: 0.058 | Train Acc: 98.4400 | Test Loss: 0.357 | Test Acc: 90.190\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7516.31902384758 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 7520.757854938507 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sca_zero_one = switch_classes_attack(0,1,25)\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_no_defense = run_federated_test(                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_no_defense_multiple_final.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         multiple_attack_rounds = [100,101,102,103,104,105,106,107,108,109]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n",
      "50000\n",
      "swapping and limiting\n",
      "\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "sca_zero_one = switch_classes_attack(0,1,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 17/Epoch 3) Train Loss: 0.565 | Train Acc: 80.880Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9736, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.769 | Test Acc: 76.890\n",
      "\n",
      "Diff: 43904.1875\n",
      "\n",
      "Round time: 44.76120591163635 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  101\n",
      "(Device 37/Epoch 3) Train Loss: 0.301 | Train Acc: 89.220Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9982, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.587 | Test Acc: 82.210\n",
      "\n",
      "Diff: 43144.16796875\n",
      "\n",
      "Round time: 84.56357192993164 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  102\n",
      "(Device 42/Epoch 3) Train Loss: 0.262 | Train Acc: 91.160Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9992, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.883 | Test Acc: 76.680\n",
      "\n",
      "Diff: 42411.2578125\n",
      "\n",
      "Round time: 122.93768692016602 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  103\n",
      "(Device 45/Epoch 3) Train Loss: 0.313 | Train Acc: 90.020Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9981, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.791 | Test Acc: 78.110\n",
      "\n",
      "Diff: 41670.84765625\n",
      "\n",
      "Round time: 162.27637887001038 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  104\n",
      "(Device 0/Epoch 3) Train Loss: 0.292 | Train Acc: 90.400Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9989, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.544 | Test Acc: 82.970\n",
      "\n",
      "Diff: 40955.68359375\n",
      "\n",
      "Round time: 202.1529278755188 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  105\n",
      "(Device 9/Epoch 3) Train Loss: 0.267 | Train Acc: 91.000Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9993, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.547 | Test Acc: 82.980\n",
      "\n",
      "Diff: 40203.3515625\n",
      "\n",
      "Round time: 242.25624799728394 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  106\n",
      "(Device 10/Epoch 3) Train Loss: 0.224 | Train Acc: 92.280Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9993, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.655 | Test Acc: 81.410\n",
      "\n",
      "Diff: 39473.84765625\n",
      "\n",
      "Round time: 282.1021571159363 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  107\n",
      "(Device 0/Epoch 3) Train Loss: 0.212 | Train Acc: 92.700Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9983, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.891 | Test Acc: 77.070\n",
      "\n",
      "Diff: 38739.546875\n",
      "\n",
      "Round time: 324.58719301223755 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  108\n",
      "(Device 44/Epoch 3) Train Loss: 0.328 | Train Acc: 88.840Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9945, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.620 | Test Acc: 81.620\n",
      "\n",
      "Diff: 38007.1171875\n",
      "\n",
      "Round time: 383.292112827301 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  109\n",
      "(Device 19/Epoch 3) Train Loss: 0.272 | Train Acc: 91.320Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9962, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.537 | Test Acc: 82.890\n",
      "\n",
      "Diff: 37272.1328125\n",
      "\n",
      "Round time: 430.8833878040314 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  110\n",
      "(Device 24/Epoch 3) Train Loss: 0.259 | Train Acc: 90.680Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.501 | Test Acc: 91.000\n",
      "Finished attacking\n",
      "\n",
      "Sigmoids!:\n",
      "\n",
      "[tensor(0.9984, device='mps:0'), tensor(0., device='mps:0')]\n",
      " | Test Loss: 0.618 | Test Acc: 81.630\n",
      "\n",
      "Diff: 36529.6015625\n",
      "\n",
      "Round time: 468.7111179828644 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  111\n",
      "(Device 44/Epoch 3) Train Loss: 0.249 | Train Acc: 92.100Sigmoids!:\n",
      "\n",
      "[tensor(0.9953, device='mps:0')]\n",
      " | Test Loss: 0.642 | Test Acc: 81.980\n",
      "\n",
      "Diff: 0.017717713490128517\n",
      "\n",
      "Round time: 523.2677040100098 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  112\n",
      "(Device 25/Epoch 3) Train Loss: 0.222 | Train Acc: 92.200Sigmoids!:\n",
      "\n",
      "[tensor(0.9978, device='mps:0')]\n",
      " | Test Loss: 0.494 | Test Acc: 84.010\n",
      "\n",
      "Diff: 5.749988849856891e-05\n",
      "\n",
      "Round time: 570.7863759994507 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  113\n",
      "(Device 41/Epoch 3) Train Loss: 0.221 | Train Acc: 92.600Sigmoids!:\n",
      "\n",
      "[tensor(0.9982, device='mps:0')]\n",
      " | Test Loss: 0.696 | Test Acc: 80.660\n",
      "\n",
      "Diff: 2.218426379840821e-05\n",
      "\n",
      "Round time: 617.1846997737885 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  114\n",
      "(Device 35/Epoch 3) Train Loss: 0.230 | Train Acc: 92.360Sigmoids!:\n",
      "\n",
      "[tensor(0.9988, device='mps:0')]\n",
      " | Test Loss: 0.516 | Test Acc: 84.870\n",
      "\n",
      "Diff: 5.783790402347222e-05\n",
      "\n",
      "Round time: 663.602686882019 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  115\n",
      "(Device 9/Epoch 3) Train Loss: 0.284 | Train Acc: 90.560Sigmoids!:\n",
      "\n",
      "[tensor(0.9968, device='mps:0')]\n",
      " | Test Loss: 0.512 | Test Acc: 84.200\n",
      "\n",
      "Diff: 0.017698213458061218\n",
      "\n",
      "Round time: 717.1992099285126 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  116\n",
      "(Device 11/Epoch 3) Train Loss: 0.240 | Train Acc: 91.700Sigmoids!:\n",
      "\n",
      "[tensor(0.9979, device='mps:0')]\n",
      " | Test Loss: 0.482 | Test Acc: 84.790\n",
      "\n",
      "Diff: 8.252044062828645e-05\n",
      "\n",
      "Round time: 775.8817570209503 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  117\n",
      "(Device 26/Epoch 3) Train Loss: 0.250 | Train Acc: 92.540Sigmoids!:\n",
      "\n",
      "[tensor(0.9981, device='mps:0')]\n",
      " | Test Loss: 0.574 | Test Acc: 82.660\n",
      "\n",
      "Diff: 0.017717240378260612\n",
      "\n",
      "Round time: 825.7659068107605 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  118\n",
      "(Device 3/Epoch 3) Train Loss: 0.168 | Train Acc: 94.540Sigmoids!:\n",
      "\n",
      "[tensor(0.9990, device='mps:0')]\n",
      " | Test Loss: 0.495 | Test Acc: 85.250\n",
      "\n",
      "Diff: 4.05262726417277e-05\n",
      "\n",
      "Round time: 869.7029948234558 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  119\n",
      "(Device 43/Epoch 3) Train Loss: 0.288 | Train Acc: 90.940Sigmoids!:\n",
      "\n",
      "[tensor(0.9973, device='mps:0')]\n",
      " | Test Loss: 0.555 | Test Acc: 82.980\n",
      "\n",
      "Diff: 0.00010385242785559967\n",
      "\n",
      "Round time: 913.4623169898987 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  120\n",
      "(Device 43/Epoch 3) Train Loss: 0.123 | Train Acc: 96.220Sigmoids!:\n",
      "\n",
      "[tensor(0.9993, device='mps:0')]\n",
      " | Test Loss: 0.549 | Test Acc: 84.480\n",
      "\n",
      "Diff: 0.00011842787353089079\n",
      "\n",
      "Round time: 957.1641459465027 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  121\n",
      "(Device 15/Epoch 3) Train Loss: 0.227 | Train Acc: 91.780Sigmoids!:\n",
      "\n",
      "[tensor(0.9956, device='mps:0')]\n",
      " | Test Loss: 0.467 | Test Acc: 85.870\n",
      "\n",
      "Diff: 9.232526645064354e-05\n",
      "\n",
      "Round time: 993.9016718864441 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  122\n",
      "(Device 5/Epoch 3) Train Loss: 0.242 | Train Acc: 91.640Sigmoids!:\n",
      "\n",
      "[tensor(0.9967, device='mps:0')]\n",
      " | Test Loss: 0.547 | Test Acc: 83.880\n",
      "\n",
      "Diff: 7.60348339099437e-05\n",
      "\n",
      "Round time: 1033.794853925705 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  123\n",
      "(Device 25/Epoch 3) Train Loss: 0.184 | Train Acc: 94.200Sigmoids!:\n",
      "\n",
      "[tensor(0.9985, device='mps:0')]\n",
      " | Test Loss: 0.535 | Test Acc: 84.640\n",
      "\n",
      "Diff: 6.808064790675417e-05\n",
      "\n",
      "Round time: 1069.2004709243774 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  124\n",
      "(Device 25/Epoch 3) Train Loss: 0.151 | Train Acc: 94.840Sigmoids!:\n",
      "\n",
      "[tensor(0.9731, device='mps:0')]\n",
      " | Test Loss: 0.572 | Test Acc: 84.850\n",
      "\n",
      "Diff: 0.017666013911366463\n",
      "\n",
      "Round time: 1112.8931159973145 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  125\n",
      "(Device 4/Epoch 3) Train Loss: 0.234 | Train Acc: 92.620Sigmoids!:\n",
      "\n",
      "[tensor(1., device='mps:0')]\n",
      " | Test Loss: 0.471 | Test Acc: 86.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1154.470589876175 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  126\n",
      "(Device 29/Epoch 3) Train Loss: 0.239 | Train Acc: 91.540Sigmoids!:\n",
      "\n",
      "[tensor(1., device='mps:0')]\n",
      " | Test Loss: 0.423 | Test Acc: 87.580\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1200.7837460041046 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  127\n",
      "(Device 39/Epoch 3) Train Loss: 0.211 | Train Acc: 93.320Sigmoids!:\n",
      "\n",
      "[tensor(1., device='mps:0')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 0.406 | Test Acc: 87.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1248.7882480621338 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  128\n",
      "(Device 13/Epoch 3) Train Loss: 0.208 | Train Acc: 92.900Sigmoids!:\n",
      "\n",
      "[tensor(1., device='mps:0')]\n",
      " | Test Loss: 0.406 | Test Acc: 88.200\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1299.8816571235657 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  129\n",
      "(Device 48/Epoch 3) Train Loss: 0.178 | Train Acc: 93.940Sigmoids!:\n",
      "\n",
      "[tensor(1., device='mps:0')]\n",
      " | Test Loss: 0.393 | Test Acc: 88.190\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1360.254744052887 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 1378.6188621520996 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_sigmoid_defense_actually_train = run_federated_test(  agg_fn = sigmoid_aggregation, \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.02,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_sigmoid_defense_multiple_final_scratch_actually_train.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         multiple_attack_rounds = [100,101,102,103,104,105,106,107,108,109, 110]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.0.0.weight',\n",
       "              tensor([[[[ 7.4600e-01,  8.4820e-01,  4.7823e-01],\n",
       "                        [ 5.1573e-01,  1.0049e+00,  1.2566e+00],\n",
       "                        [-5.9251e-01, -3.1604e-01, -1.8427e-01]],\n",
       "              \n",
       "                       [[-5.3358e-01, -8.8284e-01, -5.9071e-01],\n",
       "                        [-1.0610e+00, -8.9894e-01,  4.1919e-01],\n",
       "                        [-1.0492e+00, -1.1256e+00, -3.9162e-01]],\n",
       "              \n",
       "                       [[ 1.2420e-01, -1.3487e-01,  8.3084e-04],\n",
       "                        [-1.8890e-01,  3.6324e-01,  1.1430e+00],\n",
       "                        [ 1.9642e-01,  6.0267e-01,  1.1399e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9969e-01,  3.3645e-01,  3.9868e-01],\n",
       "                        [-1.1443e+00, -3.7165e-01,  1.0054e+00],\n",
       "                        [-9.7183e-02, -2.2539e-02,  3.8900e-01]],\n",
       "              \n",
       "                       [[-4.9338e-01, -3.5729e-01,  2.5315e-01],\n",
       "                        [-1.0529e+00, -6.9401e-01,  9.6998e-01],\n",
       "                        [-1.2148e-01,  2.0725e-01,  6.5304e-01]],\n",
       "              \n",
       "                       [[-2.0915e-01, -1.8893e-01,  2.2833e-01],\n",
       "                        [-9.4088e-01, -3.4339e-01,  8.9907e-01],\n",
       "                        [-2.1204e-03,  3.0896e-01,  4.9953e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.1321e-01,  5.4262e-02,  1.8721e-01],\n",
       "                        [ 7.7245e-01, -4.2362e-01, -9.7571e-01],\n",
       "                        [ 5.8087e-01,  2.3426e-01, -4.4087e-01]],\n",
       "              \n",
       "                       [[ 4.8934e-01, -5.9026e-02,  1.6839e-03],\n",
       "                        [ 6.8533e-01, -4.8358e-01, -1.3530e+00],\n",
       "                        [ 5.4588e-01,  3.4553e-01, -5.9782e-01]],\n",
       "              \n",
       "                       [[ 8.5630e-02,  2.2176e-01,  5.3444e-01],\n",
       "                        [ 3.4456e-01, -4.1069e-01, -6.1995e-01],\n",
       "                        [ 2.2876e-01,  1.3409e-01, -5.0664e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2215e-01, -6.2452e-01, -4.0303e-01],\n",
       "                        [-6.1050e-01, -7.5069e-01, -7.8922e-01],\n",
       "                        [-5.7783e-01, -9.8292e-01, -8.4384e-01]],\n",
       "              \n",
       "                       [[-1.5789e-01,  1.2242e-01, -2.9747e-01],\n",
       "                        [ 3.8840e-01,  6.6292e-01,  3.0778e-01],\n",
       "                        [ 6.8307e-02,  1.0262e-01,  2.9979e-02]],\n",
       "              \n",
       "                       [[ 1.3978e-01,  5.4648e-01,  5.9003e-02],\n",
       "                        [ 5.8368e-01,  1.4628e+00,  8.6412e-01],\n",
       "                        [ 3.7575e-01,  8.5173e-01,  4.0163e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3140e-01,  6.5279e-01, -4.1369e-01],\n",
       "                        [-6.2593e-01, -1.2069e+00,  7.2996e-02],\n",
       "                        [ 3.0293e-01,  3.2063e-01,  8.2402e-01]],\n",
       "              \n",
       "                       [[ 8.4682e-01,  1.0004e+00,  1.0241e-01],\n",
       "                        [-8.5225e-01, -1.4023e+00,  4.8998e-02],\n",
       "                        [ 4.3616e-02, -3.0539e-01,  7.6269e-01]],\n",
       "              \n",
       "                       [[ 6.5153e-01,  6.1098e-01,  3.9530e-02],\n",
       "                        [-3.9860e-01, -1.0256e+00, -1.0201e-01],\n",
       "                        [-3.6981e-02, -5.1075e-01,  3.8620e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.8512e-01,  5.6056e-01,  1.9931e-01],\n",
       "                        [-7.1396e-01, -7.4205e-01,  7.5849e-02],\n",
       "                        [ 3.0334e-01,  8.7610e-02,  1.6386e-01]],\n",
       "              \n",
       "                       [[ 3.0663e-01,  8.5636e-01,  5.9673e-01],\n",
       "                        [-8.3291e-01, -7.5372e-01, -3.6590e-01],\n",
       "                        [ 5.2356e-03, -2.3271e-02, -1.1739e-01]],\n",
       "              \n",
       "                       [[ 7.4793e-01,  8.3830e-01,  2.3682e-01],\n",
       "                        [-4.7219e-01, -4.7703e-01, -1.9997e-01],\n",
       "                        [ 9.9404e-02, -3.7972e-01, -4.1654e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4389e-01, -1.6766e-01, -1.9358e-01],\n",
       "                        [-6.5485e-01, -5.7184e-01, -7.9434e-01],\n",
       "                        [-6.7295e-01, -5.9633e-01, -5.9251e-01]],\n",
       "              \n",
       "                       [[-1.0399e-01,  4.6378e-01,  1.6142e-02],\n",
       "                        [-1.9374e-01, -8.2373e-02, -1.3812e-01],\n",
       "                        [-2.2333e-01, -2.1542e-01, -2.5140e-01]],\n",
       "              \n",
       "                       [[ 5.3864e-01,  1.1065e+00,  7.2999e-01],\n",
       "                        [ 5.7934e-01,  1.0594e+00,  6.2575e-01],\n",
       "                        [-3.3914e-02,  4.3139e-01,  3.0039e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.5557e-01,  7.7835e-01, -1.7814e-01],\n",
       "                        [ 1.9155e-01,  4.1013e-01,  3.6364e-01],\n",
       "                        [ 2.6347e-02, -2.8235e-02,  2.2341e-01]],\n",
       "              \n",
       "                       [[-5.0296e-01, -3.9266e-01, -1.8949e-01],\n",
       "                        [-8.2804e-01, -6.1499e-01,  1.1121e-02],\n",
       "                        [-3.1366e-01, -1.7471e-01,  5.4434e-01]],\n",
       "              \n",
       "                       [[-4.5348e-01, -2.3656e-01, -4.0574e-01],\n",
       "                        [-6.1831e-01, -4.2694e-01, -3.8332e-02],\n",
       "                        [-2.6471e-01,  1.9310e-01,  3.0584e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8455e-01,  1.2126e+00,  8.0503e-01],\n",
       "                        [ 1.4647e+00, -1.2733e+00,  1.1694e-01],\n",
       "                        [-1.1123e+00, -1.0796e+00,  9.6822e-01]],\n",
       "              \n",
       "                       [[-8.3310e-01, -1.0898e-01,  6.5875e-02],\n",
       "                        [ 1.4889e+00, -2.1803e+00, -2.3472e-01],\n",
       "                        [-5.5331e-02, -3.7314e-01,  1.6295e+00]],\n",
       "              \n",
       "                       [[-3.3531e-01,  5.5317e-01, -4.9394e-01],\n",
       "                        [ 1.5348e+00, -1.1421e+00, -8.6518e-01],\n",
       "                        [ 6.2617e-01, -5.4433e-02,  4.6411e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4604e-01,  1.2608e+00,  1.0239e+00],\n",
       "                        [ 6.0905e-02,  2.5489e-01,  1.5757e-01],\n",
       "                        [-7.9464e-01, -9.7786e-01, -9.5878e-01]],\n",
       "              \n",
       "                       [[ 5.0220e-01,  4.0124e-01,  4.4711e-01],\n",
       "                        [ 4.7901e-03, -1.1536e-01, -5.1425e-02],\n",
       "                        [-2.7524e-01, -7.8621e-01, -5.0263e-01]],\n",
       "              \n",
       "                       [[ 3.3087e-01,  1.4517e-01, -7.0901e-02],\n",
       "                        [-4.0793e-02,  2.4547e-01,  1.8638e-02],\n",
       "                        [-1.9316e-01, -1.1762e-01, -4.6944e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.1189e-01, -1.3706e-01,  1.5707e-01],\n",
       "                        [-2.3585e-03, -1.0508e+00, -5.0788e-01],\n",
       "                        [ 3.5409e-01,  8.6983e-02, -7.2599e-02]],\n",
       "              \n",
       "                       [[ 3.8382e-01, -9.1621e-02,  1.8528e-01],\n",
       "                        [ 1.1166e-01, -6.7314e-01, -2.4659e-01],\n",
       "                        [ 1.8720e-01,  2.8836e-01,  2.8809e-01]],\n",
       "              \n",
       "                       [[ 1.1892e-01, -1.9946e-01,  2.2136e-01],\n",
       "                        [-5.0872e-01, -8.2646e-01, -4.7349e-01],\n",
       "                        [-2.0345e-01, -3.7981e-01, -4.1044e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0992e+00, -3.6817e-03,  1.0181e+00],\n",
       "                        [-3.5721e-01,  3.9177e-02,  1.1431e-01],\n",
       "                        [ 1.0578e+00,  6.4090e-02, -8.5409e-01]],\n",
       "              \n",
       "                       [[-1.0481e+00, -1.6901e-01,  1.0465e+00],\n",
       "                        [-1.7580e-01,  2.1051e-01,  1.4377e-01],\n",
       "                        [ 9.2165e-01, -3.2907e-01, -8.3878e-01]],\n",
       "              \n",
       "                       [[-7.5452e-01,  3.0559e-01,  6.2661e-01],\n",
       "                        [-2.5981e-01,  3.1321e-01,  2.6233e-01],\n",
       "                        [ 7.7262e-01, -2.7381e-01, -8.9623e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2306e-01, -9.2307e-01,  4.5236e-01],\n",
       "                        [-3.3498e-01, -1.0396e+00, -2.1117e-01],\n",
       "                        [ 8.3307e-01,  8.2997e-01,  9.0825e-01]],\n",
       "              \n",
       "                       [[-4.3742e-01, -6.6041e-01,  1.1552e-01],\n",
       "                        [-1.5698e-01, -8.8968e-01, -1.3198e-01],\n",
       "                        [ 9.4637e-01,  7.2481e-01,  7.0995e-01]],\n",
       "              \n",
       "                       [[ 1.1793e-02, -5.7150e-01,  3.5219e-01],\n",
       "                        [ 2.2607e-02, -9.5980e-01, -2.1904e-01],\n",
       "                        [ 6.5610e-01,  3.8355e-01,  2.2455e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2842e-01, -1.3555e-01, -2.0949e-01],\n",
       "                        [-1.2356e-01,  4.2564e-01,  4.5131e-01],\n",
       "                        [ 7.0156e-01,  1.1215e+00,  7.0723e-01]],\n",
       "              \n",
       "                       [[-4.6227e-01, -2.9525e-01, -6.7845e-01],\n",
       "                        [-9.5757e-01, -8.7562e-01, -1.0047e+00],\n",
       "                        [-4.4320e-01, -5.2527e-01, -7.1877e-01]],\n",
       "              \n",
       "                       [[ 8.4654e-01,  8.8862e-01,  7.5659e-01],\n",
       "                        [ 1.1400e-01,  2.7547e-01,  2.3232e-01],\n",
       "                        [-1.9108e-01,  5.0945e-02, -1.6371e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5764e-01,  7.8496e-02,  7.0316e-02],\n",
       "                        [-9.8618e-03,  3.5874e-01,  1.4442e-01],\n",
       "                        [ 9.6966e-02,  1.8458e-01,  9.3510e-03]],\n",
       "              \n",
       "                       [[-1.9471e-01,  2.5681e-01, -1.8786e-01],\n",
       "                        [ 1.4716e-01,  9.6748e-02, -8.6126e-02],\n",
       "                        [ 3.5148e-02,  3.1066e-01, -6.5107e-02]],\n",
       "              \n",
       "                       [[ 2.5573e-01,  4.7144e-01,  1.9082e-01],\n",
       "                        [ 3.6060e-01,  5.8935e-01,  8.1626e-02],\n",
       "                        [-1.7770e-02,  1.2095e-01, -9.5245e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0492e-01, -7.9451e-01, -2.0177e-01],\n",
       "                        [ 2.9974e-01,  9.3592e-01,  5.7752e-01],\n",
       "                        [ 4.5517e-01,  5.7255e-01,  1.0055e-01]],\n",
       "              \n",
       "                       [[-3.9410e-01, -9.5116e-01, -4.0499e-01],\n",
       "                        [-7.1705e-02,  4.2422e-01,  2.7775e-01],\n",
       "                        [-1.3216e-01, -2.8865e-01, -3.6781e-01]],\n",
       "              \n",
       "                       [[-9.0501e-02, -2.5837e-01,  1.6859e-01],\n",
       "                        [ 2.0695e-01,  7.0540e-01,  3.1135e-01],\n",
       "                        [-3.1081e-02,  5.2900e-02, -1.8616e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.1950e-01, -4.1508e-01, -2.8218e-01],\n",
       "                        [-1.4278e-02, -1.2342e-01, -7.2811e-02],\n",
       "                        [-2.5550e-01,  2.3747e-02,  4.0854e-02]],\n",
       "              \n",
       "                       [[-2.0806e-01,  1.3843e-02, -3.3811e-01],\n",
       "                        [ 2.6848e-03,  2.5554e-01, -8.4292e-02],\n",
       "                        [-5.0058e-01, -4.9409e-01, -4.0431e-01]],\n",
       "              \n",
       "                       [[ 2.5918e-01,  5.0689e-01,  2.3340e-01],\n",
       "                        [ 2.8925e-01,  6.9060e-01,  2.6207e-01],\n",
       "                        [-1.8518e-01, -7.2043e-02, -2.1737e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.2315e-01, -9.4369e-01, -7.2165e-01],\n",
       "                        [-5.0508e-02,  9.0636e-02, -1.1688e-01],\n",
       "                        [ 4.9442e-01,  6.7985e-01,  6.4325e-01]],\n",
       "              \n",
       "                       [[-3.7517e-01, -5.2720e-01, -4.6969e-01],\n",
       "                        [ 4.9999e-01,  3.3864e-01,  3.6457e-01],\n",
       "                        [ 1.5075e-01,  8.9029e-01,  7.5332e-01]],\n",
       "              \n",
       "                       [[-1.0619e-01, -9.4547e-01, -5.8482e-01],\n",
       "                        [ 7.8396e-02,  3.2259e-03,  1.5844e-02],\n",
       "                        [ 2.9331e-01,  1.3868e-01,  3.6770e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.6490e-01, -2.8646e-01,  3.7080e-02],\n",
       "                        [-4.6360e-01, -8.8054e-01, -2.2498e-01],\n",
       "                        [ 1.1127e-01, -6.8173e-02,  3.6203e-01]],\n",
       "              \n",
       "                       [[ 2.8686e-01, -1.3965e-02,  2.5676e-01],\n",
       "                        [-2.0552e-01, -4.3999e-01, -2.6020e-01],\n",
       "                        [ 1.2199e-01,  5.1533e-02,  2.1905e-01]],\n",
       "              \n",
       "                       [[ 2.5427e-01, -2.5339e-01,  1.3233e-01],\n",
       "                        [-4.4425e-01, -7.9445e-01, -2.3966e-01],\n",
       "                        [ 5.9852e-02, -1.1686e-01, -8.3175e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1581e+00,  1.5876e+00,  1.2343e+00],\n",
       "                        [ 1.4999e-01,  2.3932e-02,  9.7031e-02],\n",
       "                        [-1.0668e+00, -1.6777e+00, -1.2232e+00]],\n",
       "              \n",
       "                       [[-2.6281e-01, -3.1214e-01, -4.1979e-01],\n",
       "                        [-6.6602e-02, -2.8669e-01, -2.1713e-02],\n",
       "                        [ 4.5412e-01,  1.3624e-01,  5.0685e-01]],\n",
       "              \n",
       "                       [[-1.0695e+00, -1.3316e+00, -1.2191e+00],\n",
       "                        [-3.3425e-01,  2.9614e-01,  2.0976e-02],\n",
       "                        [ 1.0264e+00,  1.4344e+00,  9.6358e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.1699e-01, -8.9804e-02, -4.8738e-01],\n",
       "                        [-1.0412e+00,  2.2035e-01, -2.7585e-02],\n",
       "                        [-1.8318e-01,  4.6138e-02,  1.6011e-01]],\n",
       "              \n",
       "                       [[ 4.5050e-02,  8.1832e-01,  4.8944e-01],\n",
       "                        [-6.2886e-01,  1.2733e+00,  1.1358e+00],\n",
       "                        [-9.8908e-02,  3.2021e-01,  3.9740e-01]],\n",
       "              \n",
       "                       [[-3.2753e-01,  2.7570e-01, -3.1633e-01],\n",
       "                        [-8.7083e-01,  3.8136e-01, -1.0155e-01],\n",
       "                        [-4.9953e-01, -1.0893e-01,  2.0734e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8644e-02, -3.2624e-01, -2.2315e-01],\n",
       "                        [ 3.7384e-01,  3.1365e-01,  2.7697e-01],\n",
       "                        [ 2.1826e-01,  2.5302e-01,  2.2848e-01]],\n",
       "              \n",
       "                       [[-2.3903e-01, -2.9286e-01, -3.7492e-01],\n",
       "                        [ 2.2696e-01,  3.0064e-01,  4.3780e-02],\n",
       "                        [ 1.6319e-01,  9.1570e-02, -4.1368e-02]],\n",
       "              \n",
       "                       [[-5.3434e-02, -5.6940e-02, -3.6186e-01],\n",
       "                        [ 4.6369e-01,  3.0135e-01, -1.4473e-01],\n",
       "                        [ 3.5113e-01,  1.4846e-01, -1.3136e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.0714e-01, -7.0433e-01, -5.0340e-01],\n",
       "                        [ 7.2643e-02,  5.7143e-02,  3.3262e-02],\n",
       "                        [ 5.6978e-01,  7.9702e-01,  6.5755e-01]],\n",
       "              \n",
       "                       [[ 5.5029e-03, -2.8522e-01, -2.0949e-01],\n",
       "                        [-1.4425e-01,  2.7532e-01,  2.0440e-01],\n",
       "                        [ 1.8113e-01,  3.5645e-01,  1.7561e-01]],\n",
       "              \n",
       "                       [[ 9.9449e-01,  1.0417e+00,  5.4901e-01],\n",
       "                        [ 3.8781e-02, -1.3406e-01, -2.5410e-01],\n",
       "                        [-5.8594e-01, -1.1834e+00, -1.1219e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7754e-02, -2.0948e-01,  1.2229e-01],\n",
       "                        [ 3.5918e-01,  2.3606e-01,  8.2641e-01],\n",
       "                        [ 1.8891e-01, -3.9553e-01,  2.4130e-01]],\n",
       "              \n",
       "                       [[ 3.8047e-01, -3.5316e-02,  4.0284e-02],\n",
       "                        [ 2.7422e-01, -4.9840e-01, -7.2813e-02],\n",
       "                        [-4.5077e-01, -1.0116e+00, -5.7418e-01]],\n",
       "              \n",
       "                       [[ 4.2405e-01,  4.4689e-01,  5.1046e-01],\n",
       "                        [ 6.5073e-01,  1.3184e-01,  2.2423e-01],\n",
       "                        [-2.9629e-01, -9.2983e-01, -3.9708e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.7836e-02, -7.5087e-01,  1.3452e+00],\n",
       "                        [-6.2575e-01,  2.6040e+00, -1.3689e+00],\n",
       "                        [-1.2604e+00,  5.9851e-02,  1.0685e-01]],\n",
       "              \n",
       "                       [[ 4.4690e-02, -5.2580e-01,  1.4804e-01],\n",
       "                        [-2.8576e-01,  2.5846e+00, -2.2666e+00],\n",
       "                        [-4.4783e-01,  5.1641e-01, -3.2163e-02]],\n",
       "              \n",
       "                       [[ 3.0412e-01, -8.0242e-01,  4.2518e-01],\n",
       "                        [-4.9551e-01,  1.5338e+00, -1.7846e+00],\n",
       "                        [ 1.7764e-01,  4.2812e-01,  3.4481e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.6875e-02,  5.1701e-01, -4.1698e-01],\n",
       "                        [-1.0267e+00,  7.3840e-01,  8.0301e-01],\n",
       "                        [-6.2991e-01,  7.0737e-02, -2.1441e-01]],\n",
       "              \n",
       "                       [[-5.2782e-01, -7.1171e-03,  1.7121e-02],\n",
       "                        [-1.5023e+00,  5.9908e-01,  1.2550e+00],\n",
       "                        [-6.6325e-01,  3.4872e-01,  3.2508e-01]],\n",
       "              \n",
       "                       [[-3.8546e-01,  8.9735e-02, -4.4784e-02],\n",
       "                        [-1.3276e+00,  7.2525e-01,  9.7362e-01],\n",
       "                        [-6.6382e-01,  5.0661e-01,  2.8737e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.0297e-01, -4.4385e-01, -3.7963e-01],\n",
       "                        [ 5.0906e-02, -2.2639e-01, -5.6638e-02],\n",
       "                        [-8.5377e-01, -1.6898e+00, -6.8952e-01]],\n",
       "              \n",
       "                       [[ 7.9777e-01,  1.0436e+00,  5.7461e-01],\n",
       "                        [ 1.6498e+00,  1.7265e+00,  1.4330e+00],\n",
       "                        [ 7.9440e-01,  8.3967e-02,  5.4361e-01]],\n",
       "              \n",
       "                       [[-7.7544e-01, -1.0937e+00, -8.4160e-01],\n",
       "                        [ 5.5908e-02, -6.1526e-01, -1.2425e-01],\n",
       "                        [-1.9594e-01, -8.8816e-01,  6.3752e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.5261e-01,  2.7538e-01, -2.0202e-01],\n",
       "                        [ 1.0844e+00,  2.6398e-01, -1.1756e+00],\n",
       "                        [ 1.3704e-01, -1.7433e-01, -2.9942e-01]],\n",
       "              \n",
       "                       [[ 7.7017e-01, -1.3015e-01, -1.2435e+00],\n",
       "                        [ 1.4542e+00, -1.1615e-01, -1.6577e+00],\n",
       "                        [ 4.1894e-01,  3.4633e-03, -5.3822e-01]],\n",
       "              \n",
       "                       [[ 4.9902e-01, -1.3109e-01, -8.7425e-01],\n",
       "                        [ 1.4743e+00,  3.6337e-01, -1.2042e+00],\n",
       "                        [ 8.6177e-01,  1.4009e-02, -4.8786e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1483e+00,  1.4821e+00,  7.9972e-01],\n",
       "                        [ 8.8295e-01, -1.0935e+00,  8.8492e-01],\n",
       "                        [-3.6108e-01, -1.7023e+00, -1.5850e+00]],\n",
       "              \n",
       "                       [[-4.0459e-01, -2.9197e-01, -4.1407e-01],\n",
       "                        [-3.1370e-03, -1.6041e+00,  1.1043e+00],\n",
       "                        [ 5.1098e-01, -1.2983e-01,  5.8607e-01]],\n",
       "              \n",
       "                       [[-7.6655e-01, -5.9248e-01, -1.0845e+00],\n",
       "                        [ 4.7828e-02, -8.3389e-01,  6.7056e-01],\n",
       "                        [ 1.0259e+00,  9.6670e-01,  7.5338e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.7627e-02, -9.3956e-01, -6.5185e-01],\n",
       "                        [ 4.4882e-01,  9.8119e-01,  5.2181e-01],\n",
       "                        [-4.9687e-02,  2.4900e-01,  9.4613e-02]],\n",
       "              \n",
       "                       [[-3.7113e-01, -7.7517e-01, -6.7337e-01],\n",
       "                        [ 2.0036e-01,  9.5474e-01,  3.9141e-01],\n",
       "                        [-3.6652e-01,  4.6237e-02, -3.3025e-01]],\n",
       "              \n",
       "                       [[-2.4663e-02, -5.6934e-01, -3.3191e-01],\n",
       "                        [ 1.7403e-01,  9.3419e-01,  5.2080e-01],\n",
       "                        [-1.9306e-01,  7.5581e-02, -1.6303e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.4526e-01, -4.7319e-01, -5.8144e-02],\n",
       "                        [ 5.2575e-01,  2.0558e-01, -7.9737e-01],\n",
       "                        [ 6.0704e-01,  3.6191e-01, -3.0978e-01]],\n",
       "              \n",
       "                       [[-1.0660e-01, -1.8084e-01,  1.4867e-01],\n",
       "                        [ 8.7418e-01,  4.1051e-01, -5.4904e-01],\n",
       "                        [ 3.5131e-01,  4.0586e-02, -5.6816e-01]],\n",
       "              \n",
       "                       [[ 5.2015e-03, -1.3211e-03, -1.5475e-01],\n",
       "                        [ 9.4173e-01,  2.8285e-01, -6.4030e-01],\n",
       "                        [ 4.9779e-01, -2.2143e-01, -7.4964e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.3121e-01, -7.5906e-01, -4.7272e-01],\n",
       "                        [-3.3155e-01, -5.1259e-01, -1.3841e-01],\n",
       "                        [-2.5138e-01, -9.3561e-02,  8.6543e-02]],\n",
       "              \n",
       "                       [[-6.1921e-01, -7.8019e-01, -4.7794e-01],\n",
       "                        [ 1.3205e-01, -1.3983e-01,  7.7905e-02],\n",
       "                        [ 3.3254e-01,  1.7180e-01,  1.6117e-01]],\n",
       "              \n",
       "                       [[-3.0669e-01, -5.3661e-01, -4.6193e-01],\n",
       "                        [ 4.1804e-01,  5.2455e-01,  4.3928e-01],\n",
       "                        [ 6.3286e-01,  5.7358e-01,  4.0605e-01]]]], device='mps:0')),\n",
       "             ('model.0.1.weight',\n",
       "              tensor([1.0030, 1.2417, 1.1108, 0.7086, 1.4484, 1.0021, 0.7617, 0.9483, 2.0205,\n",
       "                      1.0026, 1.6280, 1.0919, 1.1475, 0.7983, 1.0918, 0.7695, 0.9342, 0.9284,\n",
       "                      1.6964, 1.3543, 0.8664, 0.4545, 1.1657, 0.8296, 2.8554, 1.4328, 1.1043,\n",
       "                      1.4243, 1.5086, 1.2256, 0.8222, 1.2479], device='mps:0')),\n",
       "             ('model.0.1.bias',\n",
       "              tensor([ 0.4378,  0.3951,  0.3621,  0.4648,  0.6802,  0.6909, -0.0453, -0.1896,\n",
       "                       1.9524,  0.5220, -0.6369,  0.4615,  0.5035,  0.1235, -0.6217,  0.3958,\n",
       "                      -0.6410,  0.4458, -1.2411, -0.4490,  0.5662,  0.2558, -0.6704,  0.0869,\n",
       "                       2.5992,  0.5860,  0.2017,  0.6434,  1.4005,  0.5692,  0.4218, -0.2638],\n",
       "                     device='mps:0')),\n",
       "             ('model.0.1.running_mean',\n",
       "              tensor([-0.1925,  0.1269,  0.1261, -0.1591,  0.2431,  0.1259,  0.0644,  0.5090,\n",
       "                       0.1148, -0.1223,  0.9110,  0.0363,  0.1858,  0.3026, -0.8746, -0.2723,\n",
       "                       0.3888, -0.1681,  0.8223,  0.0382, -0.1881, -0.4679, -0.0647, -0.0038,\n",
       "                      -0.1742, -0.1371, -0.0841, -0.0358,  0.1297, -0.2176, -0.0398,  0.6656],\n",
       "                     device='mps:0')),\n",
       "             ('model.0.1.running_var',\n",
       "              tensor([12.2908, 21.1225, 15.0946, 18.5407, 10.2946, 11.1816, 16.6319, 10.5193,\n",
       "                       6.9239, 24.3742, 17.8641,  4.8731, 22.7847,  4.5885, 14.5112,  7.1435,\n",
       "                       4.8605, 22.8186, 13.8453,  2.8103, 10.1067,  7.4839,  1.3455,  9.2337,\n",
       "                       9.1040, 23.1241, 11.0842, 46.6388,  5.3780, 10.1471, 11.1330, 20.9485],\n",
       "                     device='mps:0')),\n",
       "             ('model.0.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.1.0.weight',\n",
       "              tensor([[[[-2.0450e-01, -2.3559e-01, -1.6379e-01],\n",
       "                        [-2.3276e-01, -2.4086e-01, -2.4306e-01],\n",
       "                        [-9.4205e-02, -8.1977e-02, -2.6264e-02]],\n",
       "              \n",
       "                       [[-8.4158e-02,  4.9218e-02,  1.4698e-01],\n",
       "                        [-1.6226e-01, -3.4957e-02,  5.6308e-02],\n",
       "                        [-3.4209e-01, -2.1271e-01, -7.5631e-02]],\n",
       "              \n",
       "                       [[ 5.9199e-02, -1.3700e-02, -3.3085e-02],\n",
       "                        [ 2.5260e-02, -6.1533e-02, -4.0967e-02],\n",
       "                        [-5.4628e-02, -2.2659e-01, -1.9803e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3319e-01, -8.3192e-02, -1.4808e-01],\n",
       "                        [-2.0992e-02, -1.0417e-01, -8.4455e-02],\n",
       "                        [-2.8770e-02, -1.0738e-01, -9.2791e-02]],\n",
       "              \n",
       "                       [[-5.6517e-02, -8.5416e-02, -9.4544e-02],\n",
       "                        [ 5.2510e-02, -8.7249e-04,  2.9981e-02],\n",
       "                        [ 7.4321e-02,  3.7569e-02, -8.6443e-02]],\n",
       "              \n",
       "                       [[ 1.2159e-01,  9.9381e-02,  6.2586e-02],\n",
       "                        [ 2.9087e-01,  4.0082e-01,  3.3885e-01],\n",
       "                        [ 2.1024e-01,  3.1226e-01,  2.0942e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.7163e-02,  8.4086e-02,  9.8936e-03],\n",
       "                        [-2.8634e-01, -6.9121e-02,  3.9750e-02],\n",
       "                        [ 1.8067e-01,  6.6750e-02, -1.2808e-01]],\n",
       "              \n",
       "                       [[-7.3003e-02, -4.6267e-02, -7.5809e-02],\n",
       "                        [-2.8704e-01, -2.5646e-03,  1.8111e-01],\n",
       "                        [-1.3710e-01, -3.4417e-03,  2.3700e-02]],\n",
       "              \n",
       "                       [[ 7.2436e-03, -5.5749e-02, -5.1142e-02],\n",
       "                        [ 1.6388e-01, -9.9758e-03, -2.7778e-01],\n",
       "                        [-5.1662e-03,  1.9875e-01,  6.9679e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.4422e-03,  1.1803e-01, -1.5709e-01],\n",
       "                        [-3.0361e-01, -3.8535e-01, -2.0324e-01],\n",
       "                        [ 3.2971e-01,  5.8092e-01,  1.0326e-01]],\n",
       "              \n",
       "                       [[-9.8443e-02,  1.5720e-01,  7.9685e-02],\n",
       "                        [ 9.2909e-02, -1.2188e-01, -2.8326e-01],\n",
       "                        [-1.5366e-01,  1.3486e-01,  2.7730e-01]],\n",
       "              \n",
       "                       [[ 2.7207e-02,  1.5755e-01,  1.4364e-02],\n",
       "                        [-1.9243e-01, -8.5793e-02, -1.1409e-01],\n",
       "                        [-1.6731e-01,  3.4023e-02, -1.7437e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.7813e-01, -6.9504e-01, -4.2501e-01],\n",
       "                        [-1.2750e-01, -4.3643e-02,  1.0075e-01],\n",
       "                        [ 5.6347e-02,  4.3477e-01,  5.2736e-01]],\n",
       "              \n",
       "                       [[-1.6504e-01, -1.2353e-01, -1.2828e-02],\n",
       "                        [-1.8011e-01, -3.3459e-01, -3.7747e-02],\n",
       "                        [ 8.0111e-02,  9.7101e-02,  1.8796e-01]],\n",
       "              \n",
       "                       [[-4.7334e-02, -9.8294e-03,  1.4581e-03],\n",
       "                        [-1.5802e-01, -2.2482e-01,  1.4797e-01],\n",
       "                        [ 9.8968e-02, -7.6019e-02,  9.6128e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.8537e-02,  3.4886e-02, -9.0443e-02],\n",
       "                        [-1.4379e-03,  1.2311e-01,  4.8882e-02],\n",
       "                        [-2.1723e-01, -1.4267e-01,  1.2544e-01]],\n",
       "              \n",
       "                       [[ 1.5352e-01,  9.9329e-02,  1.5618e-01],\n",
       "                        [ 1.1465e-04, -6.1684e-03,  1.0449e-01],\n",
       "                        [-5.4194e-02, -2.0709e-01, -1.6001e-01]],\n",
       "              \n",
       "                       [[-3.0162e-02,  1.1974e-01, -2.4478e-02],\n",
       "                        [-1.7273e-01, -9.5262e-02, -1.8966e-01],\n",
       "                        [-1.8664e-01, -1.7581e-01, -1.9986e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.9211e-01, -6.6578e-02,  5.4359e-01],\n",
       "                        [-3.9380e-01, -3.5574e-01,  4.5573e-01],\n",
       "                        [-3.0933e-01, -3.1804e-01,  1.7506e-01]],\n",
       "              \n",
       "                       [[-4.7914e-01, -1.8475e-02,  1.8818e-01],\n",
       "                        [-3.6681e-01, -1.6110e-01,  4.8769e-01],\n",
       "                        [-2.2666e-01, -3.7882e-01,  1.4193e-01]],\n",
       "              \n",
       "                       [[ 1.1715e-01,  2.2990e-01, -2.9764e-01],\n",
       "                        [ 5.6628e-03,  4.7946e-01, -4.0796e-01],\n",
       "                        [-2.8893e-01,  3.0915e-01, -1.7982e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.4583e-01, -2.9147e-02,  1.8430e-01],\n",
       "                        [ 1.6447e-01, -2.2349e-01, -3.5572e-02],\n",
       "                        [ 2.0772e-01, -2.0713e-02, -1.7547e-01]],\n",
       "              \n",
       "                       [[ 1.9872e-01,  1.5174e-01, -3.9253e-01],\n",
       "                        [ 1.6320e-02,  2.8003e-01, -3.9971e-01],\n",
       "                        [-1.0956e-01,  3.0948e-01, -3.2189e-02]],\n",
       "              \n",
       "                       [[ 5.8455e-02, -3.5647e-02,  5.8517e-02],\n",
       "                        [ 7.2825e-02, -7.1414e-02, -3.7585e-02],\n",
       "                        [ 1.2763e-01,  2.1051e-02,  2.1819e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.5174e-02,  1.3353e-01,  6.8893e-02],\n",
       "                        [-3.3124e-02,  9.8053e-02, -1.6580e-02],\n",
       "                        [-1.2421e-01, -1.5969e-01, -1.9190e-01]],\n",
       "              \n",
       "                       [[-8.6922e-02,  1.1312e-01,  1.5660e-01],\n",
       "                        [ 6.0805e-02,  1.3355e-01, -9.8768e-03],\n",
       "                        [-2.7388e-02, -7.5958e-03,  1.0256e-02]],\n",
       "              \n",
       "                       [[-5.9784e-04,  7.7569e-02, -1.4120e-01],\n",
       "                        [ 4.2412e-02,  1.3326e-02, -3.5752e-02],\n",
       "                        [ 1.8106e-01,  1.4062e-01,  4.2871e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.0071e-02, -5.2341e-02, -3.7222e-03],\n",
       "                        [ 1.9100e-01,  3.3520e-01,  2.1077e-01],\n",
       "                        [-1.1907e-02, -1.8068e-04,  1.8417e-02]],\n",
       "              \n",
       "                       [[-8.2724e-02, -1.5755e-02, -9.2832e-02],\n",
       "                        [ 1.5520e-02,  1.2935e-01,  8.7191e-02],\n",
       "                        [ 8.9260e-02,  3.1560e-02,  8.9415e-02]],\n",
       "              \n",
       "                       [[-8.5277e-02,  1.3269e-01,  6.2698e-02],\n",
       "                        [-3.3066e-03,  1.2319e-01,  6.2084e-02],\n",
       "                        [-1.1390e-01, -1.2787e-01, -1.2984e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.3697e-02, -1.0072e-01, -6.1971e-03],\n",
       "                        [-1.5723e-01, -3.0556e-01, -1.1410e-01],\n",
       "                        [-2.4133e-01, -2.7589e-01, -1.9675e-01]],\n",
       "              \n",
       "                       [[-1.3003e-01, -1.4654e-04,  1.6108e-01],\n",
       "                        [-1.9634e-01, -7.3402e-02,  8.6209e-02],\n",
       "                        [-1.5153e-01, -8.3117e-02,  2.6136e-01]],\n",
       "              \n",
       "                       [[-6.2059e-02, -1.6364e-01, -2.6964e-01],\n",
       "                        [ 9.4567e-03, -6.1158e-02, -1.3125e-01],\n",
       "                        [ 2.9274e-01,  1.2319e-01, -5.1836e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3252e-02,  4.9279e-02,  1.3983e-02],\n",
       "                        [-2.2418e-01, -2.0307e-01, -1.7296e-01],\n",
       "                        [ 1.5339e-01,  1.2269e-01,  9.4176e-02]],\n",
       "              \n",
       "                       [[-7.9548e-02, -1.2242e-01, -1.1164e-01],\n",
       "                        [-2.3198e-02, -7.0765e-02, -1.0083e-01],\n",
       "                        [ 1.8194e-01,  9.9496e-02, -1.4201e-02]],\n",
       "              \n",
       "                       [[-1.0527e-01, -2.1383e-02, -9.0091e-02],\n",
       "                        [-3.6708e-01, -3.3485e-01, -3.5718e-01],\n",
       "                        [-2.2739e-01, -2.0871e-01, -2.3488e-01]]]], device='mps:0')),\n",
       "             ('model.1.1.weight',\n",
       "              tensor([1.2075, 1.6137, 1.5039, 1.1208, 1.3842, 1.8038, 1.8866, 1.1497, 2.3041,\n",
       "                      1.7560, 1.3912, 1.3800, 1.8007, 1.0066, 1.3157, 1.4913, 1.0902, 2.0610,\n",
       "                      1.0382, 2.1025, 1.1388, 1.5548, 1.8249, 1.8082, 1.3678, 2.6456, 2.4558,\n",
       "                      1.4036, 1.2387, 1.8133, 1.1103, 1.3620], device='mps:0')),\n",
       "             ('model.1.1.bias',\n",
       "              tensor([ 0.1179,  0.1031, -0.2335, -0.2701, -0.3128, -0.5125,  0.5676, -0.1560,\n",
       "                      -0.2460, -0.5754, -0.2256,  0.6038, -0.1898, -0.4951, -0.6099, -0.7462,\n",
       "                      -0.3689, -0.5736, -0.3172, -0.4241, -0.2158, -0.0523, -0.1862, -0.9262,\n",
       "                      -0.1928, -0.1471,  0.9170, -0.4284, -0.3327, -0.1152, -0.3112, -0.0748],\n",
       "                     device='mps:0')),\n",
       "             ('model.1.1.running_mean',\n",
       "              tensor([ -9.4253,  -2.8421,  -7.0705,  -4.4638,  -4.1854,  -7.7021,  -9.6240,\n",
       "                       -8.3632,  -4.1775,  -4.0232, -11.2920,  -4.9486,  -8.0370,   6.3452,\n",
       "                       -5.7452,   3.9910,  -1.4648,  -6.1724,  -3.3224,  -8.0714,  -7.5331,\n",
       "                       -4.3516,  -1.3795,  10.8637,  -9.4199,  -3.7977,  -5.9999,  -6.1545,\n",
       "                        7.2919,  -2.9348,   3.2052,  -7.9581], device='mps:0')),\n",
       "             ('model.1.1.running_var',\n",
       "              tensor([ 29.1162,  34.1855,  20.8430,  18.1495,  40.8200,  38.2013,  42.4680,\n",
       "                       34.3650, 242.1780,  51.0480,  40.0988,  44.0444, 105.4031,  13.3313,\n",
       "                       23.3735,  26.5661,  17.5198,  56.2843,  28.1791, 126.2668,  29.5352,\n",
       "                       65.6716,  92.5008,  48.2277,  25.6542, 410.7781,  56.3970,  38.1616,\n",
       "                       33.2006, 148.6181,  21.2483,  29.4248], device='mps:0')),\n",
       "             ('model.1.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.2.0.weight',\n",
       "              tensor([[[[-0.2047, -0.1127, -0.2001],\n",
       "                        [ 0.1811,  0.1478,  0.1489],\n",
       "                        [ 0.0889,  0.0572,  0.1019]],\n",
       "              \n",
       "                       [[ 0.1447, -0.0318, -0.1641],\n",
       "                        [-0.2170,  0.0112,  0.1703],\n",
       "                        [-0.2715, -0.5288, -0.3099]],\n",
       "              \n",
       "                       [[ 0.1018,  0.2345,  0.3516],\n",
       "                        [-0.0803, -0.2030, -0.1575],\n",
       "                        [-0.0365,  0.0885, -0.0690]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1357, -0.1432,  0.0248],\n",
       "                        [-0.1570, -0.0315, -0.0737],\n",
       "                        [ 0.0488,  0.0523,  0.0546]],\n",
       "              \n",
       "                       [[-0.1076, -0.0442,  0.0269],\n",
       "                        [-0.0019, -0.0290,  0.0323],\n",
       "                        [ 0.1646,  0.1136,  0.1026]],\n",
       "              \n",
       "                       [[-0.0303, -0.0221, -0.0988],\n",
       "                        [-0.1768, -0.0711, -0.0264],\n",
       "                        [-0.0498, -0.0046,  0.0040]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0866, -0.1529, -0.0138],\n",
       "                        [ 0.0116, -0.1068, -0.0356],\n",
       "                        [ 0.0731,  0.0089, -0.0824]],\n",
       "              \n",
       "                       [[ 0.0918, -0.0516, -0.1196],\n",
       "                        [ 0.0061,  0.1402, -0.0640],\n",
       "                        [-0.1016, -0.0927,  0.0177]],\n",
       "              \n",
       "                       [[-0.1282,  0.0934,  0.1125],\n",
       "                        [-0.0618, -0.0905,  0.1292],\n",
       "                        [ 0.0087, -0.0483, -0.0514]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0770, -0.2613, -0.1909],\n",
       "                        [ 0.0834,  0.0418,  0.0565],\n",
       "                        [ 0.1051,  0.2371,  0.2921]],\n",
       "              \n",
       "                       [[ 0.0201,  0.0307,  0.1518],\n",
       "                        [-0.0197, -0.0188,  0.0354],\n",
       "                        [-0.1265,  0.0069,  0.0107]],\n",
       "              \n",
       "                       [[ 0.0052, -0.0188,  0.0846],\n",
       "                        [-0.0449, -0.1434, -0.1113],\n",
       "                        [ 0.0011, -0.0100, -0.1220]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1060,  0.0847,  0.0848],\n",
       "                        [ 0.1148,  0.1475,  0.1645],\n",
       "                        [-0.0387,  0.0526,  0.0013]],\n",
       "              \n",
       "                       [[ 0.0229,  0.0112,  0.0490],\n",
       "                        [ 0.1034,  0.0858, -0.0886],\n",
       "                        [ 0.0517,  0.0633,  0.1276]],\n",
       "              \n",
       "                       [[ 0.0126, -0.0448, -0.0514],\n",
       "                        [ 0.1116,  0.0011, -0.0918],\n",
       "                        [ 0.2544,  0.1428,  0.0274]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0638, -0.0745,  0.0011],\n",
       "                        [-0.2179, -0.2851, -0.1804],\n",
       "                        [-0.0477, -0.3101, -0.1535]],\n",
       "              \n",
       "                       [[-0.0757, -0.1224, -0.1786],\n",
       "                        [-0.0625,  0.0166,  0.0569],\n",
       "                        [-0.0148,  0.0368, -0.0032]],\n",
       "              \n",
       "                       [[-0.1264, -0.0981, -0.0872],\n",
       "                        [ 0.0349, -0.0211,  0.0758],\n",
       "                        [-0.0213,  0.0303,  0.1087]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1678,  0.1229, -0.0932],\n",
       "                        [ 0.1106,  0.0357, -0.1184],\n",
       "                        [ 0.3243,  0.1664, -0.0230]],\n",
       "              \n",
       "                       [[ 0.2427, -0.0276, -0.3133],\n",
       "                        [ 0.5886,  0.1057, -0.4671],\n",
       "                        [ 0.1340, -0.1473, -0.5652]],\n",
       "              \n",
       "                       [[ 0.0940,  0.0701, -0.3001],\n",
       "                        [ 0.4073,  0.2877, -0.0375],\n",
       "                        [ 0.2765,  0.3032, -0.0733]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0824, -0.0833,  0.1645],\n",
       "                        [-0.0165,  0.1692,  0.3578],\n",
       "                        [ 0.0216,  0.2752,  0.2568]],\n",
       "              \n",
       "                       [[ 0.2045, -0.0729, -0.1481],\n",
       "                        [ 0.2564, -0.0909, -0.2747],\n",
       "                        [ 0.1700, -0.1715, -0.2892]],\n",
       "              \n",
       "                       [[ 0.1707,  0.0988, -0.1609],\n",
       "                        [ 0.1244, -0.0649, -0.2620],\n",
       "                        [ 0.2649,  0.0546, -0.1681]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0117, -0.0740, -0.0565],\n",
       "                        [-0.0262, -0.0954, -0.0321],\n",
       "                        [-0.1090,  0.0018,  0.0836]],\n",
       "              \n",
       "                       [[ 0.2174, -0.0588, -0.2400],\n",
       "                        [ 0.2047, -0.1886, -0.0908],\n",
       "                        [-0.0807, -0.0451, -0.0545]],\n",
       "              \n",
       "                       [[ 0.0508, -0.1656, -0.0206],\n",
       "                        [ 0.0406, -0.0241, -0.0577],\n",
       "                        [ 0.1356, -0.1414, -0.0403]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1350,  0.0061, -0.0161],\n",
       "                        [ 0.0854, -0.0183, -0.1447],\n",
       "                        [ 0.0770, -0.1295, -0.0736]],\n",
       "              \n",
       "                       [[-0.1838,  0.0188,  0.0858],\n",
       "                        [-0.0660,  0.0651,  0.0555],\n",
       "                        [-0.1312, -0.0774, -0.1413]],\n",
       "              \n",
       "                       [[-0.0256, -0.0832,  0.0044],\n",
       "                        [-0.0884, -0.1029,  0.0329],\n",
       "                        [ 0.0342, -0.0391,  0.0804]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0467, -0.0580,  0.1572],\n",
       "                        [ 0.0072, -0.0206,  0.1267],\n",
       "                        [ 0.0245,  0.0372,  0.2685]],\n",
       "              \n",
       "                       [[-0.0505, -0.2868, -0.1865],\n",
       "                        [ 0.1293, -0.1056, -0.2029],\n",
       "                        [ 0.2643,  0.0313, -0.0833]],\n",
       "              \n",
       "                       [[-0.0836, -0.2319, -0.2564],\n",
       "                        [ 0.0273, -0.2309, -0.1155],\n",
       "                        [-0.0040, -0.1378, -0.1510]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2878, -0.0362, -0.2228],\n",
       "                        [ 0.1939, -0.0413, -0.2647],\n",
       "                        [ 0.2101, -0.0301, -0.1748]],\n",
       "              \n",
       "                       [[-0.0894, -0.2128, -0.1667],\n",
       "                        [ 0.0366, -0.0243, -0.1481],\n",
       "                        [ 0.1919, -0.0343, -0.0528]],\n",
       "              \n",
       "                       [[ 0.0331,  0.0574,  0.0927],\n",
       "                        [ 0.0194,  0.0018,  0.0250],\n",
       "                        [ 0.0826, -0.0610, -0.0608]]]], device='mps:0')),\n",
       "             ('model.2.1.weight',\n",
       "              tensor([1.9256, 1.1403, 1.7446, 1.2133, 1.1309, 1.2049, 1.3980, 1.0988, 1.2684,\n",
       "                      1.2076, 1.0705, 1.2139, 1.4536, 1.6127, 1.3342, 1.1467, 1.1203, 1.3695,\n",
       "                      1.2475, 1.1073, 1.3371, 1.0271, 1.1530, 1.3951, 1.4328, 1.1859, 1.2219,\n",
       "                      1.0953, 1.0390, 1.0514, 1.7910, 0.9668, 1.2804, 1.6110, 1.2575, 0.9457,\n",
       "                      1.8649, 1.2512, 1.2944, 1.4385, 1.2408, 1.1781, 1.2670, 1.5709, 1.2379,\n",
       "                      1.6553, 1.2537, 1.6272, 1.2396, 1.4157, 1.3694, 1.4025, 1.1961, 1.3674,\n",
       "                      1.3943, 1.2592, 1.3740, 1.2671, 1.4051, 0.8829, 1.2363, 1.2853, 1.2570,\n",
       "                      1.3252], device='mps:0')),\n",
       "             ('model.2.1.bias',\n",
       "              tensor([-0.4465, -0.3081, -0.0381,  0.0147, -0.1504, -0.1281, -0.3135, -0.2718,\n",
       "                      -0.4213, -0.1064, -0.1362,  0.0986, -0.2400, -0.3684, -0.7908, -0.2050,\n",
       "                      -0.3550, -0.6338, -0.2392, -0.0587, -0.4594, -0.1533, -0.3746, -0.3429,\n",
       "                      -0.1735, -0.2453, -0.2189, -0.6403,  0.1461, -0.3892, -0.3638,  0.1967,\n",
       "                      -0.3444, -0.3732, -0.2118, -0.1732, -0.7756, -0.7076, -0.0654, -0.3057,\n",
       "                      -0.2316, -0.2749, -0.0192, -0.5112, -0.3534, -0.3142, -0.4422, -0.6408,\n",
       "                      -0.1298, -0.2533, -0.3032, -0.4673, -0.2066, -0.2858, -0.4452, -0.5167,\n",
       "                      -0.5261,  0.3231, -0.3390, -0.1759, -0.2902, -0.2319, -0.4067, -0.2068],\n",
       "                     device='mps:0')),\n",
       "             ('model.2.1.running_mean',\n",
       "              tensor([-1.9928,  3.0382, -6.5124, -2.5793, -2.6497, -2.0214, -1.1960, -0.7739,\n",
       "                      -3.1130, -2.6572, -2.8864, -3.6639, -2.5891, -1.7849, -0.1971, -1.2064,\n",
       "                      -4.4809, -3.9984, -4.9632, -0.7046, -2.1570,  0.9328, -0.2520, -2.5753,\n",
       "                      -3.9227,  2.5348, -1.7117,  3.9908, -1.9985,  0.1702, -4.8052, -4.0665,\n",
       "                      -2.4480, -1.6426, -3.2465,  3.1451, -7.5892,  0.2519, -1.9659,  5.7323,\n",
       "                      -0.5691, -1.1463, -3.8796, -2.2772,  0.0522, -3.3473, -3.8480, -6.5169,\n",
       "                      -0.8059, -1.1953, -1.6095, -4.3841, -1.7210, -5.1408, -4.3451, -3.5907,\n",
       "                      -2.9697, -2.5128, -2.1638, -0.7995, -1.1497, -1.0447, -0.7016, -2.9561],\n",
       "                     device='mps:0')),\n",
       "             ('model.2.1.running_var',\n",
       "              tensor([55.7063, 14.0940, 65.2682, 22.1385, 20.2429, 17.2499, 24.1205, 12.7838,\n",
       "                      15.0807, 19.4452, 17.7245, 19.9584, 29.9409, 28.9750, 13.0230, 15.5467,\n",
       "                      14.6561, 27.8253, 15.9239, 13.6248, 16.4803,  8.8682, 21.2981, 31.1923,\n",
       "                      32.0190, 18.9847, 21.8575, 12.7613, 24.6558, 10.7394, 49.6963, 22.5372,\n",
       "                      15.1605, 30.8096, 22.0495, 10.7526, 42.3788, 12.0234, 16.2590, 33.4382,\n",
       "                      23.1852, 22.7367, 26.1346, 32.4730, 22.2370, 30.9052, 25.3398, 21.3355,\n",
       "                      19.1165, 30.5844, 31.7406, 18.4297, 16.4332, 16.6414, 22.3340, 18.9612,\n",
       "                      22.1248, 31.1446, 25.2996, 15.0442, 16.4801,  8.6418, 24.4000, 20.9368],\n",
       "                     device='mps:0')),\n",
       "             ('model.2.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.3.0.weight',\n",
       "              tensor([[[[-0.0083,  0.1500, -0.0054],\n",
       "                        [-0.0723,  0.0230, -0.2313],\n",
       "                        [-0.0227, -0.0838, -0.0619]],\n",
       "              \n",
       "                       [[ 0.1004,  0.0312, -0.1434],\n",
       "                        [-0.0500, -0.0283, -0.2366],\n",
       "                        [-0.0978, -0.1703, -0.0821]],\n",
       "              \n",
       "                       [[-0.0550, -0.0554, -0.0895],\n",
       "                        [-0.0345,  0.0335,  0.0371],\n",
       "                        [ 0.0686,  0.1515,  0.0998]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1164,  0.1141,  0.1143],\n",
       "                        [-0.1796, -0.0766, -0.1126],\n",
       "                        [-0.0150, -0.0876,  0.0009]],\n",
       "              \n",
       "                       [[ 0.1904, -0.0516,  0.1672],\n",
       "                        [ 0.1991,  0.2198,  0.1382],\n",
       "                        [-0.0008,  0.0580, -0.1694]],\n",
       "              \n",
       "                       [[ 0.1730,  0.3803,  0.1922],\n",
       "                        [-0.2188,  0.0720, -0.0495],\n",
       "                        [-0.0767, -0.1826, -0.2539]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0702,  0.1338,  0.0045],\n",
       "                        [ 0.1851,  0.1291,  0.0475],\n",
       "                        [ 0.1103,  0.1706,  0.0817]],\n",
       "              \n",
       "                       [[-0.0374, -0.0801, -0.0948],\n",
       "                        [-0.0886, -0.1439, -0.1945],\n",
       "                        [-0.0125, -0.0558, -0.0249]],\n",
       "              \n",
       "                       [[ 0.0395,  0.0319, -0.1479],\n",
       "                        [ 0.0409,  0.0687, -0.1958],\n",
       "                        [ 0.1202,  0.1357, -0.1844]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0822,  0.0975, -0.0858],\n",
       "                        [-0.0877,  0.1262, -0.0104],\n",
       "                        [-0.1335,  0.1006,  0.1042]],\n",
       "              \n",
       "                       [[ 0.0050, -0.0523, -0.0383],\n",
       "                        [-0.0202, -0.1341, -0.1067],\n",
       "                        [-0.0758, -0.2085, -0.1334]],\n",
       "              \n",
       "                       [[ 0.0784,  0.0547, -0.0939],\n",
       "                        [-0.0526, -0.1416, -0.1999],\n",
       "                        [ 0.1080, -0.1683, -0.1837]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1442,  0.1356, -0.0660],\n",
       "                        [ 0.1100,  0.0578, -0.1165],\n",
       "                        [ 0.0167, -0.1166, -0.1031]],\n",
       "              \n",
       "                       [[-0.0341,  0.0712,  0.0497],\n",
       "                        [-0.0460, -0.0800, -0.0388],\n",
       "                        [ 0.0139, -0.0669, -0.0230]],\n",
       "              \n",
       "                       [[ 0.0204, -0.0644, -0.1453],\n",
       "                        [ 0.2527,  0.1273,  0.0850],\n",
       "                        [ 0.0836,  0.0221,  0.0422]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0093,  0.0234,  0.0585],\n",
       "                        [ 0.0175, -0.0381, -0.0210],\n",
       "                        [ 0.0135, -0.0540, -0.0571]],\n",
       "              \n",
       "                       [[ 0.0590,  0.0646,  0.0917],\n",
       "                        [-0.0856,  0.0141,  0.1569],\n",
       "                        [-0.0221, -0.0038,  0.1658]],\n",
       "              \n",
       "                       [[-0.1884, -0.0609,  0.1747],\n",
       "                        [-0.0564, -0.0908,  0.1538],\n",
       "                        [ 0.1046,  0.0723,  0.1130]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0940,  0.1750, -0.0022],\n",
       "                        [-0.1133,  0.0053,  0.0422],\n",
       "                        [-0.0602,  0.1732,  0.0832]],\n",
       "              \n",
       "                       [[ 0.0394,  0.1203,  0.0671],\n",
       "                        [-0.1050, -0.0093, -0.0752],\n",
       "                        [-0.1130,  0.1176, -0.0034]],\n",
       "              \n",
       "                       [[-0.3203, -0.0097,  0.1765],\n",
       "                        [-0.4532,  0.0232,  0.3331],\n",
       "                        [-0.2294, -0.1019,  0.1312]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0563,  0.0013,  0.0709],\n",
       "                        [ 0.2010, -0.1721,  0.0615],\n",
       "                        [ 0.1396, -0.0913,  0.2144]],\n",
       "              \n",
       "                       [[ 0.0123, -0.1960, -0.1536],\n",
       "                        [ 0.3914,  0.0433, -0.2460],\n",
       "                        [ 0.1856,  0.0222, -0.2020]],\n",
       "              \n",
       "                       [[-0.1993, -0.1158, -0.1832],\n",
       "                        [-0.0041,  0.2234, -0.0272],\n",
       "                        [-0.2288, -0.1128, -0.0563]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0531,  0.0115,  0.0063],\n",
       "                        [ 0.1512,  0.0624,  0.1500],\n",
       "                        [-0.0294,  0.0015,  0.0682]],\n",
       "              \n",
       "                       [[-0.1657, -0.2169, -0.0378],\n",
       "                        [-0.1453, -0.1703, -0.0532],\n",
       "                        [-0.1080, -0.1303, -0.0451]],\n",
       "              \n",
       "                       [[-0.0166,  0.0583,  0.0124],\n",
       "                        [-0.1330, -0.0136, -0.1286],\n",
       "                        [-0.1505, -0.0036, -0.0315]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0193, -0.0110,  0.0438],\n",
       "                        [ 0.1742,  0.0646,  0.1578],\n",
       "                        [-0.0017, -0.0186,  0.1116]],\n",
       "              \n",
       "                       [[ 0.0292, -0.0349, -0.0704],\n",
       "                        [-0.0202,  0.0031, -0.2036],\n",
       "                        [-0.0212, -0.1400, -0.1568]],\n",
       "              \n",
       "                       [[-0.0624, -0.0781, -0.1796],\n",
       "                        [-0.0022, -0.1193, -0.1671],\n",
       "                        [-0.0821, -0.2195, -0.1771]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1056,  0.0142, -0.2399],\n",
       "                        [-0.0049,  0.0453,  0.1754],\n",
       "                        [ 0.0466, -0.0077, -0.1130]],\n",
       "              \n",
       "                       [[ 0.0567,  0.1212,  0.1499],\n",
       "                        [-0.0321, -0.1286,  0.2002],\n",
       "                        [-0.0457,  0.0176, -0.0976]],\n",
       "              \n",
       "                       [[-0.0215,  0.0314,  0.0674],\n",
       "                        [-0.1464, -0.1990, -0.0037],\n",
       "                        [ 0.0149, -0.1820, -0.0784]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0761, -0.0619, -0.1314],\n",
       "                        [ 0.0161,  0.0051,  0.0543],\n",
       "                        [-0.1800, -0.0262, -0.0177]],\n",
       "              \n",
       "                       [[ 0.0266,  0.0476,  0.0704],\n",
       "                        [ 0.0892,  0.0362, -0.0326],\n",
       "                        [-0.1064,  0.0497, -0.0992]],\n",
       "              \n",
       "                       [[ 0.0980, -0.1993, -0.1468],\n",
       "                        [ 0.1409,  0.0760, -0.3261],\n",
       "                        [-0.1185, -0.0214,  0.0533]]]], device='mps:0')),\n",
       "             ('model.3.1.weight',\n",
       "              tensor([1.3190, 1.1180, 1.2382, 1.2546, 1.1259, 1.0365, 1.1074, 1.0645, 1.2794,\n",
       "                      1.2908, 1.2159, 1.2696, 1.2822, 1.3349, 1.3393, 1.2548, 1.4102, 1.4814,\n",
       "                      1.6202, 1.6525, 1.2932, 1.3967, 1.2795, 1.6835, 1.1727, 1.1932, 1.0921,\n",
       "                      1.0750, 1.2434, 1.5235, 1.0360, 1.1725, 1.4504, 1.3967, 1.0869, 1.2597,\n",
       "                      1.4732, 1.3651, 1.3401, 1.2821, 1.2864, 1.1884, 1.7954, 1.4907, 1.5965,\n",
       "                      1.6143, 1.4476, 1.3842, 1.2697, 0.9808, 1.0397, 1.4917, 1.4099, 1.2984,\n",
       "                      1.5375, 1.3735, 1.5544, 1.1962, 1.4156, 1.4147, 1.1754, 1.5876, 1.1456,\n",
       "                      1.1748], device='mps:0')),\n",
       "             ('model.3.1.bias',\n",
       "              tensor([-0.4854, -0.2668, -0.5873, -0.4109, -0.4938, -0.6440, -0.5770, -0.0908,\n",
       "                      -0.1735, -0.3019, -0.5167, -0.7163, -0.3336, -0.7762, -0.7664, -0.5621,\n",
       "                      -0.6404, -0.4426, -0.2865, -0.3600, -0.6672, -0.3504, -0.5030, -0.5217,\n",
       "                      -0.3071, -0.5105, -0.2730, -0.3122, -0.5838, -0.5779, -0.3074, -0.5794,\n",
       "                      -0.9491, -0.5583, -0.4573, -0.4525, -0.7482, -0.3548, -0.7252, -0.6638,\n",
       "                      -0.3966, -0.4201, -0.6708, -0.5003, -0.4045, -0.7621, -0.5259, -0.4067,\n",
       "                      -0.2220, -0.3420, -0.3101, -0.3821, -0.3612, -0.5837, -0.6755, -0.5457,\n",
       "                      -0.5992, -0.2263, -0.3358, -0.2267, -0.1568, -0.6493, -0.3560, -0.6185],\n",
       "                     device='mps:0')),\n",
       "             ('model.3.1.running_mean',\n",
       "              tensor([ -0.4888,  -5.6021,  -0.8027,  -4.3722,   3.8333,  -2.1104,  -4.5647,\n",
       "                        0.1274,  -5.2746,  -4.1091,  -0.5473,  -3.4791,  -2.7392,  -7.1562,\n",
       "                       -3.0521,  -2.5922,  -2.7966,  -5.7435,  -0.7540,  -3.5287,  -1.3683,\n",
       "                       -2.3207,  -1.3927,  -4.2081,  -3.2862,  -2.8587,  -4.6630,  -2.5314,\n",
       "                       -2.9189,  -2.7521,  -3.3458,   1.2015,  -4.6294,  -2.7083,   0.5240,\n",
       "                       -4.2818,  -6.4446,  -3.2725,  -6.1555,  -3.1081,  -5.0032,   0.0339,\n",
       "                       -1.4064,  -6.2180,  -5.2362,  -1.5057,  -6.0688,  -1.3179,  -2.7370,\n",
       "                       -3.2648,  -1.4364,  -3.8325,  -4.3731,  -4.9827,  -5.5363,  -2.1743,\n",
       "                       -3.5827,  -3.9385,  -4.7900, -10.1379,  -5.0204,  -2.8411,  -7.1180,\n",
       "                       -1.6577], device='mps:0')),\n",
       "             ('model.3.1.running_var',\n",
       "              tensor([16.8949, 12.5606,  9.8843, 16.4332, 16.1254, 10.8427, 10.5227, 11.8037,\n",
       "                      13.3581, 12.0905, 13.1394, 12.7650, 15.4342, 13.9630, 10.2298,  9.4212,\n",
       "                      11.5822, 16.5965, 25.2216, 19.5119, 13.0804, 15.1732, 16.8932, 20.7458,\n",
       "                      12.7237,  6.6458, 10.7897, 10.4430, 11.8725, 13.3705,  9.0570, 11.9949,\n",
       "                      11.5323, 12.3209, 14.3645, 11.6610, 12.7421, 15.6724, 12.5693, 10.1125,\n",
       "                       9.3820,  9.3522, 14.4353, 11.6209, 27.1242, 15.4302, 12.3543, 15.8935,\n",
       "                      11.2553,  7.3714,  9.8187, 20.9656, 12.3698, 13.3801, 16.7866, 16.1229,\n",
       "                      12.5393, 12.8755,  9.3663, 27.3126, 14.5344, 14.2208, 12.8565, 15.5489],\n",
       "                     device='mps:0')),\n",
       "             ('model.3.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.4.0.weight',\n",
       "              tensor([[[[-0.1176,  0.0107, -0.0429],\n",
       "                        [ 0.1977,  0.2009,  0.1522],\n",
       "                        [ 0.1119,  0.0461, -0.1063]],\n",
       "              \n",
       "                       [[-0.0538, -0.0435, -0.2097],\n",
       "                        [ 0.0731,  0.0501,  0.0081],\n",
       "                        [-0.0064,  0.0285,  0.0931]],\n",
       "              \n",
       "                       [[ 0.0086,  0.0426,  0.0070],\n",
       "                        [ 0.0221, -0.0206,  0.0221],\n",
       "                        [-0.0106, -0.0497, -0.0731]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0120, -0.0125, -0.0387],\n",
       "                        [-0.0222, -0.0578,  0.1192],\n",
       "                        [-0.0322,  0.0282,  0.1220]],\n",
       "              \n",
       "                       [[-0.0295, -0.1214, -0.1559],\n",
       "                        [-0.0158, -0.0017,  0.0271],\n",
       "                        [ 0.1386,  0.2121,  0.1496]],\n",
       "              \n",
       "                       [[ 0.2929,  0.3589,  0.1960],\n",
       "                        [ 0.2914,  0.4306,  0.2696],\n",
       "                        [ 0.0058,  0.0098, -0.0213]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0259, -0.0383, -0.1965],\n",
       "                        [ 0.0822,  0.0561,  0.1431],\n",
       "                        [-0.1519, -0.0313,  0.2850]],\n",
       "              \n",
       "                       [[ 0.2061,  0.1306, -0.1561],\n",
       "                        [ 0.0655, -0.0775, -0.2238],\n",
       "                        [-0.0571, -0.2417, -0.2259]],\n",
       "              \n",
       "                       [[-0.1108,  0.0578,  0.2150],\n",
       "                        [-0.1000,  0.1666,  0.2679],\n",
       "                        [-0.0473,  0.1298,  0.1487]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.2559, -0.1803,  0.0141],\n",
       "                        [-0.2492, -0.0955,  0.0846],\n",
       "                        [-0.1614, -0.0608, -0.0487]],\n",
       "              \n",
       "                       [[-0.1505,  0.0650,  0.0168],\n",
       "                        [-0.1898, -0.0146,  0.1504],\n",
       "                        [-0.0555,  0.1159,  0.1735]],\n",
       "              \n",
       "                       [[ 0.1641,  0.2030,  0.0422],\n",
       "                        [ 0.1504, -0.1502, -0.0497],\n",
       "                        [ 0.1450, -0.1265, -0.2116]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2430, -0.1891, -0.1834],\n",
       "                        [-0.0622, -0.1214, -0.2139],\n",
       "                        [ 0.5142,  0.2361,  0.1361]],\n",
       "              \n",
       "                       [[-0.1484, -0.1370, -0.1995],\n",
       "                        [-0.1627, -0.1367, -0.2503],\n",
       "                        [-0.0077,  0.0106, -0.0370]],\n",
       "              \n",
       "                       [[ 0.0856,  0.0673,  0.0646],\n",
       "                        [-0.0218, -0.0816, -0.1267],\n",
       "                        [ 0.0535, -0.0434, -0.1558]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0121, -0.0453, -0.0152],\n",
       "                        [-0.0701, -0.0980, -0.0884],\n",
       "                        [-0.1358,  0.0586,  0.1427]],\n",
       "              \n",
       "                       [[ 0.0969,  0.0974,  0.0183],\n",
       "                        [ 0.0044,  0.0136, -0.0318],\n",
       "                        [-0.0781, -0.0261, -0.0914]],\n",
       "              \n",
       "                       [[-0.0347, -0.0168,  0.0208],\n",
       "                        [-0.0017,  0.0636,  0.0129],\n",
       "                        [-0.0184,  0.0770,  0.1377]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0739, -0.2572, -0.0998],\n",
       "                        [ 0.0658, -0.1494, -0.1830],\n",
       "                        [-0.0858, -0.2126, -0.2987]],\n",
       "              \n",
       "                       [[ 0.0018,  0.1105, -0.1209],\n",
       "                        [ 0.1622,  0.3122,  0.0259],\n",
       "                        [ 0.0030,  0.1779,  0.0544]],\n",
       "              \n",
       "                       [[-0.1063, -0.1610, -0.1484],\n",
       "                        [-0.0393, -0.1491, -0.1667],\n",
       "                        [ 0.1375,  0.0430,  0.0804]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1852, -0.1992, -0.2095],\n",
       "                        [-0.1139, -0.1705, -0.1691],\n",
       "                        [-0.0067, -0.0662, -0.1142]],\n",
       "              \n",
       "                       [[ 0.0605, -0.0094,  0.0999],\n",
       "                        [ 0.0206, -0.0034,  0.0122],\n",
       "                        [-0.0587, -0.0027, -0.0717]],\n",
       "              \n",
       "                       [[ 0.0370,  0.1835,  0.1163],\n",
       "                        [ 0.0177,  0.2201,  0.2675],\n",
       "                        [ 0.0089,  0.1282,  0.1708]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2747,  0.0768, -0.0370],\n",
       "                        [ 0.0990, -0.0089, -0.0917],\n",
       "                        [-0.1302, -0.0299, -0.0374]],\n",
       "              \n",
       "                       [[ 0.0270,  0.0469,  0.1108],\n",
       "                        [-0.0093,  0.1143,  0.1700],\n",
       "                        [-0.1847, -0.1940, -0.1680]],\n",
       "              \n",
       "                       [[-0.0548, -0.1692, -0.2138],\n",
       "                        [-0.0183, -0.2422, -0.1510],\n",
       "                        [-0.0006, -0.2289, -0.1065]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2382,  0.0461, -0.1419],\n",
       "                        [ 0.1135,  0.0208,  0.0111],\n",
       "                        [-0.1012, -0.0770,  0.0903]],\n",
       "              \n",
       "                       [[-0.1574, -0.0317, -0.1266],\n",
       "                        [ 0.0207,  0.1278,  0.0411],\n",
       "                        [-0.0558,  0.1040,  0.0760]],\n",
       "              \n",
       "                       [[-0.1348,  0.0779,  0.1519],\n",
       "                        [ 0.0086,  0.1342,  0.2490],\n",
       "                        [ 0.1279,  0.2465,  0.1962]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0512,  0.0185,  0.1555],\n",
       "                        [-0.1807, -0.0139,  0.2570],\n",
       "                        [-0.1918, -0.0514,  0.0820]],\n",
       "              \n",
       "                       [[ 0.1704,  0.0982, -0.0390],\n",
       "                        [ 0.0485,  0.0422, -0.0185],\n",
       "                        [-0.0411, -0.1185, -0.2182]],\n",
       "              \n",
       "                       [[-0.2099, -0.1831, -0.0087],\n",
       "                        [-0.1119,  0.0168,  0.1992],\n",
       "                        [-0.0338,  0.0972,  0.1409]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1506, -0.0294,  0.0917],\n",
       "                        [ 0.1225,  0.0391,  0.0012],\n",
       "                        [-0.0934, -0.2671, -0.1637]],\n",
       "              \n",
       "                       [[-0.0131, -0.0187, -0.0307],\n",
       "                        [ 0.0254, -0.0020, -0.0125],\n",
       "                        [ 0.0163, -0.0057, -0.0451]],\n",
       "              \n",
       "                       [[ 0.0298, -0.0842, -0.2562],\n",
       "                        [-0.0779, -0.1239, -0.2586],\n",
       "                        [ 0.0265, -0.0361, -0.2389]]]], device='mps:0')),\n",
       "             ('model.4.1.weight',\n",
       "              tensor([1.2667, 1.4633, 1.4652, 1.2473, 1.2678, 1.3275, 1.2704, 1.3103, 1.3267,\n",
       "                      1.3089, 1.1075, 1.1774, 1.2050, 1.2696, 1.1714, 1.5410, 1.3020, 1.1395,\n",
       "                      1.1654, 1.1934, 1.3384, 1.2071, 1.4387, 1.2159, 1.2397, 1.2435, 1.4120,\n",
       "                      1.3159, 1.2508, 1.4048, 1.6459, 1.2926, 1.4018, 1.0484, 1.1737, 1.2419,\n",
       "                      1.3507, 1.0782, 1.3316, 1.2210, 1.3727, 1.2936, 1.3843, 1.1571, 1.3361,\n",
       "                      1.4103, 1.6268, 1.4152, 1.2670, 1.2594, 1.3058, 1.2780, 1.2616, 1.3644,\n",
       "                      1.2317, 1.5056, 1.6880, 1.2385, 1.4461, 1.5951, 1.2388, 1.2257, 1.4276,\n",
       "                      1.2208], device='mps:0')),\n",
       "             ('model.4.1.bias',\n",
       "              tensor([-0.6408, -0.6198, -0.7592, -0.4416, -0.4156, -0.2914, -0.3514, -0.3897,\n",
       "                      -0.4619, -0.5087, -0.6733, -0.3757, -0.6500, -0.3709, -0.2966, -0.4767,\n",
       "                      -0.6845, -0.3249, -0.6998, -0.6008, -0.7427, -0.5350, -0.5317, -0.0403,\n",
       "                      -0.4801, -0.3535, -0.4311, -0.2167, -0.5179, -0.7103, -1.2621, -0.2590,\n",
       "                      -0.7217, -0.4630, -0.3937, -0.1279, -0.7425, -0.3795, -0.5655, -0.3992,\n",
       "                      -0.6073, -0.5747, -0.5160, -0.2287, -0.4712, -0.4138, -0.4513, -0.4475,\n",
       "                      -0.1692, -0.8053, -0.4273, -0.3812, -0.3055, -0.4501, -0.6182, -0.6035,\n",
       "                      -0.6540, -1.2893, -0.2947, -1.0344, -0.5830, -0.1799, -0.5099, -0.4666],\n",
       "                     device='mps:0')),\n",
       "             ('model.4.1.running_mean',\n",
       "              tensor([-0.1712, -2.9924, -3.9894, -3.7864, -0.4492, -0.5111, -1.9707,  0.2430,\n",
       "                      -3.3982, -2.9897, -2.7136, -3.6269, -1.5591, -0.5647,  1.7384, -5.5602,\n",
       "                      -0.7546, -1.8628, -0.7234, -2.7783, -4.3481, -0.9485, -5.6084, -5.9963,\n",
       "                      -4.2262, -2.9135, -0.9096, -2.3539, -2.8570, -1.7209, -5.6677, -4.7934,\n",
       "                      -0.3744, -2.1989, -3.3858, -1.3962, -4.3127, -3.4048,  0.4998, -1.2620,\n",
       "                      -6.0606,  0.2153, -6.9155, -0.7795, -3.5615, -1.5350, -4.2344,  0.6406,\n",
       "                      -1.8597, -3.0793, -0.6737, -7.0803, -5.9711, -3.1867, -1.6543, -2.5406,\n",
       "                      -2.3667,  2.4432, -2.5341, -1.2085, -5.0150, -3.8083, -4.3958, -3.2634],\n",
       "                     device='mps:0')),\n",
       "             ('model.4.1.running_var',\n",
       "              tensor([ 9.2968, 14.5978, 12.4169, 15.6569, 10.9786, 11.1443, 12.0076, 11.0371,\n",
       "                      13.0240, 16.7714, 15.1350, 14.0162, 13.4145, 15.1080,  7.8095, 12.5433,\n",
       "                      14.3805, 11.1915,  8.3504,  9.7478,  8.9307, 11.0929, 12.6689, 17.2849,\n",
       "                      13.1578,  9.0983, 11.1299, 11.2598,  8.2720, 11.4439, 15.6425, 18.8047,\n",
       "                      16.7939, 12.8289,  9.0320, 14.2778,  9.3551,  7.4770,  8.5497, 18.9538,\n",
       "                      12.2480, 12.8766, 13.4617, 13.5801, 20.2872, 13.4006, 34.3165, 14.4026,\n",
       "                      19.9387,  8.7656, 13.6607, 14.7772, 24.2815, 11.3039, 12.5187, 17.9699,\n",
       "                      23.4652,  7.1584, 17.3399,  9.9262, 12.7152, 14.1361, 14.4367, 10.9589],\n",
       "                     device='mps:0')),\n",
       "             ('model.4.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.5.0.weight',\n",
       "              tensor([[[[-0.0205,  0.0247,  0.0467],\n",
       "                        [ 0.0772,  0.1259,  0.0626],\n",
       "                        [ 0.1496,  0.1431,  0.1448]],\n",
       "              \n",
       "                       [[ 0.1279,  0.0364,  0.0453],\n",
       "                        [ 0.0520, -0.0388, -0.0640],\n",
       "                        [-0.0325, -0.0919, -0.0671]],\n",
       "              \n",
       "                       [[-0.0360, -0.0230, -0.0184],\n",
       "                        [ 0.0748,  0.1162,  0.0832],\n",
       "                        [ 0.0859,  0.0921,  0.0654]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0460, -0.0099, -0.0158],\n",
       "                        [ 0.0531,  0.1059,  0.0305],\n",
       "                        [ 0.1434,  0.2077,  0.1595]],\n",
       "              \n",
       "                       [[-0.0768,  0.0232,  0.0048],\n",
       "                        [-0.0434,  0.0096, -0.0145],\n",
       "                        [-0.0662, -0.0897, -0.0662]],\n",
       "              \n",
       "                       [[ 0.0391,  0.0036, -0.0158],\n",
       "                        [-0.0323, -0.0143,  0.0606],\n",
       "                        [-0.0804, -0.0757, -0.0080]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0415, -0.0392, -0.1055],\n",
       "                        [ 0.0267,  0.0222,  0.1402],\n",
       "                        [ 0.1352,  0.1365,  0.1584]],\n",
       "              \n",
       "                       [[-0.0452, -0.0955, -0.0099],\n",
       "                        [ 0.0216, -0.0048, -0.0030],\n",
       "                        [ 0.1167,  0.0180,  0.0296]],\n",
       "              \n",
       "                       [[ 0.0756,  0.0569,  0.0418],\n",
       "                        [ 0.0625,  0.0471,  0.0101],\n",
       "                        [-0.0270, -0.0600, -0.0563]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0248, -0.0065, -0.1094],\n",
       "                        [ 0.1249,  0.0277, -0.0280],\n",
       "                        [ 0.0777,  0.0496,  0.0189]],\n",
       "              \n",
       "                       [[ 0.0467,  0.0615, -0.0585],\n",
       "                        [-0.0526, -0.0643, -0.0714],\n",
       "                        [-0.0739, -0.0950, -0.1224]],\n",
       "              \n",
       "                       [[-0.0545, -0.0589, -0.0288],\n",
       "                        [-0.0756, -0.0372, -0.0241],\n",
       "                        [-0.0644, -0.0760, -0.0891]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0256, -0.0905,  0.0598],\n",
       "                        [-0.0650, -0.0473,  0.0266],\n",
       "                        [-0.1142, -0.1023, -0.0545]],\n",
       "              \n",
       "                       [[-0.0794, -0.1641, -0.2534],\n",
       "                        [-0.1043, -0.1951, -0.2406],\n",
       "                        [-0.0295, -0.0298, -0.1966]],\n",
       "              \n",
       "                       [[-0.1387,  0.0119, -0.0139],\n",
       "                        [-0.0163,  0.0771,  0.0741],\n",
       "                        [ 0.0339, -0.0287, -0.0170]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0377, -0.0585, -0.0464],\n",
       "                        [ 0.0158, -0.0892, -0.0256],\n",
       "                        [-0.0705, -0.0818, -0.0254]],\n",
       "              \n",
       "                       [[ 0.0407, -0.0483, -0.0504],\n",
       "                        [ 0.0868,  0.0413, -0.0414],\n",
       "                        [ 0.0294,  0.0045,  0.0348]],\n",
       "              \n",
       "                       [[-0.0209, -0.0362, -0.0505],\n",
       "                        [-0.1339, -0.1714, -0.1521],\n",
       "                        [-0.1785, -0.1308, -0.0697]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0389, -0.0631, -0.0263],\n",
       "                        [-0.0772, -0.1617, -0.0986],\n",
       "                        [ 0.0463, -0.0223, -0.0764]],\n",
       "              \n",
       "                       [[ 0.0483, -0.0113,  0.0040],\n",
       "                        [ 0.0447, -0.0682,  0.0025],\n",
       "                        [-0.0331,  0.0082,  0.0393]],\n",
       "              \n",
       "                       [[-0.0326, -0.0277, -0.0194],\n",
       "                        [-0.0448, -0.0107, -0.0672],\n",
       "                        [ 0.0269,  0.0913,  0.1017]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1041,  0.1090,  0.1676],\n",
       "                        [ 0.0436, -0.0584,  0.0537],\n",
       "                        [ 0.0176, -0.1275, -0.1277]],\n",
       "              \n",
       "                       [[-0.1125, -0.0690, -0.1392],\n",
       "                        [-0.0446, -0.0037, -0.0108],\n",
       "                        [-0.0512,  0.0039,  0.1312]],\n",
       "              \n",
       "                       [[-0.0469, -0.0614, -0.1127],\n",
       "                        [-0.0237, -0.0179, -0.0512],\n",
       "                        [-0.0174,  0.0101,  0.0169]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0629,  0.0406,  0.0430],\n",
       "                        [ 0.0305,  0.0280,  0.0607],\n",
       "                        [-0.0104,  0.0841,  0.0612]],\n",
       "              \n",
       "                       [[ 0.0005, -0.0212, -0.0243],\n",
       "                        [-0.0570, -0.1607, -0.0841],\n",
       "                        [-0.0319, -0.1210, -0.1309]],\n",
       "              \n",
       "                       [[-0.0665, -0.0370, -0.0666],\n",
       "                        [-0.0038, -0.0154,  0.0040],\n",
       "                        [-0.0663, -0.0978,  0.0248]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1144,  0.1104,  0.0134],\n",
       "                        [ 0.0984,  0.0386, -0.0228],\n",
       "                        [-0.0412, -0.0298,  0.0089]],\n",
       "              \n",
       "                       [[-0.0660, -0.1089, -0.1419],\n",
       "                        [ 0.0575,  0.0316,  0.0074],\n",
       "                        [ 0.0342, -0.0590, -0.0515]],\n",
       "              \n",
       "                       [[-0.0121,  0.0391,  0.0767],\n",
       "                        [-0.0518, -0.0084,  0.0083],\n",
       "                        [-0.0430, -0.0959, -0.0616]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1468,  0.2227,  0.1510],\n",
       "                        [ 0.0730,  0.1294,  0.1378],\n",
       "                        [-0.0169, -0.0746, -0.0816]],\n",
       "              \n",
       "                       [[ 0.0389,  0.0496, -0.0165],\n",
       "                        [ 0.1249,  0.0191,  0.0284],\n",
       "                        [ 0.1653,  0.0833,  0.0912]],\n",
       "              \n",
       "                       [[-0.0023,  0.1798,  0.1027],\n",
       "                        [-0.0187,  0.0257,  0.0559],\n",
       "                        [ 0.0389,  0.0220, -0.0217]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0811,  0.0605,  0.0709],\n",
       "                        [-0.0063, -0.0554, -0.0178],\n",
       "                        [-0.0323, -0.0548, -0.0778]],\n",
       "              \n",
       "                       [[ 0.0766,  0.0812,  0.1239],\n",
       "                        [ 0.0490,  0.0838,  0.0445],\n",
       "                        [-0.0417,  0.0430, -0.0313]],\n",
       "              \n",
       "                       [[ 0.0398,  0.0680,  0.1484],\n",
       "                        [ 0.0247,  0.0051,  0.0182],\n",
       "                        [-0.0787, -0.0992, -0.0988]]]], device='mps:0')),\n",
       "             ('model.5.1.weight',\n",
       "              tensor([1.0140, 0.9736, 1.2556, 1.2318, 0.9264, 1.0344, 1.0676, 1.1272, 1.1088,\n",
       "                      1.0506, 1.2019, 1.1125, 1.0218, 1.0692, 1.1038, 1.3065, 0.9893, 1.0874,\n",
       "                      1.0050, 1.0805, 0.9795, 0.9976, 1.1720, 1.1852, 0.9186, 1.0760, 1.0798,\n",
       "                      1.0449, 0.9614, 0.9175, 1.2760, 1.0854, 1.2161, 1.1676, 1.0621, 1.0518,\n",
       "                      0.9884, 0.8626, 1.1106, 1.1950, 1.1592, 1.0935, 1.1499, 1.0638, 0.9965,\n",
       "                      0.9562, 1.0989, 1.3429, 0.9100, 1.0978, 1.0770, 0.9810, 1.0555, 1.1279,\n",
       "                      1.2253, 0.9577, 1.0505, 0.9827, 1.0881, 0.9525, 0.7627, 1.1386, 1.1727,\n",
       "                      0.9317, 1.0694, 1.1305, 1.1657, 1.1649, 1.0793, 0.9340, 1.0519, 1.2099,\n",
       "                      1.0100, 0.8113, 1.1334, 0.8933, 1.2030, 1.1499, 1.0374, 1.0672, 0.8542,\n",
       "                      1.1928, 1.1136, 1.2677, 1.2306, 1.0328, 1.0910, 1.1113, 1.1547, 1.1631,\n",
       "                      1.0613, 1.0297, 0.9618, 0.9999, 1.2014, 1.0486, 1.1726, 1.0459, 1.0579,\n",
       "                      1.1641, 1.1641, 1.0820, 1.1023, 1.1828, 1.0458, 1.2258, 0.9245, 0.9279,\n",
       "                      1.0948, 1.1835, 1.1201, 0.8823, 0.9543, 1.0699, 1.1234, 1.1590, 0.7684,\n",
       "                      1.1993, 1.0217, 1.1087, 1.0649, 1.2777, 1.1501, 1.1071, 0.9283, 1.1411,\n",
       "                      0.8879, 0.9850], device='mps:0')),\n",
       "             ('model.5.1.bias',\n",
       "              tensor([-0.5263, -0.5478, -0.4994, -0.5335, -0.5097, -0.5218, -0.4032, -0.6312,\n",
       "                      -0.5050, -0.5273, -0.6893, -0.5927, -0.6077, -0.4639, -0.5039, -0.6330,\n",
       "                      -0.7610, -0.4761, -0.3610, -0.3871, -0.4580, -0.5331, -0.5201, -0.6251,\n",
       "                      -0.6084, -0.5871, -0.5085, -0.4806, -0.5408, -0.4773, -0.3139, -0.5220,\n",
       "                      -0.2583, -0.5592, -0.5345, -0.4714, -0.5243, -0.4496, -0.5270, -0.4659,\n",
       "                      -0.2209, -0.2945, -0.5870, -0.4366, -0.4439, -0.4272, -0.7581, -0.2973,\n",
       "                      -0.3479, -0.3981, -0.6374, -0.1865, -0.4967, -0.4857, -0.5466, -0.3689,\n",
       "                      -0.7173, -0.6589, -0.3944, -0.4567, -0.3389, -0.4487, -0.3903, -0.5621,\n",
       "                      -0.4034, -0.4803, -0.3478, -1.0259, -0.4243, -0.5426, -0.5957, -0.3485,\n",
       "                      -0.4469, -0.3792, -0.5411, -0.5224, -0.6441, -0.4373, -0.4811, -0.4358,\n",
       "                      -0.5821, -0.4319, -0.5532, -0.4229, -0.4360, -0.4681, -0.3269, -0.3979,\n",
       "                      -0.4030, -0.6962, -0.2599, -0.7554, -0.4021, -0.6306, -0.5172, -0.6080,\n",
       "                      -0.4151, -0.4502, -0.8285, -0.3271, -0.6280, -0.4987, -0.6136, -0.4464,\n",
       "                      -0.6666, -0.4909, -0.5058, -0.5042, -0.6150, -0.2558, -0.1543, -0.5681,\n",
       "                      -0.6638, -0.6351, -0.4955, -0.1856, -0.5583, -0.4768, -0.4187, -0.4695,\n",
       "                      -0.4704, -0.2672, -0.3975, -0.3925, -0.6803, -0.3118, -0.5160, -0.5535],\n",
       "                     device='mps:0')),\n",
       "             ('model.5.1.running_mean',\n",
       "              tensor([-0.9107, -1.7900, -2.0575, -2.7286, -0.1095, -4.3137, -0.7282, -0.0096,\n",
       "                      -2.2766, -3.7708, -3.2161,  0.1168, -1.8256,  0.5896, -1.2735, -2.5248,\n",
       "                       1.2874, -2.0714, -2.6182, -2.3522, -3.3696, -2.6149, -0.5353, -3.3668,\n",
       "                      -2.8591, -3.8399, -1.3838, -2.3339, -1.2429, -1.3038, -3.5862, -1.1368,\n",
       "                      -4.6798, -2.1754, -4.7398, -1.4704, -2.6670, -2.0895, -2.3351, -1.3321,\n",
       "                      -4.8913, -0.7241, -3.9685, -0.8153, -2.1489, -2.9554,  2.5546,  0.2889,\n",
       "                      -2.7581, -1.6367, -0.7957, -0.5689, -0.1830, -3.8407, -2.0927, -2.3386,\n",
       "                      -0.4910,  0.0958, -0.3657, -3.8049, -2.1003, -1.2631, -1.6398, -0.2197,\n",
       "                      -2.9286, -2.4470, -1.7715, -3.3434, -2.2361, -2.5269, -2.1939, -0.5478,\n",
       "                       0.4924, -2.8499, -2.9747, -2.8869, -2.6982,  1.1808,  1.0057, -1.9344,\n",
       "                       1.7154, -2.2927, -1.4055, -0.2220,  1.8079, -0.3228, -4.7191, -1.6164,\n",
       "                      -2.1280, -1.7292, -3.2136, -0.7913, -1.8100,  0.6174,  2.6839,  0.7837,\n",
       "                      -1.4608, -3.4953, -2.9808, -2.2763,  2.4667, -0.5746, -1.2432, -3.7246,\n",
       "                      -0.3427, -1.8846, -2.3365, -1.9829, -2.5499, -3.8720, -2.2094,  2.4115,\n",
       "                      -3.4715,  0.6986, -3.7998, -3.9402,  0.0683, -2.3245,  0.7748, -1.5087,\n",
       "                      -4.3211,  2.0422, -1.6110, -3.2429, -0.1015, -3.0634, -0.6176, -0.7073],\n",
       "                     device='mps:0')),\n",
       "             ('model.5.1.running_var',\n",
       "              tensor([ 7.3390,  5.8133, 10.4117,  6.9549,  5.5715,  9.2830,  7.1265,  8.9383,\n",
       "                       5.9799,  6.3383,  6.8513,  5.4686,  5.5909,  6.5746,  7.4847,  9.3103,\n",
       "                      10.4039,  6.8292,  9.1206,  8.1145,  5.5759,  8.7575,  8.0304,  7.1336,\n",
       "                       7.1955,  5.9291,  9.2876,  5.9773,  5.9917,  4.2327,  8.1502,  6.2169,\n",
       "                       9.4943,  8.1296,  7.5176,  6.9142,  7.2648,  8.0803,  5.7002,  7.5392,\n",
       "                       8.6073,  7.4584,  9.5716,  7.0691,  5.0327,  7.2253,  8.3937,  6.9662,\n",
       "                       7.4576,  6.9791,  5.7069,  6.6470,  8.0972,  7.9759,  7.3617,  4.6515,\n",
       "                       9.9394,  6.9997,  6.1799,  9.2857,  4.8129,  5.0629,  7.7315,  5.4794,\n",
       "                       7.8451,  6.2684,  6.5667,  8.8231, 12.3260,  5.6659,  7.8807,  7.3687,\n",
       "                       5.3254,  7.4585, 10.1799,  6.1135,  6.1652,  7.4181,  5.9595,  7.1828,\n",
       "                      10.6328,  9.4817,  6.8109,  7.7314,  8.8330,  7.3143,  7.2334,  5.0406,\n",
       "                       9.6885,  9.8645,  8.6359,  8.4416,  6.2059, 10.1041,  7.0868,  9.3789,\n",
       "                       8.1381, 10.0535,  7.0274,  5.5471,  5.2143,  6.0760,  8.7418,  9.1028,\n",
       "                       8.5876,  7.3243, 10.3937,  5.7336,  7.0564,  5.9117,  7.3540,  6.6826,\n",
       "                       8.1686,  6.7226,  6.6651,  7.4824,  5.7141,  8.1723,  5.5397,  7.0979,\n",
       "                       7.3226,  9.0407,  8.2133,  6.5011,  6.7883,  6.3399,  8.1782,  5.9610],\n",
       "                     device='mps:0')),\n",
       "             ('model.5.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.6.0.weight',\n",
       "              tensor([[[[-1.5354e-02,  1.6217e-02,  1.7414e-02],\n",
       "                        [ 1.6167e-02,  1.3051e-02,  1.5055e-02],\n",
       "                        [ 2.0788e-01,  1.7980e-01,  7.1811e-02]],\n",
       "              \n",
       "                       [[-1.3643e-01, -1.5158e-01, -1.0574e-01],\n",
       "                        [-1.3999e-01, -1.2414e-01, -8.7790e-02],\n",
       "                        [ 2.7520e-02,  4.3926e-02, -3.7491e-02]],\n",
       "              \n",
       "                       [[ 1.2896e-02,  3.6678e-02,  6.1837e-02],\n",
       "                        [-4.8332e-02, -1.4126e-02,  2.9848e-02],\n",
       "                        [-1.3820e-01, -1.3733e-01, -9.9609e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.9680e-02,  4.9325e-02,  3.0791e-02],\n",
       "                        [-1.2682e-02, -1.9302e-02, -1.4824e-03],\n",
       "                        [ 1.1458e-02,  4.3487e-02, -8.9041e-03]],\n",
       "              \n",
       "                       [[-1.6942e-01, -3.8658e-02, -3.2368e-02],\n",
       "                        [-5.1043e-02, -1.5225e-02, -7.8275e-02],\n",
       "                        [ 3.4903e-02, -2.3936e-02, -7.7602e-02]],\n",
       "              \n",
       "                       [[-2.6266e-02, -1.7725e-02,  7.9433e-02],\n",
       "                        [ 3.0650e-02, -7.7182e-05,  3.9189e-02],\n",
       "                        [ 8.2557e-02, -1.2076e-02,  2.5773e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.9670e-02,  1.3784e-01,  1.2194e-01],\n",
       "                        [ 2.8056e-03, -4.7721e-02, -2.3364e-02],\n",
       "                        [-1.3856e-01, -8.9754e-02, -1.1520e-01]],\n",
       "              \n",
       "                       [[-6.2918e-02, -1.9526e-02, -3.8300e-02],\n",
       "                        [-7.4681e-02, -7.5941e-02, -1.0325e-02],\n",
       "                        [-1.0021e-01,  8.1888e-03,  5.9245e-03]],\n",
       "              \n",
       "                       [[ 2.4499e-01,  1.9405e-01, -4.6967e-02],\n",
       "                        [ 5.3671e-02, -1.4612e-02, -5.6238e-02],\n",
       "                        [ 1.7039e-02, -6.5469e-02,  2.0335e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3782e-01, -5.5361e-02, -5.8022e-02],\n",
       "                        [-2.7038e-02, -1.2664e-01, -9.8652e-02],\n",
       "                        [-6.0174e-02, -2.7194e-02,  8.4918e-03]],\n",
       "              \n",
       "                       [[ 4.5778e-02,  3.2863e-02, -8.2634e-03],\n",
       "                        [-1.1836e-01, -1.0031e-02,  1.4164e-01],\n",
       "                        [-1.2959e-01, -5.9157e-02,  5.2683e-02]],\n",
       "              \n",
       "                       [[-1.0234e-01, -7.7967e-02, -8.1150e-02],\n",
       "                        [-7.8564e-02, -9.0317e-02, -7.2680e-02],\n",
       "                        [ 3.7609e-02,  3.1480e-02, -2.8134e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.7578e-03, -3.5649e-02, -4.9075e-02],\n",
       "                        [-5.6350e-02, -6.6085e-02, -7.1923e-02],\n",
       "                        [-5.3546e-02,  1.1262e-01, -2.7459e-02]],\n",
       "              \n",
       "                       [[-4.5255e-03, -1.8942e-02, -8.3203e-02],\n",
       "                        [ 2.4911e-02, -1.0033e-01, -1.1002e-01],\n",
       "                        [-3.1624e-03,  5.0052e-02,  5.3223e-02]],\n",
       "              \n",
       "                       [[-3.2017e-02,  1.0766e-01, -3.9899e-03],\n",
       "                        [-7.0967e-02,  8.7982e-02, -3.3040e-02],\n",
       "                        [-1.0673e-01, -5.3850e-02, -7.4965e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.9319e-02, -6.7305e-02, -3.0483e-02],\n",
       "                        [ 1.2687e-02, -1.5986e-01, -2.8070e-02],\n",
       "                        [ 1.0238e-01,  5.7692e-02,  5.8612e-02]],\n",
       "              \n",
       "                       [[-3.9243e-02, -5.6864e-02, -3.9086e-02],\n",
       "                        [ 5.4323e-03, -5.7328e-02, -8.1355e-02],\n",
       "                        [ 1.2421e-01, -7.4687e-02, -7.1855e-02]],\n",
       "              \n",
       "                       [[ 5.9919e-02,  1.8137e-02,  1.9677e-02],\n",
       "                        [ 8.8455e-02,  1.2633e-01,  1.1621e-01],\n",
       "                        [ 7.9000e-02,  8.2516e-02,  5.1222e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0566e-02, -3.5078e-02, -2.1313e-02],\n",
       "                        [ 4.2362e-02,  7.4342e-02, -1.2727e-02],\n",
       "                        [ 6.6565e-02,  7.7948e-03, -2.2316e-02]],\n",
       "              \n",
       "                       [[ 2.3791e-02, -2.3853e-02,  3.1778e-02],\n",
       "                        [ 8.4099e-02, -1.8164e-02,  6.9623e-02],\n",
       "                        [ 5.9658e-02, -1.9268e-02,  3.2313e-02]],\n",
       "              \n",
       "                       [[-7.3192e-02, -1.8820e-02,  4.4859e-02],\n",
       "                        [-1.1540e-01, -1.8180e-01, -4.8628e-02],\n",
       "                        [-2.4636e-01, -1.1763e-01,  5.9367e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.5585e-03,  1.8020e-03,  1.1613e-01],\n",
       "                        [-4.1104e-02, -3.0720e-02, -1.4741e-01],\n",
       "                        [ 4.5172e-02, -2.4631e-02, -6.5192e-02]],\n",
       "              \n",
       "                       [[-3.2372e-02, -7.7876e-02,  6.7020e-02],\n",
       "                        [-3.5187e-02, -1.1203e-01,  2.6171e-03],\n",
       "                        [-1.4505e-01, -2.3333e-01, -7.9461e-02]],\n",
       "              \n",
       "                       [[-8.0288e-02, -9.9464e-02, -7.6488e-02],\n",
       "                        [-7.1549e-02, -1.6537e-01, -1.3707e-01],\n",
       "                        [-7.4038e-02, -1.8226e-01, -1.5557e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.0025e-02, -3.4925e-02, -4.1067e-03],\n",
       "                        [ 6.1060e-02, -2.3774e-02,  5.7831e-03],\n",
       "                        [ 1.0447e-02, -3.6296e-02, -5.6996e-02]],\n",
       "              \n",
       "                       [[-5.4087e-02, -6.1758e-02, -7.7486e-02],\n",
       "                        [-1.3123e-01, -7.0396e-02, -1.5892e-01],\n",
       "                        [ 6.0707e-02,  5.0854e-02,  1.7449e-02]],\n",
       "              \n",
       "                       [[ 6.3034e-03, -1.1662e-01, -1.0970e-01],\n",
       "                        [ 1.1141e-02, -1.1890e-01, -4.3812e-02],\n",
       "                        [-9.6343e-02,  5.3895e-03, -2.7611e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.3115e-02,  4.9802e-02,  2.5395e-02],\n",
       "                        [-1.0413e-01, -6.8861e-02, -8.5125e-02],\n",
       "                        [ 8.5972e-02,  2.2125e-02, -6.6227e-02]],\n",
       "              \n",
       "                       [[-1.3475e-01, -7.7926e-02, -4.3808e-02],\n",
       "                        [-1.0636e-01, -1.7218e-01, -1.7948e-01],\n",
       "                        [ 2.7263e-05,  4.1266e-02,  1.5567e-03]],\n",
       "              \n",
       "                       [[ 5.4045e-02,  8.2080e-02,  4.9001e-02],\n",
       "                        [ 3.8261e-02,  4.8881e-02,  6.1113e-02],\n",
       "                        [-1.2305e-02, -4.0608e-02,  3.3018e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.3554e-02,  9.0679e-02, -5.5785e-02],\n",
       "                        [-1.0929e-02,  9.2945e-02, -2.4075e-02],\n",
       "                        [-1.9748e-02,  1.9609e-02, -2.2558e-02]],\n",
       "              \n",
       "                       [[-3.1773e-02, -4.9519e-02, -3.5621e-02],\n",
       "                        [ 1.4005e-02,  1.8999e-02,  5.4526e-02],\n",
       "                        [-3.9120e-02, -7.5056e-03,  6.6775e-03]],\n",
       "              \n",
       "                       [[-9.3685e-02,  5.9152e-02,  1.0313e-01],\n",
       "                        [-1.4960e-01, -1.9510e-02,  2.0944e-01],\n",
       "                        [-2.1063e-01, -8.7310e-02,  1.8700e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.8574e-02,  9.2517e-03,  1.8408e-02],\n",
       "                        [ 6.4905e-02, -2.0691e-02,  2.5479e-02],\n",
       "                        [-1.2619e-01, -1.2009e-01, -1.3951e-01]],\n",
       "              \n",
       "                       [[ 3.8722e-02,  8.0839e-02,  4.4349e-02],\n",
       "                        [ 5.8926e-02,  1.0239e-01,  1.5708e-01],\n",
       "                        [-6.2716e-02, -6.4815e-02,  1.6227e-02]],\n",
       "              \n",
       "                       [[-7.1990e-02, -1.5712e-02, -4.2307e-02],\n",
       "                        [-4.4866e-02, -7.8299e-03, -9.6548e-03],\n",
       "                        [-2.9060e-02, -4.8894e-02, -1.8387e-02]]]], device='mps:0')),\n",
       "             ('model.6.1.weight',\n",
       "              tensor([1.1526, 1.1482, 0.9900, 1.1343, 1.1138, 1.2443, 1.1020, 1.0665, 0.8731,\n",
       "                      0.8043, 0.7415, 0.7914, 0.5157, 1.1658, 1.1919, 1.0789, 1.1050, 1.0320,\n",
       "                      1.0102, 0.9659, 1.1503, 1.0771, 1.1378, 0.9645, 1.2206, 0.8819, 1.2276,\n",
       "                      1.0232, 0.9599, 1.0652, 1.0426, 0.8801, 0.9001, 1.0520, 1.2384, 1.0371,\n",
       "                      1.1459, 0.7239, 1.0081, 1.0407, 0.9410, 1.0131, 1.0289, 1.2335, 1.1129,\n",
       "                      1.1447, 0.8910, 0.9432, 1.1408, 1.4637, 1.1949, 0.9326, 1.2022, 0.9732,\n",
       "                      1.0157, 1.0846, 0.8173, 1.0330, 1.1555, 0.9685, 1.0584, 0.9141, 1.1383,\n",
       "                      0.8645, 1.0814, 1.0079, 1.1124, 1.1677, 0.9491, 1.0846, 1.1392, 1.0338,\n",
       "                      1.2344, 1.1792, 0.9903, 0.9972, 0.9688, 0.9192, 0.7430, 1.1353, 0.8268,\n",
       "                      0.9240, 1.1942, 0.9969, 0.8375, 1.0839, 1.2274, 0.9947, 1.0217, 1.0462,\n",
       "                      1.0816, 1.1179, 0.8991, 1.0624, 0.8386, 0.9563, 1.2655, 1.0359, 1.1616,\n",
       "                      0.9273, 1.1063, 1.2023, 1.1115, 0.8698, 0.9787, 1.3083, 0.9990, 1.0867,\n",
       "                      0.9825, 1.0406, 1.1095, 0.8490, 1.0567, 1.1508, 1.1538, 0.9430, 1.0294,\n",
       "                      0.9534, 1.1051, 0.9429, 1.1147, 1.2227, 1.2274, 1.2263, 1.0179, 1.2231,\n",
       "                      1.0968, 1.1759], device='mps:0')),\n",
       "             ('model.6.1.bias',\n",
       "              tensor([-0.9318, -0.8034, -0.7789, -0.7513, -0.5571, -0.6597, -0.5878, -0.4918,\n",
       "                      -0.6506, -0.6969, -0.6652, -0.7630, -0.5041, -0.5478, -0.9574, -0.9306,\n",
       "                      -0.6893, -0.6976, -0.7303, -0.4917, -0.6462, -0.8852, -0.5242, -0.6753,\n",
       "                      -0.7111, -0.5506, -0.6987, -0.6386, -0.6250, -0.7620, -0.8163, -0.5814,\n",
       "                      -0.7545, -0.8194, -0.8086, -0.9323, -0.7641, -0.8011, -0.6057, -0.7011,\n",
       "                      -0.5599, -0.3462, -0.8906, -0.9003, -0.9468, -0.9139, -0.6954, -0.8006,\n",
       "                      -0.6122, -0.5867, -0.7812, -0.7346, -0.8918, -0.9232, -1.0643, -0.7710,\n",
       "                      -0.7530, -0.3898, -1.1535, -0.7662, -0.9408, -0.5669, -0.7147, -0.7523,\n",
       "                      -0.7713, -0.6036, -0.9161, -0.8026, -0.6351, -0.5427, -0.6794, -0.8186,\n",
       "                      -0.6333, -0.6090, -0.8912, -0.7917, -0.7036, -1.0113, -0.6024, -0.7377,\n",
       "                      -0.4211, -0.9564, -0.6686, -0.4839, -0.8220, -0.8057, -0.6200, -0.6759,\n",
       "                      -0.7390, -0.6504, -0.6110, -0.7558, -0.8080, -0.8608, -0.6657, -0.5955,\n",
       "                      -0.7639, -0.7052, -0.8412, -0.8861, -0.7786, -0.8541, -0.7427, -0.9499,\n",
       "                      -0.7204, -0.4734, -0.8762, -0.6549, -0.3505, -0.6037, -0.7420, -0.5641,\n",
       "                      -0.7070, -0.4766, -0.5968, -0.8190, -0.6498, -0.8889, -0.7401, -0.7742,\n",
       "                      -0.5113, -1.0216, -0.6875, -0.8230, -0.7808, -0.5557, -0.7319, -0.4652],\n",
       "                     device='mps:0')),\n",
       "             ('model.6.1.running_mean',\n",
       "              tensor([-1.1957, -1.7162, -0.8805, -2.1526, -2.9462, -2.3159, -0.8520, -4.8048,\n",
       "                      -2.0099, -0.0535, -3.0584, -1.4745, -0.1293, -2.8202, -1.1799,  0.0596,\n",
       "                      -1.3392, -1.3800, -1.9197, -3.5598, -0.6140, -1.9829,  0.0196, -2.6455,\n",
       "                      -3.1544, -3.4206, -2.1984, -3.1482, -0.8784, -4.3819, -3.8112, -2.6930,\n",
       "                      -2.9225, -1.1028, -2.7071, -1.7872, -2.1325,  0.3216, -1.0077, -1.4445,\n",
       "                      -2.3025, -2.0434, -1.6410, -2.1695, -2.3755, -1.3966, -3.0577, -2.2409,\n",
       "                      -3.1820, -3.1014, -1.6404, -3.3476, -2.2201, -2.6425,  1.0515, -2.0435,\n",
       "                       0.0334, -1.6489, -2.9894, -1.1960, -1.9713, -2.0550, -2.0630, -0.3144,\n",
       "                      -1.0672, -2.1179, -2.4707, -2.9483, -1.2084, -2.1797, -2.5332, -1.1906,\n",
       "                      -3.4954, -1.5278, -1.6549, -2.7654, -1.4122, -0.8728, -1.4721, -3.4363,\n",
       "                      -1.8930, -1.9702, -2.5747, -2.6061, -2.1635, -2.2426, -1.8597, -2.9178,\n",
       "                      -0.4660, -2.7665, -2.6516, -0.7597, -0.1164, -1.0808,  0.3845, -2.0383,\n",
       "                      -2.1645, -1.3188, -1.8660, -0.5465, -2.2045, -0.8688, -1.1150, -0.5432,\n",
       "                      -0.4113, -1.6053, -1.5969, -2.0374, -1.9859,  1.2539, -1.9291, -0.4869,\n",
       "                      -1.6793, -4.4315, -1.9309, -1.2451, -3.1039, -2.3003, -1.8690, -2.2327,\n",
       "                      -2.8768, -1.5186, -1.6882, -2.8017, -2.7879, -2.2827, -1.1081, -3.1748],\n",
       "                     device='mps:0')),\n",
       "             ('model.6.1.running_var',\n",
       "              tensor([ 6.6900,  5.4153,  6.0425,  5.4622,  8.0007,  7.7510,  6.1009,  7.2614,\n",
       "                       6.6004,  4.3530,  4.5596,  6.4508,  4.9979,  6.1042,  6.9753,  5.7561,\n",
       "                       5.7045,  7.0969,  5.4256,  7.1702,  6.3720,  5.3390,  7.6964,  5.7568,\n",
       "                       6.3745,  5.8920,  6.8791,  6.0229,  4.6042,  7.5006,  4.8315,  4.6364,\n",
       "                       5.0875,  5.8688,  7.4879,  5.7091,  6.5328,  6.9680,  5.1269,  5.3722,\n",
       "                       4.9809,  5.9907,  6.4453,  7.4565,  7.1081,  6.4177,  5.9539,  5.9835,\n",
       "                       6.6002, 13.4296,  6.9831,  6.0318,  6.9048,  4.7802,  5.3641,  5.6029,\n",
       "                       5.6991,  7.9826,  7.0423,  4.4167,  5.2923,  5.5239,  4.8785,  4.5030,\n",
       "                       5.9117,  4.6106,  5.5053,  7.4638,  5.5717,  5.2481,  5.4679,  5.8507,\n",
       "                       9.4832,  8.5251,  6.4527,  5.7425,  5.9205,  4.2573,  3.8259,  5.9157,\n",
       "                       5.6662,  4.6673,  6.3395,  4.7957,  5.6240,  5.6875,  7.4769,  5.2389,\n",
       "                       5.7160,  4.2078,  5.1445,  4.5642,  4.6558,  5.3440,  3.0982,  4.9475,\n",
       "                       6.9748,  6.1372,  5.4026,  5.8553,  6.7545,  6.3827,  6.2735,  5.0943,\n",
       "                       6.2072,  8.3780,  5.7689,  5.5307,  6.3222,  5.5570,  4.8683,  4.8156,\n",
       "                       4.9667,  6.9944,  5.3638,  5.2753,  6.8807,  5.2042,  5.1856,  3.9278,\n",
       "                       6.9632,  6.9907,  8.6224,  7.0119,  6.3654,  8.0286,  5.5482,  7.5286],\n",
       "                     device='mps:0')),\n",
       "             ('model.6.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.7.0.weight',\n",
       "              tensor([[[[ 7.5910e-02,  5.4683e-02,  3.0322e-02],\n",
       "                        [-1.7837e-03,  8.4180e-03, -5.0583e-02],\n",
       "                        [-1.0774e-01, -1.1413e-01, -1.1907e-01]],\n",
       "              \n",
       "                       [[-9.7641e-02, -7.5787e-02, -8.8121e-02],\n",
       "                        [-8.6561e-02, -6.3873e-02, -4.0782e-02],\n",
       "                        [-2.1441e-02,  2.4564e-02,  3.9275e-02]],\n",
       "              \n",
       "                       [[-3.1951e-02, -6.4867e-02, -7.3506e-04],\n",
       "                        [-2.8872e-03, -5.3425e-02,  1.2965e-02],\n",
       "                        [-1.4857e-02, -3.1940e-02, -1.0621e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3396e-03, -2.2119e-02,  3.3186e-02],\n",
       "                        [-1.3899e-02,  2.1387e-02,  5.2410e-02],\n",
       "                        [-4.3119e-02, -4.9171e-02, -3.2710e-02]],\n",
       "              \n",
       "                       [[-3.6841e-02, -2.6411e-02, -5.1245e-02],\n",
       "                        [ 2.5460e-02,  7.3194e-02,  5.7084e-02],\n",
       "                        [-1.8237e-02, -8.0592e-03, -1.5945e-02]],\n",
       "              \n",
       "                       [[ 6.8811e-02,  4.7476e-02,  5.9196e-02],\n",
       "                        [-1.5873e-02, -4.8520e-02, -6.2704e-02],\n",
       "                        [-7.4144e-02, -6.6195e-02, -8.0565e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.3267e-03,  2.6044e-02,  1.8906e-02],\n",
       "                        [ 2.1960e-02,  3.1476e-02,  2.7443e-02],\n",
       "                        [-6.0798e-02, -6.3141e-02, -4.1343e-02]],\n",
       "              \n",
       "                       [[-1.6539e-02,  2.1315e-02, -2.2696e-03],\n",
       "                        [ 1.2876e-02, -4.2154e-02, -3.8450e-02],\n",
       "                        [-4.3300e-02,  1.2805e-02,  7.3587e-03]],\n",
       "              \n",
       "                       [[-6.3485e-02, -5.3899e-02, -3.0412e-02],\n",
       "                        [ 1.5611e-02, -1.7121e-02,  8.8125e-04],\n",
       "                        [-2.2552e-02, -3.4309e-02, -2.8597e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.6796e-02,  3.2238e-02,  4.4980e-02],\n",
       "                        [ 1.9905e-02, -2.8269e-02,  8.0557e-03],\n",
       "                        [-8.8761e-02, -7.1015e-02, -1.0170e-01]],\n",
       "              \n",
       "                       [[-1.1105e-02, -5.7393e-02, -6.4641e-03],\n",
       "                        [-4.0826e-02, -6.1926e-02, -3.6526e-02],\n",
       "                        [-5.1630e-02, -8.0944e-02, -4.5080e-02]],\n",
       "              \n",
       "                       [[-6.4609e-02, -8.5225e-03, -5.9021e-02],\n",
       "                        [ 4.4836e-02,  7.5375e-02,  4.8544e-02],\n",
       "                        [ 4.1624e-02,  9.3464e-02,  6.7039e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.9389e-02,  4.8539e-02,  8.5840e-02],\n",
       "                        [ 2.9569e-02,  4.1249e-02,  1.7625e-02],\n",
       "                        [-4.5169e-02, -1.0071e-01, -6.0172e-02]],\n",
       "              \n",
       "                       [[-2.9260e-02, -5.4970e-02, -4.9870e-02],\n",
       "                        [-4.9780e-02, -6.6647e-03,  6.6983e-03],\n",
       "                        [-5.5037e-02, -6.4092e-03, -3.4609e-02]],\n",
       "              \n",
       "                       [[ 8.7412e-02,  5.5105e-02,  5.9010e-02],\n",
       "                        [-1.0163e-02, -1.3180e-02, -2.2225e-02],\n",
       "                        [-2.5894e-02, -6.8413e-02, -5.6265e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.0336e-02, -5.8663e-03,  2.5627e-02],\n",
       "                        [ 3.6439e-02,  1.4832e-02,  2.3641e-02],\n",
       "                        [ 7.8313e-03, -1.9043e-02, -1.9285e-02]],\n",
       "              \n",
       "                       [[ 1.5435e-01,  1.0475e-01,  1.3456e-01],\n",
       "                        [ 1.0886e-01,  4.9376e-02,  4.2451e-02],\n",
       "                        [-9.2743e-03, -7.0502e-02, -7.6000e-02]],\n",
       "              \n",
       "                       [[-7.6290e-03,  6.3104e-02, -3.5033e-03],\n",
       "                        [-3.8742e-02,  3.5503e-02, -4.9684e-02],\n",
       "                        [-1.0177e-01, -6.4619e-02, -1.5883e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 4.3049e-02,  6.1279e-02,  5.0074e-02],\n",
       "                        [-3.8527e-02, -2.3193e-02, -2.8086e-02],\n",
       "                        [-5.4679e-02, -2.0224e-02, -1.4098e-02]],\n",
       "              \n",
       "                       [[-3.7530e-02, -6.9741e-02, -2.3459e-02],\n",
       "                        [-1.9276e-02, -5.6814e-02, -1.2904e-02],\n",
       "                        [ 5.2592e-02,  3.3004e-02,  3.5216e-02]],\n",
       "              \n",
       "                       [[-5.8410e-02, -2.2886e-02, -3.2053e-02],\n",
       "                        [-7.3035e-03, -3.8817e-02, -8.1954e-03],\n",
       "                        [ 1.6932e-03,  1.3742e-02, -7.6392e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.0695e-03, -1.4148e-02, -1.2213e-04],\n",
       "                        [-5.4539e-02, -6.0491e-02, -2.1562e-02],\n",
       "                        [-5.4261e-02, -6.0061e-02, -6.2087e-02]],\n",
       "              \n",
       "                       [[-7.2051e-02, -3.3394e-02, -2.6836e-02],\n",
       "                        [-5.4487e-02, -1.2019e-02, -1.0087e-02],\n",
       "                        [ 7.0735e-04,  7.0171e-03,  3.7843e-02]],\n",
       "              \n",
       "                       [[ 1.0297e-01,  9.1523e-02,  6.1391e-02],\n",
       "                        [-3.2094e-02,  2.9106e-03, -8.3848e-03],\n",
       "                        [-5.2674e-02, -8.6778e-02, -6.8391e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.9346e-02,  6.1875e-02,  4.6642e-02],\n",
       "                        [-1.7912e-03,  1.3161e-03, -4.8518e-03],\n",
       "                        [-4.5249e-02,  2.4303e-03,  1.2281e-02]],\n",
       "              \n",
       "                       [[-2.4536e-02, -4.9064e-02, -3.0009e-02],\n",
       "                        [ 4.7805e-02,  3.2606e-02, -7.6464e-03],\n",
       "                        [ 4.1986e-02,  3.3052e-02,  2.1605e-02]],\n",
       "              \n",
       "                       [[-6.8668e-02, -8.0491e-02, -8.2970e-02],\n",
       "                        [-3.8623e-02, -6.5300e-02, -7.1152e-02],\n",
       "                        [-3.0703e-02, -8.3503e-04, -3.8969e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1149e-03,  3.9709e-02,  2.9492e-02],\n",
       "                        [-6.8064e-02, -7.1409e-02, -2.5037e-02],\n",
       "                        [-7.0903e-02, -8.9650e-02, -7.3290e-02]],\n",
       "              \n",
       "                       [[ 4.8375e-02, -1.3903e-02,  1.7467e-02],\n",
       "                        [ 3.8539e-02,  2.4640e-02,  3.2465e-02],\n",
       "                        [-3.8135e-02, -6.6060e-02, -3.0767e-02]],\n",
       "              \n",
       "                       [[-1.6285e-02,  2.4335e-02, -2.4263e-02],\n",
       "                        [-2.4966e-02,  2.6745e-02, -2.3130e-02],\n",
       "                        [-8.0837e-02, -7.0704e-02, -7.7369e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3759e-02, -4.1179e-02, -4.3488e-02],\n",
       "                        [-3.0179e-02, -7.1476e-03, -2.5191e-02],\n",
       "                        [ 1.7340e-02, -3.1599e-02, -6.2961e-04]],\n",
       "              \n",
       "                       [[ 2.7178e-02,  2.3510e-02,  3.8324e-02],\n",
       "                        [ 2.5877e-02,  5.8817e-02,  9.2815e-02],\n",
       "                        [ 7.8795e-02,  1.0072e-01,  1.3112e-01]],\n",
       "              \n",
       "                       [[-2.8865e-02, -3.7765e-02,  2.8971e-02],\n",
       "                        [ 1.3437e-02,  3.8527e-02,  4.4278e-02],\n",
       "                        [-1.6616e-02, -4.3777e-03,  3.8411e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4029e-03,  7.6663e-03,  1.8545e-02],\n",
       "                        [ 6.8405e-02,  1.0965e-01,  7.6022e-02],\n",
       "                        [ 7.9486e-02,  1.2414e-01,  7.4817e-02]],\n",
       "              \n",
       "                       [[-3.5782e-03,  1.2330e-02, -2.8389e-03],\n",
       "                        [ 3.3706e-03,  1.5497e-02,  1.7250e-02],\n",
       "                        [-2.3481e-02, -2.1112e-02, -8.3011e-03]],\n",
       "              \n",
       "                       [[ 6.6304e-02,  1.8096e-02,  6.3936e-02],\n",
       "                        [ 6.8328e-02,  5.8837e-02,  7.6811e-02],\n",
       "                        [-1.9997e-02, -1.7378e-02, -1.3286e-02]]]], device='mps:0')),\n",
       "             ('model.7.1.weight',\n",
       "              tensor([0.9675, 1.1227, 0.9505, 0.9792, 0.8670, 1.0075, 1.0217, 0.9969, 0.9976,\n",
       "                      0.8107, 0.9353, 0.9790, 1.0015, 0.9258, 1.0368, 0.8956, 0.9118, 0.9010,\n",
       "                      1.0990, 0.9334, 0.8525, 0.8062, 0.8805, 0.9170, 1.0043, 0.8684, 1.0201,\n",
       "                      1.0005, 1.0232, 0.9896, 0.8027, 0.8861, 0.9150, 0.9570, 0.8736, 0.7612,\n",
       "                      0.5939, 0.6253, 0.9191, 1.0288, 0.7436, 0.9745, 0.9506, 1.0051, 0.8694,\n",
       "                      0.9926, 0.8788, 0.9170, 0.8358, 0.9738, 0.8432, 1.0165, 0.9400, 0.7918,\n",
       "                      0.9296, 0.7039, 0.9315, 0.9786, 0.5991, 0.6464, 0.7739, 0.8393, 0.8832,\n",
       "                      0.9610, 0.8336, 1.0620, 0.9754, 1.1158, 0.9440, 0.8312, 0.8923, 0.7150,\n",
       "                      1.0553, 0.7510, 1.0099, 1.0093, 0.7670, 1.1920, 0.7898, 1.0881, 0.8562,\n",
       "                      0.9329, 1.0688, 0.9703, 0.6822, 0.8515, 0.9583, 0.6701, 0.7661, 0.7491,\n",
       "                      0.6676, 0.7705, 0.9715, 0.8901, 0.9341, 0.8181, 0.9634, 0.9325, 0.8489,\n",
       "                      0.9321, 0.9530, 0.9211, 1.0159, 0.6856, 0.9134, 0.6189, 1.1494, 0.9075,\n",
       "                      0.9497, 0.8040, 0.9941, 1.0023, 0.6952, 0.6142, 0.7752, 0.8037, 1.0146,\n",
       "                      0.6795, 0.8229, 0.8837, 0.7624, 1.0425, 1.1014, 0.9375, 1.0194, 1.0619,\n",
       "                      0.8820, 1.0318, 0.9902, 0.8797, 1.0018, 0.8912, 0.7288, 0.7511, 1.0302,\n",
       "                      1.0637, 0.8714, 0.9398, 0.7332, 0.7498, 0.7444, 0.9964, 1.0121, 1.0214,\n",
       "                      1.0661, 1.1010, 0.9395, 0.8545, 0.6963, 0.9288, 0.8703, 0.7366, 0.8033,\n",
       "                      1.0432, 0.9577, 1.0301, 1.0357, 1.1212, 0.9146, 0.8835, 0.7933, 0.7355,\n",
       "                      0.6121, 0.8581, 0.9033, 0.6410, 0.9294, 0.7579, 0.9763, 0.6196, 0.8305,\n",
       "                      0.9989, 0.7367, 0.7182, 0.8106, 1.0546, 0.9253, 0.8100, 0.9875, 0.7990,\n",
       "                      0.9309, 0.7698, 0.9340, 1.1791, 0.9321, 1.0301, 0.8304, 0.8540, 1.0163,\n",
       "                      0.8360, 0.7153, 1.0644, 0.7483, 0.8535, 1.0328, 0.8178, 0.6414, 0.8936,\n",
       "                      0.9257, 1.0292, 1.1175, 0.9245, 0.9438, 1.1169, 1.0007, 1.0611, 0.8198,\n",
       "                      0.6578, 1.1177, 0.9248, 0.8831, 0.9267, 0.8492, 0.9321, 0.9184, 0.6631,\n",
       "                      1.0676, 0.8431, 0.9155, 0.9293, 1.0957, 0.8003, 0.9349, 0.9939, 0.8517,\n",
       "                      0.7088, 1.0490, 0.8172, 0.8282, 0.9732, 0.9923, 1.0389, 0.9256, 1.0332,\n",
       "                      1.0361, 0.9681, 0.9311, 1.0209, 0.8738, 0.9698, 0.9123, 0.9389, 0.8953,\n",
       "                      0.9298, 1.1734, 0.9290, 0.9590, 0.6831, 0.8074, 0.5237, 0.8675, 0.8996,\n",
       "                      1.0215, 1.1326, 0.9885, 0.9673], device='mps:0')),\n",
       "             ('model.7.1.bias',\n",
       "              tensor([-0.3631, -0.2535, -0.4933, -0.2911, -0.5154, -0.3594, -0.3964, -0.5251,\n",
       "                      -0.5125, -0.4565, -0.4301, -0.4195, -0.3274, -0.4769, -0.4231, -0.2692,\n",
       "                      -0.6417, -0.5386, -0.7778, -0.4097, -0.5882, -0.3741, -0.4079, -0.4366,\n",
       "                      -0.3672, -0.6616, -0.4605, -0.5837, -0.4053, -0.5818, -0.6013, -0.4677,\n",
       "                      -0.6780, -0.4626, -0.3418, -0.4844, -0.3988, -0.2979, -0.7535, -0.4268,\n",
       "                      -0.4917, -0.3264, -0.4143, -0.4697, -0.5850, -0.3086, -0.3711, -0.6802,\n",
       "                      -0.4121, -0.6085, -0.2482, -0.4463, -0.8062, -0.4348, -0.5698, -0.5209,\n",
       "                      -0.3616, -0.4922, -0.2932, -0.3203, -0.6081, -0.7090, -0.5163, -0.5407,\n",
       "                      -0.5896, -0.5831, -0.6676, -0.5157, -0.7168, -0.3995, -0.6961, -0.4851,\n",
       "                      -0.4681, -0.3496, -0.2774, -0.2923, -0.4847, -0.2635, -0.4703, -0.4654,\n",
       "                      -0.5551, -0.5421, -0.5531, -0.2778, -0.4615, -0.4270, -0.5681, -0.4005,\n",
       "                      -0.5452, -0.5952, -0.5463, -0.3357, -0.6981, -0.6524, -0.4547, -0.5307,\n",
       "                      -0.3002, -0.8021, -0.4798, -0.6531, -0.3885, -0.4805, -0.3084, -0.3684,\n",
       "                      -0.6540, -0.4146, -0.4140, -0.4019, -0.7932, -0.5788, -0.3240, -0.7795,\n",
       "                      -0.5478, -0.3913, -0.6161, -0.4759, -0.1447, -0.5430, -0.5702, -0.4835,\n",
       "                      -0.5187, -0.4749, -0.2456, -0.3872, -0.6612, -0.4473, -0.5705, -0.5146,\n",
       "                      -0.6018, -0.4247, -0.6101, -0.6984, -0.5798, -0.4584, -0.4559, -0.5827,\n",
       "                      -0.4955, -0.5527, -0.6057, -0.5652, -0.4539, -0.6581, -0.4583, -0.6554,\n",
       "                      -0.3348, -0.3056, -0.6021, -0.5355, -0.4436, -0.4183, -0.4224, -0.4843,\n",
       "                      -0.4990, -0.4588, -0.5205, -0.3869, -0.8551, -0.5169, -0.5031, -0.4082,\n",
       "                      -0.5333, -0.4860, -0.5533, -0.4498, -0.5693, -0.4342, -0.4153, -0.3920,\n",
       "                      -0.4110, -0.3813, -0.6381, -0.3633, -0.4350, -0.5351, -0.6140, -0.5995,\n",
       "                      -0.6324, -0.5977, -0.6879, -0.4875, -0.6665, -0.4264, -0.6529, -0.7651,\n",
       "                      -0.5679, -0.4144, -0.6439, -0.5457, -0.3782, -0.4351, -0.5073, -0.5471,\n",
       "                      -0.3623, -0.3947, -0.2191, -0.6344, -0.4285, -0.5190, -0.6481, -0.4989,\n",
       "                      -0.7070, -0.5802, -0.4934, -0.2108, -0.6921, -0.5497, -0.3981, -0.4961,\n",
       "                      -0.2869, -0.4953, -0.3909, -0.7479, -0.7506, -0.5584, -0.7428, -0.5841,\n",
       "                      -0.1845, -0.6970, -0.5349, -0.5148, -0.5365, -0.6355, -0.5170, -0.5247,\n",
       "                      -0.5758, -0.4382, -0.7726, -0.6694, -0.5079, -0.5408, -0.6548, -0.6612,\n",
       "                      -0.3504, -0.2096, -0.3456, -0.2435, -0.5484, -0.5911, -0.5124, -0.4016,\n",
       "                      -0.5403, -0.2215, -0.4316, -0.4712, -0.5997, -0.5339, -0.5466, -0.5285,\n",
       "                      -0.4417, -0.3767, -0.4852, -0.3781, -0.5914, -0.3974, -0.3611, -0.4404],\n",
       "                     device='mps:0')),\n",
       "             ('model.7.1.running_mean',\n",
       "              tensor([-0.6555, -1.6841, -0.8074, -1.8573, -1.3348, -1.2559, -1.0963, -0.7839,\n",
       "                      -0.7733, -0.3453, -1.9863, -1.3936, -2.0823, -0.3456, -1.2113, -1.0551,\n",
       "                      -1.2143, -1.3734, -0.4885, -1.4554, -1.1317, -0.8874, -2.0634, -0.9569,\n",
       "                      -2.2711, -1.9078, -0.8468, -0.6898, -2.6416, -0.7960, -0.6104, -1.2734,\n",
       "                      -0.6714, -1.8492, -1.5798, -1.1345, -0.3184, -0.9569, -0.3854, -0.5022,\n",
       "                      -0.4822, -1.2595, -0.3874, -1.8064, -0.9220, -1.4273, -1.9071, -1.5201,\n",
       "                      -1.4767, -1.8050, -1.8924, -0.5506, -1.1172, -1.4393,  0.0544, -0.9915,\n",
       "                      -1.0812, -1.6903, -1.3242, -1.1250, -0.9423, -0.7232, -0.8183, -0.4369,\n",
       "                      -0.4872, -1.5214, -0.8619, -1.0103, -0.5936, -1.5708, -1.5443, -1.3813,\n",
       "                      -1.2713, -1.5322, -1.2358, -2.0794, -1.1028, -0.6411, -0.6882, -0.7965,\n",
       "                      -0.8932, -1.0523, -1.8078, -1.5494, -0.2682, -1.0592, -2.3417, -1.0728,\n",
       "                      -1.2573, -0.6237, -0.8770, -1.2095, -1.5869, -0.5884, -1.2690, -0.8940,\n",
       "                      -1.0300, -1.4574,  0.9158, -1.8267, -0.4450, -1.2022, -2.2810, -0.1987,\n",
       "                      -0.5187, -0.5227, -0.3335, -1.5320, -0.4255,  0.0416, -0.1532, -1.2937,\n",
       "                      -0.9074, -0.7648, -1.1885, -0.4124, -1.3946, -1.1673, -0.5521, -1.0891,\n",
       "                      -0.6600, -1.2950, -1.2913, -2.2490, -0.4895, -1.1671, -1.6298, -0.8405,\n",
       "                      -0.5882, -2.2902, -0.7467, -1.2419, -0.7930, -1.0105, -1.5396, -1.3559,\n",
       "                       0.2114, -0.8757, -0.5745, -0.8246, -0.7537, -1.7519, -0.8622, -1.8593,\n",
       "                      -0.6234, -1.4580, -1.2905, -1.5350, -1.2270, -1.4816, -1.1138, -1.2412,\n",
       "                      -1.4718, -1.5491, -1.7276, -1.2614, -0.8606,  0.2111, -0.8306, -0.2578,\n",
       "                      -1.1774, -0.1454,  0.0453, -0.8885, -1.2955, -1.4122, -1.5175, -1.3313,\n",
       "                      -0.4546, -0.7996, -1.3150, -1.2654, -1.2870, -0.6461, -1.0462, -0.8878,\n",
       "                      -0.8947, -0.8530, -1.0887, -0.6492, -0.3391, -0.4926, -1.0511, -0.8236,\n",
       "                      -0.6541, -1.5487, -1.4966, -0.1163,  0.2459, -1.2538, -1.1806, -0.6739,\n",
       "                      -2.0794, -1.0592, -1.8548, -0.7669, -1.0098, -0.9189, -1.9149, -0.5218,\n",
       "                      -1.2524,  0.3932, -0.0250, -1.2860, -1.1513, -1.0084, -0.7118, -0.8175,\n",
       "                      -1.7773, -0.7762, -1.2448, -1.2721, -0.7212, -1.0295,  0.2190, -0.5115,\n",
       "                      -1.6553, -0.2365, -1.6499, -1.6393, -1.5411, -0.4533, -1.7174, -1.1422,\n",
       "                      -1.2922, -0.0932, -0.8124, -0.8142, -0.1430, -1.6784, -1.5711, -3.0365,\n",
       "                      -2.3976, -2.0246, -0.4032, -0.7458, -1.4214, -0.6815, -0.7927, -0.8775,\n",
       "                      -1.5740, -1.6397, -1.3682, -0.4840, -1.1867, -1.9936, -1.9949, -1.4031,\n",
       "                      -1.0557, -0.6072, -1.0122, -1.2556, -1.7868, -1.3698, -0.8597,  0.1704],\n",
       "                     device='mps:0')),\n",
       "             ('model.7.1.running_var',\n",
       "              tensor([0.9829, 1.4027, 1.0510, 1.7831, 1.1523, 1.5572, 1.0334, 1.2622, 1.6725,\n",
       "                      0.8937, 1.4009, 1.2957, 1.1115, 1.3498, 1.1917, 1.1282, 1.3961, 1.3674,\n",
       "                      1.9431, 1.0785, 1.2682, 1.1357, 1.3984, 1.6072, 1.3146, 1.5860, 1.3781,\n",
       "                      1.1044, 2.1857, 1.2687, 1.4049, 1.2430, 1.1573, 1.2344, 1.0413, 1.0435,\n",
       "                      1.2424, 1.3175, 1.3974, 1.1450, 1.2048, 1.1545, 1.1807, 1.5916, 1.2088,\n",
       "                      1.3017, 1.2250, 1.4039, 1.0721, 1.2330, 1.6546, 1.1391, 1.5601, 1.0400,\n",
       "                      1.4962, 0.9934, 0.8599, 1.1104, 1.1557, 1.1937, 0.8430, 1.4038, 1.0378,\n",
       "                      1.2170, 1.3056, 1.6450, 1.2347, 1.4771, 1.4288, 1.2105, 1.7176, 1.2423,\n",
       "                      1.7317, 1.1736, 1.0647, 1.4809, 1.0097, 1.3838, 1.6264, 1.2863, 1.1015,\n",
       "                      0.9013, 1.8438, 1.2961, 1.0801, 1.3217, 1.4647, 1.3246, 1.2514, 0.9844,\n",
       "                      1.1939, 1.2877, 1.2311, 1.5338, 1.1197, 1.4086, 1.1582, 1.6622, 1.0676,\n",
       "                      1.3275, 0.9435, 1.0982, 1.4205, 0.8226, 1.5027, 1.7053, 2.0806, 1.2883,\n",
       "                      1.4541, 0.9824, 0.9909, 1.6238, 1.8280, 1.6758, 1.5779, 1.0293, 1.2719,\n",
       "                      1.0734, 1.1838, 0.8859, 1.6214, 1.4890, 1.0071, 1.4748, 1.8360, 1.1294,\n",
       "                      1.6289, 1.4353, 1.3130, 1.7049, 1.5616, 1.2752, 1.2518, 0.8671, 1.3156,\n",
       "                      1.5499, 1.0464, 1.4211, 1.7135, 1.9433, 0.9475, 1.4147, 1.0183, 1.5593,\n",
       "                      1.3289, 1.3351, 1.1505, 1.6158, 1.4333, 1.3074, 1.3580, 1.4599, 1.2217,\n",
       "                      1.3432, 1.5867, 1.3102, 1.4224, 1.4816, 1.7894, 1.0682, 1.6374, 0.8694,\n",
       "                      1.5599, 0.8973, 1.6104, 1.1903, 0.9622, 0.9628, 1.0339, 1.2180, 1.4436,\n",
       "                      1.5732, 1.3914, 1.2750, 1.2889, 1.5073, 1.5404, 1.2277, 1.8020, 1.1398,\n",
       "                      1.4426, 0.9158, 1.8035, 2.1949, 1.1733, 1.3180, 1.5100, 1.2248, 1.2223,\n",
       "                      1.0844, 0.8742, 1.5064, 1.5284, 1.8062, 1.6403, 1.2048, 1.2073, 1.3405,\n",
       "                      1.2085, 1.1214, 1.9673, 1.7692, 1.0907, 1.2806, 1.1829, 1.4972, 0.9866,\n",
       "                      1.2293, 1.7453, 1.0302, 1.0799, 1.2489, 1.2640, 1.3757, 1.3127, 1.2886,\n",
       "                      1.6026, 1.7580, 1.1661, 1.2047, 1.4451, 1.2550, 1.4218, 1.6187, 1.5248,\n",
       "                      1.1563, 1.9787, 1.4328, 0.9431, 1.2193, 1.4249, 2.3322, 1.3269, 1.5257,\n",
       "                      1.3408, 0.8798, 1.1422, 1.1257, 1.1731, 1.2861, 1.4049, 0.8277, 1.3222,\n",
       "                      1.3853, 2.1064, 1.3436, 1.5919, 0.9468, 1.4868, 1.2504, 0.9047, 1.0434,\n",
       "                      1.3671, 1.2401, 1.1654, 1.9629], device='mps:0')),\n",
       "             ('model.7.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.8.0.weight',\n",
       "              tensor([[[[-3.0168e-02, -7.6708e-03, -1.5573e-02],\n",
       "                        [-9.8868e-03, -3.2023e-02, -1.9806e-02],\n",
       "                        [ 5.8903e-03, -1.3656e-03, -1.0718e-02]],\n",
       "              \n",
       "                       [[ 3.0271e-02,  6.4691e-03,  1.2573e-02],\n",
       "                        [-2.1954e-02, -2.4908e-02, -2.5616e-03],\n",
       "                        [-4.2953e-02, -3.9630e-02, -2.5609e-02]],\n",
       "              \n",
       "                       [[-2.7080e-02,  1.0444e-02,  1.6147e-02],\n",
       "                        [-4.1309e-02, -2.4608e-02, -3.2125e-02],\n",
       "                        [-4.9239e-02, -4.2255e-02, -5.4569e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1926e-02, -2.0021e-02, -4.7760e-02],\n",
       "                        [-4.8920e-03, -2.8714e-02, -2.6230e-02],\n",
       "                        [-2.8033e-02,  7.1174e-04, -2.9601e-02]],\n",
       "              \n",
       "                       [[-3.2270e-02, -1.6481e-02, -1.1581e-02],\n",
       "                        [ 1.0719e-02, -9.6610e-03, -3.9159e-02],\n",
       "                        [ 2.8188e-02, -4.7967e-03, -7.2376e-03]],\n",
       "              \n",
       "                       [[ 6.9307e-03,  1.4343e-02,  1.5391e-03],\n",
       "                        [ 1.8817e-02,  7.5693e-03,  2.3330e-02],\n",
       "                        [ 4.7858e-03,  1.6419e-02,  7.0070e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.9061e-03, -1.2735e-02, -1.6869e-03],\n",
       "                        [ 5.6892e-03, -2.4985e-02, -6.2826e-03],\n",
       "                        [ 6.2254e-03, -3.5305e-02, -1.7403e-02]],\n",
       "              \n",
       "                       [[ 4.8222e-03,  3.6315e-04,  4.4588e-03],\n",
       "                        [-3.1164e-02, -4.4497e-02, -1.5847e-02],\n",
       "                        [-2.0965e-02, -3.9476e-03, -1.7445e-02]],\n",
       "              \n",
       "                       [[ 5.3467e-03,  2.4315e-02,  7.4126e-03],\n",
       "                        [-8.2150e-03, -1.5851e-02, -6.2708e-03],\n",
       "                        [-3.6171e-02, -1.2939e-03, -1.4976e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.3617e-03,  7.1514e-04,  1.7155e-02],\n",
       "                        [-1.0853e-02, -1.2416e-02, -1.8132e-02],\n",
       "                        [-1.4768e-02, -1.0473e-02, -2.8021e-02]],\n",
       "              \n",
       "                       [[-3.8187e-03,  2.2462e-02,  2.8636e-02],\n",
       "                        [-6.9663e-03,  3.5659e-03,  3.9363e-02],\n",
       "                        [ 1.3404e-02, -1.6896e-02,  1.1430e-02]],\n",
       "              \n",
       "                       [[-1.0430e-03,  9.1268e-03,  1.0148e-02],\n",
       "                        [-2.6466e-02, -2.3425e-02, -1.0852e-02],\n",
       "                        [-5.3916e-02, -3.9715e-02, -3.4506e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7843e-02, -5.0821e-02, -4.5108e-02],\n",
       "                        [-6.2374e-02, -5.5430e-02, -4.0633e-02],\n",
       "                        [-2.5661e-02, -4.3405e-02, -1.7859e-02]],\n",
       "              \n",
       "                       [[ 1.0539e-02, -3.7343e-02, -2.6692e-02],\n",
       "                        [-1.8067e-02, -7.1004e-02, -3.6903e-02],\n",
       "                        [-4.1934e-02, -6.0858e-02, -4.8291e-02]],\n",
       "              \n",
       "                       [[ 1.8932e-03, -7.6767e-03, -2.2510e-03],\n",
       "                        [-4.7701e-02, -3.8065e-02, -6.0919e-03],\n",
       "                        [-3.3916e-03,  6.6357e-04, -3.6400e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.7771e-02, -6.7366e-02, -4.5924e-02],\n",
       "                        [-6.4883e-02, -7.1817e-02, -8.3660e-02],\n",
       "                        [-6.4472e-02, -7.8665e-02, -4.9041e-02]],\n",
       "              \n",
       "                       [[ 2.2448e-02, -7.9445e-03, -2.5902e-02],\n",
       "                        [-3.0015e-02, -5.3157e-02, -4.2718e-02],\n",
       "                        [-8.5998e-03, -2.3035e-02, -1.2705e-02]],\n",
       "              \n",
       "                       [[-1.9064e-02, -3.4704e-02, -1.7819e-02],\n",
       "                        [-3.0182e-02, -4.5836e-02, -5.9080e-02],\n",
       "                        [-3.8873e-02, -3.4770e-02, -1.6886e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 5.8702e-03,  6.8515e-03, -2.0388e-03],\n",
       "                        [-1.0833e-02, -1.8401e-02, -2.4071e-02],\n",
       "                        [-4.0374e-03, -8.0181e-03, -2.3214e-02]],\n",
       "              \n",
       "                       [[-2.1485e-02, -1.1634e-02, -6.1990e-03],\n",
       "                        [-1.2291e-02, -6.3019e-02, -1.5111e-02],\n",
       "                        [-1.7218e-02, -3.4021e-02,  4.8477e-03]],\n",
       "              \n",
       "                       [[-8.4766e-03, -3.5826e-02, -3.5586e-03],\n",
       "                        [-5.2973e-02, -3.0279e-02, -9.8959e-03],\n",
       "                        [-2.1247e-02, -1.1753e-02, -9.0205e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2522e-02,  3.6628e-03,  3.7974e-02],\n",
       "                        [ 2.3555e-02, -2.8655e-02,  1.1324e-02],\n",
       "                        [ 3.2601e-02,  2.9777e-04,  3.1403e-03]],\n",
       "              \n",
       "                       [[ 1.8980e-02,  3.3130e-02,  2.6358e-02],\n",
       "                        [ 1.5671e-02, -1.7249e-02,  2.9767e-02],\n",
       "                        [-1.8166e-02, -1.2555e-02, -1.0856e-02]],\n",
       "              \n",
       "                       [[-7.3598e-03,  1.9659e-02,  1.4315e-02],\n",
       "                        [ 2.0768e-03, -3.3350e-02, -9.2786e-03],\n",
       "                        [-5.8694e-02, -3.0899e-02, -4.6824e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4034e-02, -3.7698e-02, -1.3985e-03],\n",
       "                        [-2.5831e-02, -2.5119e-02, -1.2089e-02],\n",
       "                        [-2.5191e-02,  4.1643e-03, -2.2906e-02]],\n",
       "              \n",
       "                       [[ 3.6644e-02,  3.1997e-04,  1.7653e-02],\n",
       "                        [ 2.1408e-02, -1.2411e-02, -2.5772e-02],\n",
       "                        [ 2.5596e-02, -2.2725e-02,  1.8071e-02]],\n",
       "              \n",
       "                       [[-2.2227e-02,  4.5408e-03, -1.2287e-02],\n",
       "                        [-1.3711e-02, -3.8088e-02, -2.4859e-02],\n",
       "                        [-2.9436e-02, -2.8553e-02, -3.6893e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.7718e-02, -2.7182e-02, -3.0649e-02],\n",
       "                        [-5.9171e-02, -5.2970e-02, -4.7795e-02],\n",
       "                        [-1.3224e-02, -2.2334e-02, -4.1423e-02]],\n",
       "              \n",
       "                       [[-1.6268e-02, -1.4023e-02, -1.3823e-02],\n",
       "                        [-2.4447e-02, -4.9765e-03, -4.2601e-02],\n",
       "                        [-2.1630e-02, -1.7375e-02, -4.9212e-03]],\n",
       "              \n",
       "                       [[-5.0970e-02, -4.8046e-02, -5.9222e-02],\n",
       "                        [-8.3811e-02, -7.6556e-02, -7.2725e-02],\n",
       "                        [-1.5504e-02, -3.9333e-02, -3.6030e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8928e-02, -4.8646e-03,  7.5118e-03],\n",
       "                        [-4.7823e-02, -4.2544e-02, -3.0703e-02],\n",
       "                        [-7.9526e-04, -1.1317e-02, -1.5849e-02]],\n",
       "              \n",
       "                       [[-4.1966e-03, -1.8554e-02, -1.9337e-02],\n",
       "                        [-4.7693e-02, -3.2356e-02, -2.8931e-02],\n",
       "                        [-1.7698e-02, -1.1199e-02, -1.9506e-05]],\n",
       "              \n",
       "                       [[-5.5008e-02, -5.6039e-02, -5.7755e-03],\n",
       "                        [-5.0614e-02, -5.8016e-02, -2.5962e-02],\n",
       "                        [ 4.0553e-06, -4.3726e-02, -7.4586e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.9035e-02,  3.1074e-02,  1.5538e-02],\n",
       "                        [-1.2119e-02,  1.9834e-02,  1.1956e-02],\n",
       "                        [ 7.8825e-03, -1.5554e-02,  2.5736e-02]],\n",
       "              \n",
       "                       [[ 7.0239e-03, -1.1370e-02, -2.9957e-02],\n",
       "                        [ 6.5035e-03,  5.1479e-03, -4.3408e-03],\n",
       "                        [ 8.9171e-03,  1.4371e-03, -1.5489e-02]],\n",
       "              \n",
       "                       [[ 2.2683e-02,  1.8384e-02,  1.2628e-02],\n",
       "                        [ 4.6655e-03, -2.0947e-03,  8.1952e-03],\n",
       "                        [-4.0643e-03, -1.3595e-02, -1.0407e-02]]]], device='mps:0')),\n",
       "             ('model.8.1.weight',\n",
       "              tensor([1.0308, 0.8874, 1.1690, 1.0939, 0.9597, 1.0182, 1.0422, 1.0519, 1.0254,\n",
       "                      0.9102, 0.8797, 1.0252, 0.9971, 0.9420, 1.0300, 0.8791, 1.0207, 0.8250,\n",
       "                      1.0679, 1.0767, 1.0078, 0.9794, 1.0096, 0.9176, 0.8638, 1.0283, 0.9737,\n",
       "                      1.2252, 1.0279, 0.9214, 0.9660, 0.8590, 0.9516, 1.0178, 1.0257, 1.0721,\n",
       "                      0.9995, 1.1100, 0.9444, 1.0553, 1.0520, 1.0258, 1.1414, 0.9887, 0.9818,\n",
       "                      0.9086, 1.1466, 1.0036, 1.1037, 1.0161, 0.8205, 1.2591, 0.8926, 0.8514,\n",
       "                      1.0578, 0.8890, 0.9357, 1.3010, 0.8723, 0.8431, 0.8837, 1.2148, 0.8862,\n",
       "                      0.9405, 0.9404, 1.1990, 1.0773, 1.0639, 0.8848, 0.9433, 1.0363, 1.0046,\n",
       "                      1.0270, 0.9579, 1.1859, 0.9207, 1.1511, 1.3795, 1.2800, 1.0925, 1.0532,\n",
       "                      0.9058, 1.1037, 1.1546, 1.4678, 0.8637, 1.0895, 0.9946, 0.8525, 1.0097,\n",
       "                      1.0130, 1.0805, 0.9000, 0.9938, 0.9160, 0.9015, 0.9371, 0.8111, 1.3362,\n",
       "                      0.9495, 1.1069, 0.9501, 0.9961, 0.9890, 1.1570, 0.9527, 1.0031, 1.0758,\n",
       "                      1.1379, 0.9681, 1.0346, 1.2044, 1.0579, 0.8749, 0.9119, 0.9735, 0.8443,\n",
       "                      1.0533, 1.0377, 1.1058, 1.0459, 1.1998, 1.0271, 1.1396, 1.0182, 0.9188,\n",
       "                      0.9236, 0.8104, 1.0594, 1.0661, 1.0839, 0.9170, 1.0707, 0.8988, 1.1952,\n",
       "                      0.9974, 1.0785, 0.8823, 0.9564, 1.0571, 0.9317, 1.0122, 0.8481, 0.9531,\n",
       "                      0.8810, 1.2565, 1.1384, 1.1721, 1.0015, 0.8507, 1.0699, 1.0163, 1.2615,\n",
       "                      0.8918, 1.0795, 0.9757, 0.8327, 0.9575, 0.8884, 0.8958, 1.2742, 1.0658,\n",
       "                      1.2866, 1.0630, 1.1079, 1.2583, 0.9767, 0.9718, 0.9714, 1.0285, 0.8815,\n",
       "                      0.9229, 0.9334, 1.0596, 0.8340, 0.8687, 1.1986, 0.8790, 1.0460, 1.0660,\n",
       "                      1.1495, 1.0142, 1.0247, 0.9169, 1.1923, 1.1508, 0.9166, 1.0844, 1.0570,\n",
       "                      1.0437, 0.9492, 1.0880, 1.1078, 1.1288, 1.1991, 1.0240, 0.9840, 1.1613,\n",
       "                      1.1755, 0.9921, 1.2538, 0.8211, 1.0968, 1.0313, 0.9014, 0.8620, 1.0041,\n",
       "                      0.8604, 0.8378, 0.9413, 0.9620, 1.0918, 0.9267, 0.8885, 1.0364, 0.9624,\n",
       "                      0.8983, 1.0512, 1.4554, 0.9081, 0.9179, 0.8989, 1.1124, 0.7234, 0.8016,\n",
       "                      0.9833, 1.1875, 0.9908, 0.9968, 0.9409, 1.0454, 1.1355, 0.9838, 1.0515,\n",
       "                      1.0068, 1.2345, 0.9344, 1.0950, 0.9760, 0.8508, 0.8350, 1.2071, 1.0936,\n",
       "                      1.1375, 0.9939, 0.9701, 1.0758, 0.9567, 1.1311, 0.8718, 1.2685, 1.0941,\n",
       "                      0.7996, 1.0430, 0.9925, 1.0784], device='mps:0')),\n",
       "             ('model.8.1.bias',\n",
       "              tensor([-0.4297, -0.5306, -0.6522, -0.9460, -0.5909, -0.6346, -0.3648, -0.4678,\n",
       "                      -0.6528, -0.5129, -0.4458, -0.5373, -0.6273, -0.6467, -0.6617, -0.4957,\n",
       "                      -0.5592, -0.4778, -0.4642, -0.5432, -0.5031, -0.5746, -0.5194, -0.4136,\n",
       "                      -0.4350, -0.5166, -0.4099, -0.8586, -0.5386, -0.4165, -0.3852, -0.6161,\n",
       "                      -0.4731, -0.4703, -0.5774, -0.6953, -0.5967, -0.5666, -0.4505, -0.5691,\n",
       "                      -0.6166, -0.4949, -0.5010, -0.4893, -0.2954, -0.5817, -0.7623, -0.5835,\n",
       "                      -0.6048, -0.4831, -0.4075, -0.8801, -0.5465, -0.5924, -0.5974, -0.4542,\n",
       "                      -0.5365, -0.7509, -0.5065, -0.5472, -0.5536, -0.9623, -0.4126, -0.6254,\n",
       "                      -0.5301, -0.6219, -0.6780, -0.5267, -0.5273, -0.3749, -0.4525, -0.5350,\n",
       "                      -0.7062, -0.5382, -0.5854, -0.4124, -0.7003, -0.7501, -0.6699, -0.5676,\n",
       "                      -0.4915, -0.5154, -0.5079, -0.6253, -1.0708, -0.3795, -0.5496, -0.4179,\n",
       "                      -0.4856, -0.5466, -0.3663, -0.7055, -0.6357, -0.6861, -0.5295, -0.6710,\n",
       "                      -0.5037, -0.5149, -0.6724, -0.4119, -0.7595, -0.4505, -0.6070, -0.4635,\n",
       "                      -0.6386, -0.5070, -0.4414, -0.4363, -0.8381, -0.5448, -0.4918, -0.3434,\n",
       "                      -0.5887, -0.4095, -0.7194, -0.5968, -0.4249, -0.5259, -0.3846, -0.7521,\n",
       "                      -0.5495, -0.7547, -0.4687, -0.4553, -0.5425, -0.4854, -0.4703, -0.4409,\n",
       "                      -0.4465, -0.6088, -0.4162, -0.5630, -0.5933, -0.5668, -0.7198, -0.3739,\n",
       "                      -0.4954, -0.4415, -0.5833, -0.5773, -0.4115, -0.5750, -0.6064, -0.4999,\n",
       "                      -0.5046, -0.7538, -0.5424, -0.2867, -0.4727, -0.6023, -0.4670, -0.5892,\n",
       "                      -0.8211, -0.4967, -0.6999, -0.5581, -0.4762, -0.4887, -0.3881, -0.6685,\n",
       "                      -0.5463, -0.4625, -0.6814, -0.6130, -0.5760, -0.5286, -0.5229, -0.5419,\n",
       "                      -0.4914, -0.5513, -0.5305, -0.6109, -0.3773, -0.6094, -0.4505, -0.4551,\n",
       "                      -0.6951, -0.5618, -0.5203, -0.4814, -0.6877, -0.6259, -0.4978, -0.6259,\n",
       "                      -0.5890, -0.6354, -0.4739, -0.4595, -0.6090, -0.5328, -0.4235, -0.6749,\n",
       "                      -0.7384, -0.7435, -0.6458, -0.7893, -0.4314, -0.4818, -0.3246, -0.6784,\n",
       "                      -0.7593, -0.4083, -0.5982, -0.5051, -0.5719, -0.5169, -0.5252, -0.4735,\n",
       "                      -0.4891, -0.5337, -0.4350, -0.6003, -0.6640, -0.4913, -0.5520, -0.6375,\n",
       "                      -0.5353, -0.6208, -1.2581, -0.5395, -0.4945, -0.5444, -0.4623, -0.4866,\n",
       "                      -0.4639, -0.3255, -0.6739, -0.5035, -0.6120, -0.6378, -0.4831, -0.4154,\n",
       "                      -0.5791, -0.4852, -0.5985, -0.9302, -0.5191, -0.6362, -0.5885, -0.4418,\n",
       "                      -0.4797, -0.6341, -0.6760, -0.6201, -0.7298, -0.5507, -0.3566, -0.6133,\n",
       "                      -0.6768, -0.4960, -0.5479, -0.6586, -0.4764, -0.5067, -0.6405, -0.6827],\n",
       "                     device='mps:0')),\n",
       "             ('model.8.1.running_mean',\n",
       "              tensor([-0.4941, -1.0094, -1.7038, -1.0933, -0.2052, -0.6391, -0.4792, -0.4224,\n",
       "                      -1.0133, -0.7561, -0.9313, -0.7992, -1.1958, -0.2733, -0.6834, -1.1708,\n",
       "                      -1.4565, -0.9840, -0.5442, -1.2844, -0.8634, -0.7614, -0.8149, -0.7377,\n",
       "                      -0.8901, -1.0905, -0.1318, -1.6965, -1.2550, -0.1603, -0.4412, -0.4027,\n",
       "                      -1.1215, -0.1332, -1.5407, -0.7312, -0.6987, -1.3687, -1.6190, -1.6145,\n",
       "                      -0.6750, -1.2861, -0.0686, -0.8464, -0.4286, -0.3853, -0.8645, -0.3802,\n",
       "                      -0.6891, -0.1906, -0.8926, -0.7729, -1.2678, -0.7436, -1.3636, -0.7532,\n",
       "                      -0.6793, -0.8838, -0.7856, -1.1342,  0.3065, -0.7740, -0.8689, -0.7690,\n",
       "                      -1.4846, -1.8971, -1.2731, -1.5975, -1.0179, -1.6392, -0.7223, -0.6726,\n",
       "                      -1.6052, -1.1385, -1.1608, -1.5979, -1.0783, -0.0740, -1.6524, -1.3062,\n",
       "                      -0.9039, -1.0959, -1.3106, -1.6887, -0.1421, -0.8588, -1.4282, -1.2323,\n",
       "                      -0.6870, -0.9969, -0.3731, -0.4749, -1.0172, -0.2783, -0.7414, -0.6548,\n",
       "                      -1.0883, -0.9434, -0.9272, -0.8400, -1.3631, -1.2235,  0.0030, -0.6526,\n",
       "                      -1.2325, -0.9278, -1.6742, -0.7637, -0.6251, -1.1302, -0.8258, -1.3936,\n",
       "                      -1.0333, -1.0198, -0.6443, -1.2059, -1.0648, -0.6850, -0.2598, -1.3440,\n",
       "                      -1.1077, -1.2384, -1.3569, -1.2979, -0.8110, -1.4729, -1.2413, -0.5510,\n",
       "                      -0.4350, -0.5714, -0.4959, -0.3828, -1.4676, -1.1333, -1.9143, -0.8810,\n",
       "                      -1.0257, -1.2960, -0.6164, -0.6843, -0.7134, -1.1263, -0.8478, -0.6832,\n",
       "                      -1.1148, -1.0303, -1.0954, -0.5106, -0.7351, -0.6655, -0.7223, -0.8696,\n",
       "                      -1.2527, -0.9287, -1.1434, -1.2369, -1.0862, -0.7628, -1.1860, -0.3517,\n",
       "                      -0.4351, -0.4609, -2.0867, -1.2452, -0.9814, -1.5421, -0.4826, -0.6479,\n",
       "                      -1.3353, -0.6166, -1.2609, -1.1498, -0.3941, -0.9518, -1.3849, -0.8849,\n",
       "                      -1.5015, -0.3610, -0.2098, -0.2693, -0.9152, -0.2270, -1.0735, -1.4492,\n",
       "                      -0.1947, -0.6456, -0.4786, -0.1632, -1.2389, -1.1187, -0.5968, -0.2930,\n",
       "                      -1.0498, -1.4973, -1.6538, -1.1066, -0.8791, -0.8750, -0.8764, -0.9775,\n",
       "                      -1.2535, -1.1121, -1.3016, -0.8005, -0.7101, -0.9187, -0.2567, -1.1557,\n",
       "                      -0.8811, -0.6118, -1.4393, -1.8863, -0.4922, -0.8364, -1.1875, -0.3090,\n",
       "                      -1.5371, -0.9290, -0.8305, -0.9168, -0.9670, -0.9073, -0.3790, -0.5844,\n",
       "                      -1.5206, -0.9008, -1.2376, -0.1514, -1.7173, -1.0512, -1.9179, -0.4761,\n",
       "                      -1.2766, -0.4609, -0.8908, -1.1010, -1.2556, -0.4665, -0.5917, -0.7208,\n",
       "                      -0.7762, -1.4240, -1.3414, -1.0601, -1.0455, -0.5723, -0.2770, -0.9137,\n",
       "                      -1.0671, -1.4327, -0.7263, -1.3254, -1.1180, -2.0386, -1.1148, -0.1366],\n",
       "                     device='mps:0')),\n",
       "             ('model.8.1.running_var',\n",
       "              tensor([1.4486, 0.9410, 1.9336, 1.8464, 0.7545, 1.1844, 2.0913, 1.9786, 1.4301,\n",
       "                      0.7836, 0.7025, 1.4848, 1.2630, 0.7371, 1.1402, 0.7764, 1.3917, 0.9301,\n",
       "                      2.0012, 1.0894, 1.8083, 1.1755, 0.6608, 1.4172, 0.8511, 1.1616, 1.9248,\n",
       "                      1.7294, 1.2275, 1.0325, 1.3673, 0.6833, 1.3174, 1.0913, 1.1091, 1.5753,\n",
       "                      1.5667, 2.0198, 1.2408, 1.3926, 1.6969, 1.0224, 2.6325, 1.6065, 1.3534,\n",
       "                      0.7884, 1.4172, 1.1191, 1.8695, 1.9037, 0.7170, 2.5935, 0.8115, 0.6250,\n",
       "                      1.3450, 0.9900, 1.0555, 1.9741, 0.4291, 1.0998, 1.3793, 2.3187, 1.0527,\n",
       "                      0.9273, 1.0971, 2.0150, 1.4723, 1.3347, 0.5419, 1.2895, 1.2344, 0.9124,\n",
       "                      1.3229, 1.4740, 1.7842, 1.0743, 1.8532, 4.2239, 3.1490, 1.5755, 1.2016,\n",
       "                      1.1128, 2.0294, 2.3130, 2.3699, 0.7994, 1.9352, 1.3770, 0.6520, 1.1812,\n",
       "                      2.0382, 1.5768, 0.9825, 0.9533, 0.8271, 1.1481, 0.8781, 0.7549, 1.9215,\n",
       "                      1.1129, 1.6512, 1.4645, 2.0936, 1.7363, 2.0574, 1.4335, 1.6929, 2.4693,\n",
       "                      1.6423, 1.5502, 1.9099, 2.5104, 1.4948, 0.8968, 0.9023, 1.0435, 0.6639,\n",
       "                      2.0681, 1.8700, 2.4401, 1.0850, 2.3445, 1.3905, 2.7239, 1.3116, 1.0093,\n",
       "                      1.1597, 0.4427, 1.9121, 1.2763, 1.8084, 0.8658, 1.2189, 0.7377, 1.8883,\n",
       "                      1.4138, 1.6949, 0.5872, 0.6841, 0.9259, 1.2526, 0.8595, 0.9855, 0.8502,\n",
       "                      0.5589, 2.9702, 1.8585, 2.5343, 1.5037, 0.6117, 2.3511, 1.0088, 1.8796,\n",
       "                      0.7482, 1.6115, 1.5352, 0.8302, 1.2797, 0.9398, 0.9961, 2.4591, 1.8624,\n",
       "                      2.2100, 1.7028, 2.4314, 2.3245, 1.3403, 0.9439, 0.8248, 1.3867, 1.1654,\n",
       "                      0.8723, 1.3498, 2.1462, 0.7504, 0.6136, 2.6036, 0.7964, 1.4090, 1.6043,\n",
       "                      1.3164, 1.2661, 2.0636, 0.8865, 2.5826, 1.9045, 1.5928, 1.2491, 1.3145,\n",
       "                      0.9583, 1.3678, 1.4880, 1.8180, 1.4515, 1.9290, 1.2365, 0.9649, 2.0984,\n",
       "                      2.5091, 0.8113, 2.4730, 0.4708, 1.8647, 1.6986, 0.8772, 0.8933, 1.3446,\n",
       "                      0.7136, 0.8453, 0.9925, 0.8827, 1.6811, 0.9046, 0.7479, 1.3585, 0.8767,\n",
       "                      0.9858, 1.6682, 2.7497, 1.0419, 0.9196, 0.9028, 2.4798, 0.6961, 0.7325,\n",
       "                      1.2366, 2.3166, 1.2347, 1.5441, 0.7337, 1.6086, 2.1972, 0.9934, 2.5463,\n",
       "                      0.9574, 1.9913, 1.3071, 1.9962, 1.0894, 0.7424, 0.8064, 2.2150, 1.6131,\n",
       "                      2.0072, 1.4788, 1.4969, 2.3147, 0.8446, 1.9977, 0.8550, 3.4108, 1.3492,\n",
       "                      0.5936, 1.6755, 1.4523, 1.3157], device='mps:0')),\n",
       "             ('model.8.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('classifier.weight',\n",
       "              tensor([[ 0.2539, -0.4129, -0.4710,  ..., -0.4428,  0.2303,  0.4016],\n",
       "                      [ 0.1029,  0.1596, -0.3664,  ..., -0.2843, -0.4157,  0.2217],\n",
       "                      [-0.5143, -0.1243,  0.5136,  ..., -0.0231,  0.6600, -0.4438],\n",
       "                      ...,\n",
       "                      [-0.2359, -0.4257, -0.5459,  ...,  0.6352, -0.4225, -0.0575],\n",
       "                      [ 0.6336, -0.2844, -0.2890,  ..., -0.3333, -0.1888, -0.2694],\n",
       "                      [ 0.2781,  0.1640,  0.0090,  ...,  0.6026, -0.0594, -0.3741]],\n",
       "                     device='mps:0')),\n",
       "             ('classifier.bias',\n",
       "              tensor([ 0.3287, -0.6605,  0.4113,  0.5110,  0.0181, -0.2609,  0.0303,  0.1200,\n",
       "                      -0.0530, -0.3119], device='mps:0'))])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_classes_no_defense.avg_weight_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 43/Epoch 0) Train Loss: 0.386 | Train Acc: 87.900Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.508 | Test Acc: 67.450\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 19.802228927612305 \n",
      "\n",
      "Round:  101\n",
      "(Device 7/Epoch 0) Train Loss: 0.438 | Train Acc: 85.440Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.583 | Test Acc: 79.280\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 31.828557014465332 \n",
      "\n",
      "Round:  102\n",
      "(Device 35/Epoch 0) Train Loss: 0.374 | Train Acc: 86.840Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.835 | Test Acc: 69.240\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 40.55018210411072 \n",
      "\n",
      "Round:  103\n",
      "(Device 19/Epoch 0) Train Loss: 0.373 | Train Acc: 86.660Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.657 | Test Acc: 77.470\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 49.13584089279175 \n",
      "\n",
      "Round:  104\n",
      "(Device 48/Epoch 0) Train Loss: 0.359 | Train Acc: 87.640Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.715 | Test Acc: 75.320\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 57.36453104019165 \n",
      "\n",
      "Round:  105\n",
      "(Device 39/Epoch 0) Train Loss: 0.357 | Train Acc: 87.160Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.696 | Test Acc: 76.980\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 65.82795310020447 \n",
      "\n",
      "Round:  106\n",
      "(Device 47/Epoch 0) Train Loss: 0.344 | Train Acc: 88.440Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.773 | Test Acc: 75.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 80.35896110534668 \n",
      "\n",
      "Round:  107\n",
      "(Device 30/Epoch 0) Train Loss: 0.352 | Train Acc: 87.580Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.681 | Test Acc: 75.670\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 92.11691689491272 \n",
      "\n",
      "Round:  108\n",
      "(Device 34/Epoch 0) Train Loss: 0.350 | Train Acc: 87.780Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.727 | Test Acc: 76.160\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 106.1803789138794 \n",
      "\n",
      "Round:  109\n",
      "(Device 49/Epoch 0) Train Loss: 0.351 | Train Acc: 87.620Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.777 | Test Acc: 74.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 116.06819200515747 \n",
      "\n",
      "Round:  110\n",
      "(Device 37/Epoch 0) Train Loss: 0.363 | Train Acc: 86.800Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.723 | Test Acc: 75.990\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 125.91239404678345 \n",
      "\n",
      "Round:  111\n",
      "(Device 30/Epoch 0) Train Loss: 0.400 | Train Acc: 86.420Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.736 | Test Acc: 73.820\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 135.5897409915924 \n",
      "\n",
      "Round:  112\n",
      "(Device 0/Epoch 0) Train Loss: 0.380 | Train Acc: 86.900Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.789 | Test Acc: 73.970\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 144.1921329498291 \n",
      "\n",
      "Round:  113\n",
      "(Device 29/Epoch 0) Train Loss: 0.344 | Train Acc: 88.060Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.865 | Test Acc: 74.200\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 153.08284711837769 \n",
      "\n",
      "Round:  114\n",
      "(Device 46/Epoch 0) Train Loss: 0.386 | Train Acc: 86.140Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.753 | Test Acc: 75.300\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 162.08767819404602 \n",
      "\n",
      "Round:  115\n",
      "(Device 16/Epoch 0) Train Loss: 0.348 | Train Acc: 87.500Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.866 | Test Acc: 73.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 172.04300713539124 \n",
      "\n",
      "Round:  116\n",
      "(Device 26/Epoch 0) Train Loss: 0.398 | Train Acc: 86.680Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.718 | Test Acc: 75.810\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 181.01415586471558 \n",
      "\n",
      "Round:  117\n",
      "(Device 6/Epoch 0) Train Loss: 0.337 | Train Acc: 88.180Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.835 | Test Acc: 74.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 193.92838311195374 \n",
      "\n",
      "Round:  118\n",
      "(Device 34/Epoch 0) Train Loss: 0.375 | Train Acc: 86.500Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.746 | Test Acc: 75.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 206.98106503486633 \n",
      "\n",
      "Round:  119\n",
      "(Device 3/Epoch 0) Train Loss: 0.430 | Train Acc: 87.220Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.843 | Test Acc: 75.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 220.10046815872192 \n",
      "\n",
      "Round:  120\n",
      "(Device 44/Epoch 0) Train Loss: 0.393 | Train Acc: 87.380Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.752 | Test Acc: 76.510\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 241.0419750213623 \n",
      "\n",
      "Round:  121\n",
      "(Device 10/Epoch 0) Train Loss: 0.345 | Train Acc: 87.780Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.796 | Test Acc: 76.320\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 255.8760280609131 \n",
      "\n",
      "Round:  122\n",
      "(Device 38/Epoch 0) Train Loss: 0.347 | Train Acc: 87.420Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.747 | Test Acc: 76.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 265.22406697273254 \n",
      "\n",
      "Round:  123\n",
      "(Device 43/Epoch 0) Train Loss: 0.378 | Train Acc: 87.140Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.734 | Test Acc: 75.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 276.5279037952423 \n",
      "\n",
      "Round:  124\n",
      "(Device 3/Epoch 0) Train Loss: 0.436 | Train Acc: 85.580Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.800 | Test Acc: 74.630\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 290.3334550857544 \n",
      "\n",
      "Round:  125\n",
      "(Device 6/Epoch 0) Train Loss: 0.406 | Train Acc: 86.400Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.084 | Test Acc: 69.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 299.6922001838684 \n",
      "\n",
      "Round:  126\n",
      "(Device 45/Epoch 0) Train Loss: 0.485 | Train Acc: 82.720Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.001 | Test Acc: 69.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 310.5632998943329 \n",
      "\n",
      "Round:  127\n",
      "(Device 21/Epoch 0) Train Loss: 0.474 | Train Acc: 82.600Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.011 | Test Acc: 69.810\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 322.5382959842682 \n",
      "\n",
      "Round:  128\n",
      "(Device 5/Epoch 0) Train Loss: 0.510 | Train Acc: 82.100Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.012 | Test Acc: 70.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 332.42797088623047 \n",
      "\n",
      "Round:  129\n",
      "(Device 32/Epoch 0) Train Loss: 0.513 | Train Acc: 81.520Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.020 | Test Acc: 69.590\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 341.52513813972473 \n",
      "\n",
      "Round:  130\n",
      "(Device 1/Epoch 0) Train Loss: 0.509 | Train Acc: 81.540 | Test Loss: 0.514 | Test Acc: 83.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 355.09473490715027 \n",
      "\n",
      "Round:  131\n",
      "(Device 3/Epoch 0) Train Loss: 0.382 | Train Acc: 87.240 | Test Loss: 0.463 | Test Acc: 84.640\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 369.60166001319885 \n",
      "\n",
      "Round:  132\n",
      "(Device 41/Epoch 0) Train Loss: 0.337 | Train Acc: 88.780 | Test Loss: 0.453 | Test Acc: 85.220\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 394.36123299598694 \n",
      "\n",
      "Round:  133\n",
      "(Device 30/Epoch 0) Train Loss: 0.328 | Train Acc: 89.080 | Test Loss: 0.438 | Test Acc: 85.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 408.3563930988312 \n",
      "\n",
      "Round:  134\n",
      "(Device 36/Epoch 0) Train Loss: 0.315 | Train Acc: 89.040 | Test Loss: 0.430 | Test Acc: 85.830\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 421.1418950557709 \n",
      "\n",
      "Round:  135\n",
      "(Device 5/Epoch 0) Train Loss: 0.337 | Train Acc: 89.600 | Test Loss: 0.438 | Test Acc: 85.610\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 440.06766510009766 \n",
      "\n",
      "Round:  136\n",
      "(Device 14/Epoch 0) Train Loss: 0.303 | Train Acc: 89.860 | Test Loss: 0.412 | Test Acc: 86.180\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 457.3614628314972 \n",
      "\n",
      "Round:  137\n",
      "(Device 34/Epoch 0) Train Loss: 0.315 | Train Acc: 89.980 | Test Loss: 0.425 | Test Acc: 86.080\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 472.93342208862305 \n",
      "\n",
      "Round:  138\n",
      "(Device 27/Epoch 0) Train Loss: 0.276 | Train Acc: 90.340 | Test Loss: 0.404 | Test Acc: 86.540\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 492.3361701965332 \n",
      "\n",
      "Round:  139\n",
      "(Device 28/Epoch 0) Train Loss: 0.265 | Train Acc: 91.020 | Test Loss: 0.404 | Test Acc: 86.830\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 511.4685800075531 \n",
      "\n",
      "Round:  140\n",
      "(Device 43/Epoch 0) Train Loss: 0.302 | Train Acc: 89.920 | Test Loss: 0.401 | Test Acc: 86.720\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 528.4219539165497 \n",
      "\n",
      "Round:  141\n",
      "(Device 26/Epoch 0) Train Loss: 0.273 | Train Acc: 90.200 | Test Loss: 0.399 | Test Acc: 86.760\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 544.2916059494019 \n",
      "\n",
      "Round:  142\n",
      "(Device 33/Epoch 0) Train Loss: 0.270 | Train Acc: 90.880 | Test Loss: 0.403 | Test Acc: 86.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 576.4343011379242 \n",
      "\n",
      "Round:  143\n",
      "(Device 12/Epoch 0) Train Loss: 0.253 | Train Acc: 91.580 | Test Loss: 0.398 | Test Acc: 86.980\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 592.7115209102631 \n",
      "\n",
      "Round:  144\n",
      "(Device 28/Epoch 0) Train Loss: 0.256 | Train Acc: 91.760 | Test Loss: 0.399 | Test Acc: 86.990\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 608.1541459560394 \n",
      "\n",
      "Round:  145\n",
      "(Device 25/Epoch 0) Train Loss: 0.279 | Train Acc: 91.000 | Test Loss: 0.399 | Test Acc: 86.950\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 650.8982601165771 \n",
      "\n",
      "Round:  146\n",
      "(Device 18/Epoch 0) Train Loss: 0.251 | Train Acc: 91.780 | Test Loss: 0.403 | Test Acc: 86.820\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 668.5553770065308 \n",
      "\n",
      "Round:  147\n",
      "(Device 19/Epoch 0) Train Loss: 0.270 | Train Acc: 91.140 | Test Loss: 0.395 | Test Acc: 86.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 686.0818939208984 \n",
      "\n",
      "Round:  148\n",
      "(Device 35/Epoch 0) Train Loss: 0.259 | Train Acc: 91.080 | Test Loss: 0.389 | Test Acc: 87.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 699.5693900585175 \n",
      "\n",
      "Round:  149\n",
      "(Device 20/Epoch 0) Train Loss: 0.262 | Train Acc: 91.780 | Test Loss: 0.396 | Test Acc: 87.020\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 720.389307975769 \n",
      "\n",
      "Total training time: 720.3893489837646 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sca_zero_one = switch_classes_attack(0,1,2)\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_no_defense_fixed_2 = run_federated_test(                    \n",
    "                                         rounds = 150, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 1,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.02,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_no_defense_multiple_final_fixed_2.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = False, \n",
    "                                         multiple_attack_rounds = (list(range(100,130)))) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_classes_no_defense_fixed = load_result(\"switch_classes_no_defense_multiple_final_fixed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 3.217 | Test Acc: 10.000\n",
      "\n",
      "Diff: 61.552120208740234\n",
      "\n",
      "Round time: 10.591506958007812 \n",
      "\n",
      "Round:  101\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.378 | Test Acc: 10.000\n",
      "\n",
      "Diff: 51.3704948425293\n",
      "\n",
      "Round time: 17.99462580680847 \n",
      "\n",
      "Round:  102\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.349 | Test Acc: 10.000\n",
      "\n",
      "Diff: 42.18416213989258\n",
      "\n",
      "Round time: 25.071220874786377 \n",
      "\n",
      "Round:  103\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.341 | Test Acc: 10.000\n",
      "\n",
      "Diff: 46.517311096191406\n",
      "\n",
      "Round time: 31.770357847213745 \n",
      "\n",
      "Round:  104\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 28.81142234802246\n",
      "\n",
      "Round time: 38.38723683357239 \n",
      "\n",
      "Round:  105\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 37.083778381347656\n",
      "\n",
      "Round time: 45.42861986160278 \n",
      "\n",
      "Round:  106\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 21.506816864013672\n",
      "\n",
      "Round time: 51.44661498069763 \n",
      "\n",
      "Round:  107\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 31.953285217285156\n",
      "\n",
      "Round time: 60.55612087249756 \n",
      "\n",
      "Round:  108\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 17.579307556152344\n",
      "\n",
      "Round time: 93.60609102249146 \n",
      "\n",
      "Round:  109\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 16.440658569335938\n",
      "\n",
      "Round time: 101.16266179084778 \n",
      "\n",
      "Round:  110\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 28.39679527282715\n",
      "\n",
      "Round time: 162.01305866241455 \n",
      "\n",
      "Round:  111\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 27.719554901123047\n",
      "\n",
      "Round time: 168.36285996437073 \n",
      "\n",
      "Round:  112\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 14.355100631713867\n",
      "\n",
      "Round time: 174.33972191810608 \n",
      "\n",
      "Round:  113\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.92732048034668\n",
      "\n",
      "Round time: 179.88981676101685 \n",
      "\n",
      "Round:  114\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.670799255371094\n",
      "\n",
      "Round time: 231.22068881988525 \n",
      "\n",
      "Round:  115\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.55969524383545\n",
      "\n",
      "Round time: 236.37237977981567 \n",
      "\n",
      "Round:  116\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.427392959594727\n",
      "\n",
      "Round time: 241.97601890563965 \n",
      "\n",
      "Round:  117\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.328110694885254\n",
      "\n",
      "Round time: 247.23519492149353 \n",
      "\n",
      "Round:  118\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.25359058380127\n",
      "\n",
      "Round time: 252.7442228794098 \n",
      "\n",
      "Round:  119\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.197632789611816\n",
      "\n",
      "Round time: 420.3441159725189 \n",
      "\n",
      "Round:  120\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.155601501464844\n",
      "\n",
      "Round time: 428.65854001045227 \n",
      "\n",
      "Round:  121\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.1240234375\n",
      "\n",
      "Round time: 443.08194875717163 \n",
      "\n",
      "Round:  122\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.10029411315918\n",
      "\n",
      "Round time: 452.06119084358215 \n",
      "\n",
      "Round:  123\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.082463264465332\n",
      "\n",
      "Round time: 458.6382179260254 \n",
      "\n",
      "Round:  124\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.034759521484375\n",
      "\n",
      "Round time: 465.1850926876068 \n",
      "\n",
      "Round:  125\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.06956958770752\n",
      "\n",
      "Round time: 471.02753591537476 \n",
      "\n",
      "Round:  126\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.059511184692383\n",
      "\n",
      "Round time: 476.9182369709015 \n",
      "\n",
      "Round:  127\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.017478942871094\n",
      "\n",
      "Round time: 482.5736289024353 \n",
      "\n",
      "Round:  128\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.020511627197266\n",
      "\n",
      "Round time: 489.25805377960205 \n",
      "\n",
      "Round:  129\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.02301597595215\n",
      "\n",
      "Round time: 495.76498889923096 \n",
      "\n",
      "Round:  130\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.4103201627731323\n",
      "\n",
      "Round time: 501.856055021286 \n",
      "\n",
      "Round:  131\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.32292768359184265\n",
      "\n",
      "Round time: 507.3080909252167 \n",
      "\n",
      "Round:  132\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.2541482746601105\n",
      "\n",
      "Round time: 513.599399805069 \n",
      "\n",
      "Round:  133\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.20001816749572754\n",
      "\n",
      "Round time: 519.2582058906555 \n",
      "\n",
      "Round:  134\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.15741708874702454\n",
      "\n",
      "Round time: 524.468759059906 \n",
      "\n",
      "Round:  135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.12388928979635239\n",
      "\n",
      "Round time: 529.5668148994446 \n",
      "\n",
      "Round:  136\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.09750255942344666\n",
      "\n",
      "Round time: 535.0100967884064 \n",
      "\n",
      "Round:  137\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.07673581689596176\n",
      "\n",
      "Round time: 541.0797357559204 \n",
      "\n",
      "Round:  138\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.060392115265131\n",
      "\n",
      "Round time: 548.1955320835114 \n",
      "\n",
      "Round:  139\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.04752938821911812\n",
      "\n",
      "Round time: 554.9258449077606 \n",
      "\n",
      "Round:  140\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.03740628808736801\n",
      "\n",
      "Round time: 560.588574886322 \n",
      "\n",
      "Round:  141\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.02943924255669117\n",
      "\n",
      "Round time: 566.3499519824982 \n",
      "\n",
      "Round:  142\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.02316909097135067\n",
      "\n",
      "Round time: 572.1516077518463 \n",
      "\n",
      "Round:  143\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.018234383314847946\n",
      "\n",
      "Round time: 577.8510336875916 \n",
      "\n",
      "Round:  144\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.014350706711411476\n",
      "\n",
      "Round time: 583.4424538612366 \n",
      "\n",
      "Round:  145\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.01129419356584549\n",
      "\n",
      "Round time: 589.2257039546967 \n",
      "\n",
      "Round:  146\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.008888684213161469\n",
      "\n",
      "Round time: 594.6060657501221 \n",
      "\n",
      "Round:  147\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.006995517294853926\n",
      "\n",
      "Round time: 599.3306179046631 \n",
      "\n",
      "Round:  148\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.005505563225597143\n",
      "\n",
      "Round time: 603.9891109466553 \n",
      "\n",
      "Round:  149\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.004332953132688999\n",
      "\n",
      "Round time: 609.0267109870911 \n",
      "\n",
      "Total training time: 609.0267210006714 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-0.01,0.01),\n",
    "    'model.1.0.weight' : (-0.01,0.01),\n",
    "    'model.2.0.weight' : (-0.01,0.01),\n",
    "    'model.3.0.weight' : (-0.01,0.01),\n",
    "    'model.4.0.weight' : (-0.01,0.01),\n",
    "    'model.5.0.weight' : (-0.01,0.01),\n",
    "    'model.6.0.weight' : (-0.01,0.01),\n",
    "    'model.7.0.weight' : (-0.01,0.01),\n",
    "    'model.8.0.weight' : (-0.01,0.01),\n",
    "}\n",
    "all_keys   = [x for x in baseline.avg_weight_history[-1].keys() if \".weight\" in x]\n",
    "test_cutoffs = {\n",
    "    x : (-0.01,0.01)\n",
    "    for x in all_keys\n",
    "}\n",
    "\n",
    "sigmoid_aggregation = make_sigmoid_defense(test_cutoffs, stickiness=0)\n",
    "\n",
    "# sca_zero_one = switch_classes_attack(0,1,2)\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_sigmoid_defense_fixed = run_federated_test(  agg_fn=sigmoid_aggregation,                \n",
    "                                         rounds = 150, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 0,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.4,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_sigmoid_defense_multiple_final_fixed.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = False, \n",
    "                                         multiple_attack_rounds = (list(range(100,130)))) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a function that extracts the data we need to make the plots about attacks that \n",
    "# switch a pair of classes (a,b)\n",
    "def make_evaluate_switch_classes_attack_weights(a, b):\n",
    "    checker_test_set_nontarget = make_testloader_subset([x for x in list(range(10)) if x not in [a,b]], 0, 0)\n",
    "    checker_test_set_target = make_testloader_subset([a,b], a, b)\n",
    "    \n",
    "    def f(w):\n",
    "        test_device = make_test_device(trainset)\n",
    "        test_device['net'].load_state_dict(w)\n",
    "\n",
    "        test(0, test_device, nn.CrossEntropyLoss(), checker_test_set_nontarget)\n",
    "        nontarget_accuracy = test_device['test_acc_tracker'][-1]\n",
    "        test(0, test_device, nn.CrossEntropyLoss(), checker_test_set_target)\n",
    "        target_accuracy = test_device['test_acc_tracker'][-1]\n",
    "        return nontarget_accuracy, target_accuracy\n",
    "    return f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      " | Test Loss: 2.495 | Test Acc: 12.750\n",
      " | Test Loss: 4.315 | Test Acc: 0.000\n",
      " | Test Loss: 1.529 | Test Acc: 43.112\n",
      " | Test Loss: 3.896 | Test Acc: 1.750\n",
      " | Test Loss: 1.215 | Test Acc: 56.038\n",
      " | Test Loss: 4.935 | Test Acc: 1.600\n",
      " | Test Loss: 1.095 | Test Acc: 60.550\n",
      " | Test Loss: 5.553 | Test Acc: 2.300\n",
      " | Test Loss: 1.050 | Test Acc: 62.237\n",
      " | Test Loss: 6.340 | Test Acc: 2.600\n",
      " | Test Loss: 0.918 | Test Acc: 66.938\n",
      " | Test Loss: 6.901 | Test Acc: 1.850\n",
      " | Test Loss: 0.874 | Test Acc: 69.225\n",
      " | Test Loss: 7.538 | Test Acc: 1.900\n",
      " | Test Loss: 0.765 | Test Acc: 72.787\n",
      " | Test Loss: 8.150 | Test Acc: 1.400\n",
      " | Test Loss: 0.732 | Test Acc: 74.875\n",
      " | Test Loss: 8.904 | Test Acc: 1.350\n",
      " | Test Loss: 0.704 | Test Acc: 75.537\n",
      " | Test Loss: 9.064 | Test Acc: 1.100\n",
      " | Test Loss: 0.641 | Test Acc: 78.325\n",
      " | Test Loss: 9.676 | Test Acc: 0.850\n",
      " | Test Loss: 0.633 | Test Acc: 78.588\n",
      " | Test Loss: 9.873 | Test Acc: 1.000\n",
      " | Test Loss: 0.591 | Test Acc: 79.938\n",
      " | Test Loss: 10.739 | Test Acc: 0.950\n",
      " | Test Loss: 0.575 | Test Acc: 80.700\n",
      " | Test Loss: 10.654 | Test Acc: 0.850\n",
      " | Test Loss: 0.553 | Test Acc: 81.550\n",
      " | Test Loss: 11.122 | Test Acc: 0.800\n",
      " | Test Loss: 0.563 | Test Acc: 81.125\n",
      " | Test Loss: 11.305 | Test Acc: 0.850\n",
      " | Test Loss: 0.511 | Test Acc: 82.838\n",
      " | Test Loss: 10.693 | Test Acc: 0.800\n",
      " | Test Loss: 0.521 | Test Acc: 83.162\n",
      " | Test Loss: 12.151 | Test Acc: 0.750\n",
      " | Test Loss: 0.492 | Test Acc: 83.963\n",
      " | Test Loss: 12.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.491 | Test Acc: 83.987\n",
      " | Test Loss: 12.486 | Test Acc: 0.750\n",
      " | Test Loss: 0.487 | Test Acc: 84.525\n",
      " | Test Loss: 13.002 | Test Acc: 0.700\n",
      " | Test Loss: 0.465 | Test Acc: 84.987\n",
      " | Test Loss: 12.353 | Test Acc: 0.750\n",
      " | Test Loss: 0.478 | Test Acc: 84.775\n",
      " | Test Loss: 12.557 | Test Acc: 0.750\n",
      " | Test Loss: 0.448 | Test Acc: 85.388\n",
      " | Test Loss: 13.923 | Test Acc: 0.650\n",
      " | Test Loss: 0.447 | Test Acc: 85.487\n",
      " | Test Loss: 13.003 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.075\n",
      " | Test Loss: 13.419 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.250\n",
      " | Test Loss: 13.364 | Test Acc: 0.600\n",
      " | Test Loss: 0.424 | Test Acc: 86.287\n",
      " | Test Loss: 13.373 | Test Acc: 0.600\n",
      " | Test Loss: 0.423 | Test Acc: 86.350\n",
      " | Test Loss: 13.393 | Test Acc: 0.550\n",
      " | Test Loss: 0.420 | Test Acc: 86.275\n",
      " | Test Loss: 13.580 | Test Acc: 0.600\n",
      " | Test Loss: 0.418 | Test Acc: 86.450\n",
      " | Test Loss: 13.570 | Test Acc: 0.600\n",
      " | Test Loss: 0.418 | Test Acc: 86.638\n",
      " | Test Loss: 13.781 | Test Acc: 0.650\n",
      " | Test Loss: 0.421 | Test Acc: 86.612\n",
      " | Test Loss: 13.731 | Test Acc: 0.700\n",
      " | Test Loss: 0.418 | Test Acc: 86.450\n",
      " | Test Loss: 13.786 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.463\n",
      " | Test Loss: 14.140 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.650\n",
      " | Test Loss: 14.069 | Test Acc: 0.700\n",
      " | Test Loss: 0.423 | Test Acc: 86.537\n",
      " | Test Loss: 14.123 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.675\n",
      " | Test Loss: 14.300 | Test Acc: 0.750\n",
      " | Test Loss: 0.425 | Test Acc: 86.500\n",
      " | Test Loss: 14.319 | Test Acc: 0.600\n",
      " | Test Loss: 0.420 | Test Acc: 86.825\n",
      " | Test Loss: 14.229 | Test Acc: 0.700\n",
      " | Test Loss: 0.425 | Test Acc: 86.775\n",
      " | Test Loss: 14.526 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.537\n",
      " | Test Loss: 14.549 | Test Acc: 0.700\n",
      " | Test Loss: 0.423 | Test Acc: 86.812\n",
      " | Test Loss: 14.497 | Test Acc: 0.650\n",
      " | Test Loss: 0.423 | Test Acc: 86.850\n",
      " | Test Loss: 14.632 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.812\n",
      " | Test Loss: 14.778 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.737\n",
      " | Test Loss: 14.787 | Test Acc: 0.750\n",
      " | Test Loss: 0.426 | Test Acc: 86.862\n",
      " | Test Loss: 14.844 | Test Acc: 0.750\n",
      " | Test Loss: 0.427 | Test Acc: 86.750\n",
      " | Test Loss: 14.902 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.800\n",
      " | Test Loss: 15.216 | Test Acc: 0.750\n",
      " | Test Loss: 0.426 | Test Acc: 87.025\n",
      " | Test Loss: 15.040 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 87.088\n",
      " | Test Loss: 15.092 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.950\n",
      " | Test Loss: 15.084 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.925\n",
      " | Test Loss: 15.199 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.812\n",
      " | Test Loss: 15.111 | Test Acc: 0.850\n",
      " | Test Loss: 0.424 | Test Acc: 87.000\n",
      " | Test Loss: 15.261 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.173 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.825\n",
      " | Test Loss: 15.223 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.325 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.912\n",
      " | Test Loss: 15.265 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.900\n",
      " | Test Loss: 15.115 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.125\n",
      " | Test Loss: 15.235 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.975\n",
      " | Test Loss: 15.302 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.050\n",
      " | Test Loss: 15.345 | Test Acc: 0.750\n",
      " | Test Loss: 0.424 | Test Acc: 86.987\n",
      " | Test Loss: 15.295 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.276 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.875\n",
      " | Test Loss: 15.285 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.950\n",
      " | Test Loss: 15.314 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.025\n",
      " | Test Loss: 15.228 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.975\n",
      " | Test Loss: 15.255 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.975\n",
      " | Test Loss: 15.257 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.975\n",
      " | Test Loss: 15.156 | Test Acc: 0.800\n",
      " | Test Loss: 0.420 | Test Acc: 87.062\n",
      " | Test Loss: 15.183 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.000\n",
      " | Test Loss: 15.287 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.297 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.025\n",
      " | Test Loss: 15.249 | Test Acc: 0.750\n",
      " | Test Loss: 0.427 | Test Acc: 86.963\n",
      " | Test Loss: 15.349 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.888\n",
      " | Test Loss: 15.307 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.963\n",
      " | Test Loss: 15.313 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.950\n",
      " | Test Loss: 15.302 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 87.013\n",
      " | Test Loss: 15.304 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.950\n",
      " | Test Loss: 15.200 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.912\n",
      " | Test Loss: 15.330 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.050\n",
      " | Test Loss: 15.409 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 87.025\n",
      " | Test Loss: 15.248 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 87.000\n",
      " | Test Loss: 15.236 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.235 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.037\n",
      " | Test Loss: 15.341 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.000\n",
      " | Test Loss: 15.322 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 87.025\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.900\n",
      " | Test Loss: 15.300 | Test Acc: 0.750\n",
      " | Test Loss: 0.424 | Test Acc: 86.900\n",
      " | Test Loss: 15.219 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.212 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.283 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.888\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.075\n",
      " | Test Loss: 15.247 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.062\n",
      " | Test Loss: 15.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.987\n",
      " | Test Loss: 15.313 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.135 | Test Acc: 0.800\n",
      " | Test Loss: 0.428 | Test Acc: 86.850\n",
      " | Test Loss: 15.289 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.191 | Test Acc: 0.800\n",
      " | Test Loss: 0.527 | Test Acc: 83.275\n",
      " | Test Loss: 0.888 | Test Acc: 73.150\n",
      " | Test Loss: 0.471 | Test Acc: 85.000\n",
      " | Test Loss: 2.341 | Test Acc: 25.100\n",
      " | Test Loss: 0.480 | Test Acc: 84.350\n",
      " | Test Loss: 0.837 | Test Acc: 71.750\n",
      " | Test Loss: 0.452 | Test Acc: 85.312\n",
      " | Test Loss: 2.059 | Test Acc: 27.650\n",
      " | Test Loss: 0.491 | Test Acc: 83.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 1.284 | Test Acc: 42.000\n",
      " | Test Loss: 0.442 | Test Acc: 85.088\n",
      " | Test Loss: 1.955 | Test Acc: 35.800\n",
      " | Test Loss: 0.541 | Test Acc: 83.075\n",
      " | Test Loss: 1.814 | Test Acc: 35.650\n",
      " | Test Loss: 0.439 | Test Acc: 85.575\n",
      " | Test Loss: 1.428 | Test Acc: 39.700\n",
      " | Test Loss: 0.478 | Test Acc: 84.213\n",
      " | Test Loss: 2.273 | Test Acc: 31.450\n",
      " | Test Loss: 0.519 | Test Acc: 83.037\n",
      " | Test Loss: 1.661 | Test Acc: 35.650\n",
      " | Test Loss: 0.506 | Test Acc: 83.975\n",
      " | Test Loss: 2.242 | Test Acc: 34.350\n",
      " | Test Loss: 0.460 | Test Acc: 84.912\n",
      " | Test Loss: 1.122 | Test Acc: 50.850\n",
      " | Test Loss: 0.484 | Test Acc: 84.037\n",
      " | Test Loss: 2.383 | Test Acc: 37.000\n",
      " | Test Loss: 0.488 | Test Acc: 84.250\n",
      " | Test Loss: 1.684 | Test Acc: 39.400\n",
      " | Test Loss: 0.466 | Test Acc: 84.500\n",
      " | Test Loss: 2.231 | Test Acc: 29.450\n",
      " | Test Loss: 0.491 | Test Acc: 84.325\n",
      " | Test Loss: 1.515 | Test Acc: 40.850\n",
      " | Test Loss: 0.471 | Test Acc: 84.612\n",
      " | Test Loss: 2.201 | Test Acc: 35.700\n",
      " | Test Loss: 0.469 | Test Acc: 84.625\n",
      " | Test Loss: 1.793 | Test Acc: 36.700\n",
      " | Test Loss: 0.463 | Test Acc: 84.900\n",
      " | Test Loss: 2.241 | Test Acc: 35.500\n",
      " | Test Loss: 0.449 | Test Acc: 85.237\n",
      " | Test Loss: 1.778 | Test Acc: 33.650\n",
      " | Test Loss: 0.476 | Test Acc: 84.562\n",
      " | Test Loss: 1.917 | Test Acc: 37.400\n",
      " | Test Loss: 0.465 | Test Acc: 84.787\n",
      " | Test Loss: 2.094 | Test Acc: 35.950\n",
      " | Test Loss: 0.458 | Test Acc: 84.912\n",
      " | Test Loss: 1.960 | Test Acc: 40.350\n",
      " | Test Loss: 0.535 | Test Acc: 82.263\n",
      " | Test Loss: 1.855 | Test Acc: 42.150\n",
      " | Test Loss: 0.480 | Test Acc: 84.037\n",
      " | Test Loss: 1.676 | Test Acc: 38.000\n",
      " | Test Loss: 0.467 | Test Acc: 84.600\n",
      " | Test Loss: 0.537 | Test Acc: 81.550\n",
      " | Test Loss: 0.480 | Test Acc: 84.200\n",
      " | Test Loss: 0.704 | Test Acc: 73.350\n",
      " | Test Loss: 0.484 | Test Acc: 84.188\n",
      " | Test Loss: 0.679 | Test Acc: 74.750\n",
      " | Test Loss: 0.476 | Test Acc: 84.513\n",
      " | Test Loss: 0.671 | Test Acc: 75.400\n",
      " | Test Loss: 0.479 | Test Acc: 84.263\n",
      " | Test Loss: 0.643 | Test Acc: 76.400\n",
      " | Test Loss: 0.495 | Test Acc: 83.938\n",
      " | Test Loss: 3.701 | Test Acc: 9.000\n",
      " | Test Loss: 0.469 | Test Acc: 84.662\n",
      " | Test Loss: 4.208 | Test Acc: 6.100\n",
      " | Test Loss: 0.446 | Test Acc: 85.487\n",
      " | Test Loss: 4.282 | Test Acc: 4.750\n",
      " | Test Loss: 0.444 | Test Acc: 85.612\n",
      " | Test Loss: 4.712 | Test Acc: 4.150\n",
      " | Test Loss: 0.428 | Test Acc: 85.912\n",
      " | Test Loss: 4.699 | Test Acc: 3.150\n",
      " | Test Loss: 0.443 | Test Acc: 85.300\n",
      " | Test Loss: 5.184 | Test Acc: 2.850\n",
      " | Test Loss: 0.431 | Test Acc: 85.625\n",
      " | Test Loss: 5.308 | Test Acc: 2.950\n",
      " | Test Loss: 0.425 | Test Acc: 86.125\n",
      " | Test Loss: 5.503 | Test Acc: 2.850\n",
      " | Test Loss: 0.422 | Test Acc: 85.987\n",
      " | Test Loss: 5.633 | Test Acc: 2.350\n",
      " | Test Loss: 0.419 | Test Acc: 86.375\n",
      " | Test Loss: 5.774 | Test Acc: 2.000\n",
      " | Test Loss: 0.429 | Test Acc: 85.925\n",
      " | Test Loss: 6.125 | Test Acc: 2.100\n",
      " | Test Loss: 0.414 | Test Acc: 86.388\n",
      " | Test Loss: 5.880 | Test Acc: 1.700\n",
      " | Test Loss: 0.426 | Test Acc: 86.200\n",
      " | Test Loss: 6.211 | Test Acc: 1.700\n",
      " | Test Loss: 0.419 | Test Acc: 86.388\n",
      " | Test Loss: 6.341 | Test Acc: 1.700\n",
      " | Test Loss: 0.422 | Test Acc: 86.425\n",
      " | Test Loss: 6.522 | Test Acc: 1.700\n",
      " | Test Loss: 0.417 | Test Acc: 86.513\n",
      " | Test Loss: 6.337 | Test Acc: 1.850\n",
      " | Test Loss: 0.425 | Test Acc: 86.275\n",
      " | Test Loss: 6.629 | Test Acc: 1.650\n",
      " | Test Loss: 0.422 | Test Acc: 86.188\n",
      " | Test Loss: 6.719 | Test Acc: 1.750\n",
      " | Test Loss: 0.409 | Test Acc: 86.662\n",
      " | Test Loss: 6.723 | Test Acc: 1.600\n",
      " | Test Loss: 0.416 | Test Acc: 86.525\n",
      " | Test Loss: 6.847 | Test Acc: 1.450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(12.75, 0.0),\n",
       " (43.1125, 1.75),\n",
       " (56.0375, 1.6),\n",
       " (60.55, 2.3),\n",
       " (62.2375, 2.6),\n",
       " (66.9375, 1.85),\n",
       " (69.225, 1.9),\n",
       " (72.7875, 1.4),\n",
       " (74.875, 1.35),\n",
       " (75.5375, 1.1),\n",
       " (78.325, 0.85),\n",
       " (78.5875, 1.0),\n",
       " (79.9375, 0.95),\n",
       " (80.7, 0.85),\n",
       " (81.55, 0.8),\n",
       " (81.125, 0.85),\n",
       " (82.8375, 0.8),\n",
       " (83.1625, 0.75),\n",
       " (83.9625, 0.8),\n",
       " (83.9875, 0.75),\n",
       " (84.525, 0.7),\n",
       " (84.9875, 0.75),\n",
       " (84.775, 0.75),\n",
       " (85.3875, 0.65),\n",
       " (85.4875, 0.6),\n",
       " (86.075, 0.6),\n",
       " (86.25, 0.6),\n",
       " (86.2875, 0.6),\n",
       " (86.35, 0.55),\n",
       " (86.275, 0.6),\n",
       " (86.45, 0.6),\n",
       " (86.6375, 0.65),\n",
       " (86.6125, 0.7),\n",
       " (86.45, 0.6),\n",
       " (86.4625, 0.75),\n",
       " (86.65, 0.7),\n",
       " (86.5375, 0.75),\n",
       " (86.675, 0.75),\n",
       " (86.5, 0.6),\n",
       " (86.825, 0.7),\n",
       " (86.775, 0.75),\n",
       " (86.5375, 0.7),\n",
       " (86.8125, 0.65),\n",
       " (86.85, 0.8),\n",
       " (86.8125, 0.8),\n",
       " (86.7375, 0.75),\n",
       " (86.8625, 0.75),\n",
       " (86.75, 0.8),\n",
       " (86.8, 0.75),\n",
       " (87.025, 0.8),\n",
       " (87.0875, 0.8),\n",
       " (86.95, 0.8),\n",
       " (86.925, 0.8),\n",
       " (86.8125, 0.85),\n",
       " (87.0, 0.8),\n",
       " (86.925, 0.8),\n",
       " (86.825, 0.8),\n",
       " (86.9875, 0.8),\n",
       " (86.9125, 0.75),\n",
       " (86.9, 0.8),\n",
       " (87.125, 0.75),\n",
       " (86.975, 0.8),\n",
       " (87.05, 0.75),\n",
       " (86.9875, 0.8),\n",
       " (87.0125, 0.8),\n",
       " (86.875, 0.8),\n",
       " (86.95, 0.75),\n",
       " (87.025, 0.8),\n",
       " (86.975, 0.8),\n",
       " (86.975, 0.8),\n",
       " (86.975, 0.8),\n",
       " (87.0625, 0.8),\n",
       " (87.0, 0.8),\n",
       " (87.0125, 0.8),\n",
       " (87.025, 0.75),\n",
       " (86.9625, 0.8),\n",
       " (86.8875, 0.8),\n",
       " (86.9625, 0.75),\n",
       " (86.95, 0.8),\n",
       " (87.0125, 0.8),\n",
       " (86.95, 0.8),\n",
       " (86.9125, 0.8),\n",
       " (87.05, 0.75),\n",
       " (87.025, 0.8),\n",
       " (87.0, 0.75),\n",
       " (87.0125, 0.8),\n",
       " (87.0375, 0.75),\n",
       " (87.0, 0.8),\n",
       " (87.025, 0.8),\n",
       " (86.9, 0.75),\n",
       " (86.9, 0.75),\n",
       " (86.9375, 0.8),\n",
       " (86.9875, 0.8),\n",
       " (86.8875, 0.8),\n",
       " (87.075, 0.8),\n",
       " (87.0625, 0.8),\n",
       " (86.9875, 0.8),\n",
       " (86.9375, 0.8),\n",
       " (86.85, 0.75),\n",
       " (86.925, 0.8),\n",
       " (83.275, 73.15),\n",
       " (85.0, 25.1),\n",
       " (84.35, 71.75),\n",
       " (85.3125, 27.65),\n",
       " (83.75, 42.0),\n",
       " (85.0875, 35.8),\n",
       " (83.075, 35.65),\n",
       " (85.575, 39.7),\n",
       " (84.2125, 31.45),\n",
       " (83.0375, 35.65),\n",
       " (83.975, 34.35),\n",
       " (84.9125, 50.85),\n",
       " (84.0375, 37.0),\n",
       " (84.25, 39.4),\n",
       " (84.5, 29.45),\n",
       " (84.325, 40.85),\n",
       " (84.6125, 35.7),\n",
       " (84.625, 36.7),\n",
       " (84.9, 35.5),\n",
       " (85.2375, 33.65),\n",
       " (84.5625, 37.4),\n",
       " (84.7875, 35.95),\n",
       " (84.9125, 40.35),\n",
       " (82.2625, 42.15),\n",
       " (84.0375, 38.0),\n",
       " (84.6, 81.55),\n",
       " (84.2, 73.35),\n",
       " (84.1875, 74.75),\n",
       " (84.5125, 75.4),\n",
       " (84.2625, 76.4),\n",
       " (83.9375, 9.0),\n",
       " (84.6625, 6.1),\n",
       " (85.4875, 4.75),\n",
       " (85.6125, 4.15),\n",
       " (85.9125, 3.15),\n",
       " (85.3, 2.85),\n",
       " (85.625, 2.95),\n",
       " (86.125, 2.85),\n",
       " (85.9875, 2.35),\n",
       " (86.375, 2.0),\n",
       " (85.925, 2.1),\n",
       " (86.3875, 1.7),\n",
       " (86.2, 1.7),\n",
       " (86.3875, 1.7),\n",
       " (86.425, 1.7),\n",
       " (86.5125, 1.85),\n",
       " (86.275, 1.65),\n",
       " (86.1875, 1.75),\n",
       " (86.6625, 1.6),\n",
       " (86.525, 1.45)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, we can load in some data and split it into classes as needed. \n",
    "no_defense_switch_classes = load_result(\"switch_classes_no_defense_multiple_final_fixed_2.pickle\")\n",
    "evaluate_switch_classes_attack_weights = make_evaluate_switch_classes_attack_weights(0,1)\n",
    "d = [evaluate_switch_classes_attack_weights(no_defense_switch_classes.avg_weight_history[r]) for r in range(len(no_defense_switch_classes.avg_weight_history))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load in our other final data for the noise experiments\n",
    "sigmoid_against_noise = load_result(\"sigmoid_against_noise_attack_final.pickle\")\n",
    "no_defense_noise = load_result(\"noise_attack_1_no_defense.pickle\")\n",
    "s6 = load_result(\"noise_attack_cheby_deg_6.pickle\")\n",
    "s10 = load_result(\"noise_attack_cheby_deg_10.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can make the plot!\n",
    "noise_data_elts = {\n",
    "    \"Hat Function No Approximation\" : sigmoid_against_noise.test_accuracy[100::],   \n",
    "    \"No Defense\" : no_defense_noise.test_accuracy[100:],\n",
    "    \"Degree 6 Chebyshev\" : s6.test_accuracy[90:120],\n",
    "    \"Degree 10 Chebyshev\" : s10.test_accuracy[90:120]\n",
    "}\n",
    "make_plot_better(tracks2, 30, title=\"\", y_axis_lab=\"Test Accuracy (%)\", should_average=False,  add_attack_region =(10,12), n=0, alpha = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheby_training = load_result(\"cheby_training_scratch_stricter.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_training = {\n",
    "    \"No Defense\" : baseline.test_accuracy[:50],   \n",
    "    \"Degree 10 Chebyshev\" : cheby_training.test_accuracy[:50]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plot_better(tracks_training, 50, title=\"\", y_axis_lab=\"Test Accuracy (%)\", should_average=False, n=0, alpha = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      " | Test Loss: 2.495 | Test Acc: 12.750\n",
      " | Test Loss: 4.315 | Test Acc: 0.000\n",
      " | Test Loss: 1.529 | Test Acc: 43.112\n",
      " | Test Loss: 3.896 | Test Acc: 1.750\n",
      " | Test Loss: 1.215 | Test Acc: 56.038\n",
      " | Test Loss: 4.935 | Test Acc: 1.600\n",
      " | Test Loss: 1.095 | Test Acc: 60.550\n",
      " | Test Loss: 5.553 | Test Acc: 2.300\n",
      " | Test Loss: 1.050 | Test Acc: 62.237\n",
      " | Test Loss: 6.340 | Test Acc: 2.600\n",
      " | Test Loss: 0.918 | Test Acc: 66.938\n",
      " | Test Loss: 6.901 | Test Acc: 1.850\n",
      " | Test Loss: 0.874 | Test Acc: 69.225\n",
      " | Test Loss: 7.538 | Test Acc: 1.900\n",
      " | Test Loss: 0.765 | Test Acc: 72.787\n",
      " | Test Loss: 8.150 | Test Acc: 1.400\n",
      " | Test Loss: 0.732 | Test Acc: 74.875\n",
      " | Test Loss: 8.904 | Test Acc: 1.350\n",
      " | Test Loss: 0.704 | Test Acc: 75.537\n",
      " | Test Loss: 9.064 | Test Acc: 1.100\n",
      " | Test Loss: 0.641 | Test Acc: 78.325\n",
      " | Test Loss: 9.676 | Test Acc: 0.850\n",
      " | Test Loss: 0.633 | Test Acc: 78.588\n",
      " | Test Loss: 9.873 | Test Acc: 1.000\n",
      " | Test Loss: 0.591 | Test Acc: 79.938\n",
      " | Test Loss: 10.739 | Test Acc: 0.950\n",
      " | Test Loss: 0.575 | Test Acc: 80.700\n",
      " | Test Loss: 10.654 | Test Acc: 0.850\n",
      " | Test Loss: 0.553 | Test Acc: 81.550\n",
      " | Test Loss: 11.122 | Test Acc: 0.800\n",
      " | Test Loss: 0.563 | Test Acc: 81.125\n",
      " | Test Loss: 11.305 | Test Acc: 0.850\n",
      " | Test Loss: 0.511 | Test Acc: 82.838\n",
      " | Test Loss: 10.693 | Test Acc: 0.800\n",
      " | Test Loss: 0.521 | Test Acc: 83.162\n",
      " | Test Loss: 12.151 | Test Acc: 0.750\n",
      " | Test Loss: 0.492 | Test Acc: 83.963\n",
      " | Test Loss: 12.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.491 | Test Acc: 83.987\n",
      " | Test Loss: 12.486 | Test Acc: 0.750\n",
      " | Test Loss: 0.487 | Test Acc: 84.525\n",
      " | Test Loss: 13.002 | Test Acc: 0.700\n",
      " | Test Loss: 0.465 | Test Acc: 84.987\n",
      " | Test Loss: 12.353 | Test Acc: 0.750\n",
      " | Test Loss: 0.478 | Test Acc: 84.775\n",
      " | Test Loss: 12.557 | Test Acc: 0.750\n",
      " | Test Loss: 0.448 | Test Acc: 85.388\n",
      " | Test Loss: 13.923 | Test Acc: 0.650\n",
      " | Test Loss: 0.447 | Test Acc: 85.487\n",
      " | Test Loss: 13.003 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.075\n",
      " | Test Loss: 13.419 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.250\n",
      " | Test Loss: 13.364 | Test Acc: 0.600\n",
      " | Test Loss: 0.424 | Test Acc: 86.287\n",
      " | Test Loss: 13.373 | Test Acc: 0.600\n",
      " | Test Loss: 0.423 | Test Acc: 86.350\n",
      " | Test Loss: 13.393 | Test Acc: 0.550\n",
      " | Test Loss: 0.420 | Test Acc: 86.275\n",
      " | Test Loss: 13.580 | Test Acc: 0.600\n",
      " | Test Loss: 0.418 | Test Acc: 86.450\n",
      " | Test Loss: 13.570 | Test Acc: 0.600\n",
      " | Test Loss: 0.418 | Test Acc: 86.638\n",
      " | Test Loss: 13.781 | Test Acc: 0.650\n",
      " | Test Loss: 0.421 | Test Acc: 86.612\n",
      " | Test Loss: 13.731 | Test Acc: 0.700\n",
      " | Test Loss: 0.418 | Test Acc: 86.450\n",
      " | Test Loss: 13.786 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.463\n",
      " | Test Loss: 14.140 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.650\n",
      " | Test Loss: 14.069 | Test Acc: 0.700\n",
      " | Test Loss: 0.423 | Test Acc: 86.537\n",
      " | Test Loss: 14.123 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.675\n",
      " | Test Loss: 14.300 | Test Acc: 0.750\n",
      " | Test Loss: 0.425 | Test Acc: 86.500\n",
      " | Test Loss: 14.319 | Test Acc: 0.600\n",
      " | Test Loss: 0.420 | Test Acc: 86.825\n",
      " | Test Loss: 14.229 | Test Acc: 0.700\n",
      " | Test Loss: 0.425 | Test Acc: 86.775\n",
      " | Test Loss: 14.526 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.537\n",
      " | Test Loss: 14.549 | Test Acc: 0.700\n",
      " | Test Loss: 0.423 | Test Acc: 86.812\n",
      " | Test Loss: 14.497 | Test Acc: 0.650\n",
      " | Test Loss: 0.423 | Test Acc: 86.850\n",
      " | Test Loss: 14.632 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.812\n",
      " | Test Loss: 14.778 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.737\n",
      " | Test Loss: 14.787 | Test Acc: 0.750\n",
      " | Test Loss: 0.426 | Test Acc: 86.862\n",
      " | Test Loss: 14.844 | Test Acc: 0.750\n",
      " | Test Loss: 0.427 | Test Acc: 86.750\n",
      " | Test Loss: 14.902 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.800\n",
      " | Test Loss: 15.216 | Test Acc: 0.750\n",
      " | Test Loss: 0.426 | Test Acc: 87.025\n",
      " | Test Loss: 15.040 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 87.088\n",
      " | Test Loss: 15.092 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.950\n",
      " | Test Loss: 15.084 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.925\n",
      " | Test Loss: 15.199 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.812\n",
      " | Test Loss: 15.111 | Test Acc: 0.850\n",
      " | Test Loss: 0.424 | Test Acc: 87.000\n",
      " | Test Loss: 15.261 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.173 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.825\n",
      " | Test Loss: 15.223 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.325 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.912\n",
      " | Test Loss: 15.265 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.900\n",
      " | Test Loss: 15.115 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.125\n",
      " | Test Loss: 15.235 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.975\n",
      " | Test Loss: 15.302 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.050\n",
      " | Test Loss: 15.345 | Test Acc: 0.750\n",
      " | Test Loss: 0.424 | Test Acc: 86.987\n",
      " | Test Loss: 15.295 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.276 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.875\n",
      " | Test Loss: 15.285 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.950\n",
      " | Test Loss: 15.314 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.025\n",
      " | Test Loss: 15.228 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.975\n",
      " | Test Loss: 15.255 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.975\n",
      " | Test Loss: 15.257 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.975\n",
      " | Test Loss: 15.156 | Test Acc: 0.800\n",
      " | Test Loss: 0.420 | Test Acc: 87.062\n",
      " | Test Loss: 15.183 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.000\n",
      " | Test Loss: 15.287 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.297 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.025\n",
      " | Test Loss: 15.249 | Test Acc: 0.750\n",
      " | Test Loss: 0.427 | Test Acc: 86.963\n",
      " | Test Loss: 15.349 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.888\n",
      " | Test Loss: 15.307 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.963\n",
      " | Test Loss: 15.313 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.950\n",
      " | Test Loss: 15.302 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 87.013\n",
      " | Test Loss: 15.304 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.950\n",
      " | Test Loss: 15.200 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.912\n",
      " | Test Loss: 15.330 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.050\n",
      " | Test Loss: 15.409 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 87.025\n",
      " | Test Loss: 15.248 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 87.000\n",
      " | Test Loss: 15.236 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.235 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.037\n",
      " | Test Loss: 15.341 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.000\n",
      " | Test Loss: 15.322 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 87.025\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.900\n",
      " | Test Loss: 15.300 | Test Acc: 0.750\n",
      " | Test Loss: 0.424 | Test Acc: 86.900\n",
      " | Test Loss: 15.219 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.212 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.283 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.888\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.075\n",
      " | Test Loss: 15.247 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.062\n",
      " | Test Loss: 15.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.987\n",
      " | Test Loss: 15.313 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.135 | Test Acc: 0.800\n",
      " | Test Loss: 0.428 | Test Acc: 86.850\n",
      " | Test Loss: 15.289 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.191 | Test Acc: 0.800\n",
      " | Test Loss: 0.824 | Test Acc: 75.675\n",
      " | Test Loss: 11.102 | Test Acc: 1.000\n",
      " | Test Loss: 0.616 | Test Acc: 81.362\n",
      " | Test Loss: 12.430 | Test Acc: 0.550\n",
      " | Test Loss: 0.857 | Test Acc: 77.275\n",
      " | Test Loss: 13.825 | Test Acc: 0.550\n",
      " | Test Loss: 0.886 | Test Acc: 75.938\n",
      " | Test Loss: 11.684 | Test Acc: 1.350\n",
      " | Test Loss: 0.583 | Test Acc: 82.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 13.626 | Test Acc: 0.750\n",
      " | Test Loss: 0.612 | Test Acc: 81.275\n",
      " | Test Loss: 15.433 | Test Acc: 1.250\n",
      " | Test Loss: 0.631 | Test Acc: 81.825\n",
      " | Test Loss: 14.179 | Test Acc: 0.750\n",
      " | Test Loss: 0.733 | Test Acc: 79.487\n",
      " | Test Loss: 13.546 | Test Acc: 1.250\n",
      " | Test Loss: 0.617 | Test Acc: 81.625\n",
      " | Test Loss: 13.896 | Test Acc: 1.050\n",
      " | Test Loss: 0.565 | Test Acc: 81.888\n",
      " | Test Loss: 11.614 | Test Acc: 1.050\n",
      " | Test Loss: 0.663 | Test Acc: 80.750\n",
      " | Test Loss: 12.890 | Test Acc: 0.650\n",
      " | Test Loss: 0.744 | Test Acc: 79.963\n",
      " | Test Loss: 13.860 | Test Acc: 0.700\n",
      " | Test Loss: 0.519 | Test Acc: 83.275\n",
      " | Test Loss: 13.681 | Test Acc: 1.050\n",
      " | Test Loss: 0.686 | Test Acc: 80.300\n",
      " | Test Loss: 12.990 | Test Acc: 0.900\n",
      " | Test Loss: 0.559 | Test Acc: 83.688\n",
      " | Test Loss: 14.816 | Test Acc: 0.750\n",
      " | Test Loss: 0.554 | Test Acc: 82.925\n",
      " | Test Loss: 15.259 | Test Acc: 0.700\n",
      " | Test Loss: 0.497 | Test Acc: 84.287\n",
      " | Test Loss: 14.029 | Test Acc: 0.500\n",
      " | Test Loss: 0.592 | Test Acc: 82.300\n",
      " | Test Loss: 13.064 | Test Acc: 1.050\n",
      " | Test Loss: 0.535 | Test Acc: 84.388\n",
      " | Test Loss: 13.483 | Test Acc: 1.000\n",
      " | Test Loss: 0.581 | Test Acc: 82.237\n",
      " | Test Loss: 11.915 | Test Acc: 0.950\n",
      " | Test Loss: 0.595 | Test Acc: 83.463\n",
      " | Test Loss: 14.649 | Test Acc: 0.650\n",
      " | Test Loss: 0.515 | Test Acc: 84.550\n",
      " | Test Loss: 13.883 | Test Acc: 0.750\n",
      " | Test Loss: 0.601 | Test Acc: 82.562\n",
      " | Test Loss: 13.322 | Test Acc: 0.500\n",
      " | Test Loss: 0.588 | Test Acc: 83.213\n",
      " | Test Loss: 14.672 | Test Acc: 0.850\n",
      " | Test Loss: 0.623 | Test Acc: 83.588\n",
      " | Test Loss: 13.809 | Test Acc: 0.900\n",
      " | Test Loss: 0.524 | Test Acc: 85.550\n",
      " | Test Loss: 15.219 | Test Acc: 0.500\n",
      " | Test Loss: 0.464 | Test Acc: 86.525\n",
      " | Test Loss: 14.905 | Test Acc: 0.500\n",
      " | Test Loss: 0.444 | Test Acc: 86.737\n",
      " | Test Loss: 15.174 | Test Acc: 0.550\n",
      " | Test Loss: 0.447 | Test Acc: 87.175\n",
      " | Test Loss: 15.210 | Test Acc: 0.500\n",
      " | Test Loss: 0.422 | Test Acc: 87.250\n",
      " | Test Loss: 15.770 | Test Acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "evaluate_switch_classes_attack_weights = make_evaluate_switch_classes_attack_weights(0,1)\n",
    "d = [evaluate_switch_classes_attack_weights(switch_classes_sigmoid_defense_actually_train.avg_weight_history[r]) for r in range(len(switch_classes_sigmoid_defense_actually_train.avg_weight_history))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_defense_switch_classes = load_result(\"switch_classes_no_defense_sat_2.pickle\")\n",
    "degree_10_switch_classes = load_result(\"sca_attack_cheby_deg_10_bigger.pickle\")\n",
    "degree_6_switch_classes = load_result(\"sca_attack_cheby_deg_6_bigger.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "evaluate_switch_classes_attack_weights = make_evaluate_switch_classes_attack_weights(0,1)\n",
    "def eval_attack_over_weight_history(x, start = 0):\n",
    "    return [evaluate_switch_classes_attack_weights(x.avg_weight_history[r]) for r in range(start, len(x.avg_weight_history))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 0.424 | Test Acc: 86.900\n",
      " | Test Loss: 15.219 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.212 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.283 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.888\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.075\n",
      " | Test Loss: 15.247 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.062\n",
      " | Test Loss: 15.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.987\n",
      " | Test Loss: 15.313 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.135 | Test Acc: 0.800\n",
      " | Test Loss: 0.428 | Test Acc: 86.850\n",
      " | Test Loss: 15.289 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.191 | Test Acc: 0.800\n",
      " | Test Loss: 0.662 | Test Acc: 83.500\n",
      " | Test Loss: 3.055 | Test Acc: 32.550\n",
      " | Test Loss: 0.536 | Test Acc: 86.975\n",
      " | Test Loss: 2.907 | Test Acc: 9.350\n",
      " | Test Loss: 0.601 | Test Acc: 86.525\n",
      " | Test Loss: 1.522 | Test Acc: 58.200\n",
      " | Test Loss: 0.629 | Test Acc: 86.575\n",
      " | Test Loss: 5.861 | Test Acc: 6.650\n",
      " | Test Loss: 0.606 | Test Acc: 86.825\n",
      " | Test Loss: 1.880 | Test Acc: 46.450\n",
      " | Test Loss: 0.649 | Test Acc: 86.138\n",
      " | Test Loss: 5.044 | Test Acc: 9.400\n",
      " | Test Loss: 0.674 | Test Acc: 85.562\n",
      " | Test Loss: 2.425 | Test Acc: 33.150\n",
      " | Test Loss: 0.644 | Test Acc: 85.862\n",
      " | Test Loss: 4.549 | Test Acc: 26.050\n",
      " | Test Loss: 0.617 | Test Acc: 86.362\n",
      " | Test Loss: 3.101 | Test Acc: 14.650\n",
      " | Test Loss: 0.599 | Test Acc: 87.037\n",
      " | Test Loss: 3.403 | Test Acc: 14.850\n",
      " | Test Loss: 0.546 | Test Acc: 86.963\n",
      " | Test Loss: 3.107 | Test Acc: 21.500\n",
      " | Test Loss: 0.681 | Test Acc: 82.987\n",
      " | Test Loss: 15.315 | Test Acc: 2.450\n",
      " | Test Loss: 0.564 | Test Acc: 84.600\n",
      " | Test Loss: 13.584 | Test Acc: 0.800\n",
      " | Test Loss: 0.492 | Test Acc: 86.088\n",
      " | Test Loss: 11.993 | Test Acc: 0.850\n",
      " | Test Loss: 0.493 | Test Acc: 85.888\n",
      " | Test Loss: 11.946 | Test Acc: 1.000\n",
      " | Test Loss: 0.515 | Test Acc: 85.250\n",
      " | Test Loss: 11.628 | Test Acc: 1.200\n",
      " | Test Loss: 0.519 | Test Acc: 86.638\n",
      " | Test Loss: 0.333 | Test Acc: 90.350\n",
      " | Test Loss: 0.537 | Test Acc: 85.237\n",
      " | Test Loss: 10.536 | Test Acc: 2.300\n",
      " | Test Loss: 0.470 | Test Acc: 86.325\n",
      " | Test Loss: 9.506 | Test Acc: 0.850\n",
      " | Test Loss: 0.505 | Test Acc: 85.537\n",
      " | Test Loss: 10.288 | Test Acc: 1.050\n",
      " | Test Loss: 0.507 | Test Acc: 85.375\n",
      " | Test Loss: 10.392 | Test Acc: 0.850\n",
      " | Test Loss: 0.516 | Test Acc: 87.300\n",
      " | Test Loss: 1.174 | Test Acc: 64.300\n",
      " | Test Loss: 0.567 | Test Acc: 85.188\n",
      " | Test Loss: 7.886 | Test Acc: 1.700\n",
      " | Test Loss: 0.486 | Test Acc: 86.438\n",
      " | Test Loss: 8.108 | Test Acc: 0.850\n",
      " | Test Loss: 0.491 | Test Acc: 86.025\n",
      " | Test Loss: 8.622 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 87.650\n",
      " | Test Loss: 1.868 | Test Acc: 8.450\n",
      " | Test Loss: 0.455 | Test Acc: 87.237\n",
      " | Test Loss: 3.086 | Test Acc: 2.350\n",
      " | Test Loss: 0.447 | Test Acc: 87.000\n",
      " | Test Loss: 3.635 | Test Acc: 1.150\n",
      " | Test Loss: 0.447 | Test Acc: 87.237\n",
      " | Test Loss: 4.042 | Test Acc: 1.000\n",
      " | Test Loss: 0.449 | Test Acc: 87.237\n",
      " | Test Loss: 4.508 | Test Acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "no_def_evaluate = eval_attack_over_weight_history(no_defense_switch_classes, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 0.424 | Test Acc: 86.900\n",
      " | Test Loss: 15.219 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.212 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.283 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.888\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.075\n",
      " | Test Loss: 15.247 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.062\n",
      " | Test Loss: 15.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.987\n",
      " | Test Loss: 15.313 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.135 | Test Acc: 0.800\n",
      " | Test Loss: 0.428 | Test Acc: 86.850\n",
      " | Test Loss: 15.289 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.191 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.562\n",
      " | Test Loss: 7.346 | Test Acc: 0.550\n",
      " | Test Loss: 0.404 | Test Acc: 87.525\n",
      " | Test Loss: 6.703 | Test Acc: 0.700\n",
      " | Test Loss: 0.396 | Test Acc: 88.400\n",
      " | Test Loss: 6.363 | Test Acc: 0.800\n",
      " | Test Loss: 0.389 | Test Acc: 88.775\n",
      " | Test Loss: 6.350 | Test Acc: 0.700\n",
      " | Test Loss: 0.412 | Test Acc: 88.375\n",
      " | Test Loss: 5.995 | Test Acc: 1.350\n",
      " | Test Loss: 0.438 | Test Acc: 88.338\n",
      " | Test Loss: 5.609 | Test Acc: 1.350\n",
      " | Test Loss: 0.454 | Test Acc: 88.438\n",
      " | Test Loss: 5.947 | Test Acc: 1.050\n",
      " | Test Loss: 0.411 | Test Acc: 88.987\n",
      " | Test Loss: 6.120 | Test Acc: 0.750\n",
      " | Test Loss: 0.414 | Test Acc: 88.862\n",
      " | Test Loss: 6.430 | Test Acc: 0.800\n",
      " | Test Loss: 0.453 | Test Acc: 87.838\n",
      " | Test Loss: 6.477 | Test Acc: 0.800\n",
      " | Test Loss: 0.438 | Test Acc: 88.525\n",
      " | Test Loss: 6.037 | Test Acc: 1.300\n",
      " | Test Loss: 0.408 | Test Acc: 88.112\n",
      " | Test Loss: 13.290 | Test Acc: 0.750\n",
      " | Test Loss: 0.406 | Test Acc: 88.300\n",
      " | Test Loss: 14.767 | Test Acc: 0.500\n",
      " | Test Loss: 0.415 | Test Acc: 88.075\n",
      " | Test Loss: 15.441 | Test Acc: 0.650\n",
      " | Test Loss: 0.403 | Test Acc: 88.100\n",
      " | Test Loss: 15.151 | Test Acc: 0.550\n",
      " | Test Loss: 0.410 | Test Acc: 88.013\n",
      " | Test Loss: 14.885 | Test Acc: 0.700\n",
      " | Test Loss: 0.413 | Test Acc: 88.188\n",
      " | Test Loss: 15.631 | Test Acc: 0.550\n",
      " | Test Loss: 0.421 | Test Acc: 88.225\n",
      " | Test Loss: 15.312 | Test Acc: 0.650\n",
      " | Test Loss: 0.412 | Test Acc: 88.688\n",
      " | Test Loss: 16.558 | Test Acc: 0.550\n",
      " | Test Loss: 0.431 | Test Acc: 88.013\n",
      " | Test Loss: 16.021 | Test Acc: 0.850\n",
      " | Test Loss: 0.405 | Test Acc: 88.525\n",
      " | Test Loss: 15.855 | Test Acc: 0.700\n",
      " | Test Loss: 0.406 | Test Acc: 88.412\n",
      " | Test Loss: 16.262 | Test Acc: 0.650\n",
      " | Test Loss: 0.423 | Test Acc: 88.362\n",
      " | Test Loss: 17.233 | Test Acc: 0.700\n",
      " | Test Loss: 0.418 | Test Acc: 88.450\n",
      " | Test Loss: 17.206 | Test Acc: 0.600\n",
      " | Test Loss: 0.410 | Test Acc: 88.600\n",
      " | Test Loss: 18.057 | Test Acc: 0.650\n",
      " | Test Loss: 0.402 | Test Acc: 88.963\n",
      " | Test Loss: 17.610 | Test Acc: 0.750\n",
      " | Test Loss: 0.404 | Test Acc: 89.150\n",
      " | Test Loss: 17.524 | Test Acc: 0.750\n",
      " | Test Loss: 0.399 | Test Acc: 89.037\n",
      " | Test Loss: 17.513 | Test Acc: 0.800\n",
      " | Test Loss: 0.386 | Test Acc: 89.600\n",
      " | Test Loss: 9.689 | Test Acc: 1.000\n",
      " | Test Loss: 0.396 | Test Acc: 89.550\n",
      " | Test Loss: 6.300 | Test Acc: 1.650\n"
     ]
    }
   ],
   "source": [
    "degree_6_evaluate = eval_attack_over_weight_history(degree_6_switch_classes, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 0.424 | Test Acc: 86.900\n",
      " | Test Loss: 15.219 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.212 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.283 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.888\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.075\n",
      " | Test Loss: 15.247 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.062\n",
      " | Test Loss: 15.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.987\n",
      " | Test Loss: 15.313 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.135 | Test Acc: 0.800\n",
      " | Test Loss: 0.428 | Test Acc: 86.850\n",
      " | Test Loss: 15.289 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.191 | Test Acc: 0.800\n",
      " | Test Loss: 0.430 | Test Acc: 85.925\n",
      " | Test Loss: 8.565 | Test Acc: 0.800\n",
      " | Test Loss: 0.402 | Test Acc: 87.700\n",
      " | Test Loss: 7.634 | Test Acc: 1.250\n",
      " | Test Loss: 0.416 | Test Acc: 87.638\n",
      " | Test Loss: 7.142 | Test Acc: 1.050\n",
      " | Test Loss: 0.404 | Test Acc: 88.537\n",
      " | Test Loss: 6.977 | Test Acc: 1.000\n",
      " | Test Loss: 0.418 | Test Acc: 88.100\n",
      " | Test Loss: 6.707 | Test Acc: 1.100\n",
      " | Test Loss: 0.435 | Test Acc: 87.862\n",
      " | Test Loss: 6.187 | Test Acc: 1.150\n",
      " | Test Loss: 0.440 | Test Acc: 88.075\n",
      " | Test Loss: 6.060 | Test Acc: 1.500\n",
      " | Test Loss: 0.425 | Test Acc: 88.562\n",
      " | Test Loss: 6.077 | Test Acc: 1.550\n",
      " | Test Loss: 0.419 | Test Acc: 88.350\n",
      " | Test Loss: 6.637 | Test Acc: 1.050\n",
      " | Test Loss: 0.434 | Test Acc: 87.938\n",
      " | Test Loss: 6.472 | Test Acc: 0.850\n",
      " | Test Loss: 0.450 | Test Acc: 88.225\n",
      " | Test Loss: 6.030 | Test Acc: 1.150\n",
      " | Test Loss: 0.448 | Test Acc: 87.388\n",
      " | Test Loss: 13.688 | Test Acc: 0.650\n",
      " | Test Loss: 0.413 | Test Acc: 88.362\n",
      " | Test Loss: 13.556 | Test Acc: 0.650\n",
      " | Test Loss: 0.432 | Test Acc: 87.787\n",
      " | Test Loss: 13.591 | Test Acc: 0.850\n",
      " | Test Loss: 0.402 | Test Acc: 88.500\n",
      " | Test Loss: 14.007 | Test Acc: 0.850\n",
      " | Test Loss: 0.408 | Test Acc: 88.350\n",
      " | Test Loss: 14.592 | Test Acc: 0.650\n",
      " | Test Loss: 0.411 | Test Acc: 88.463\n",
      " | Test Loss: 14.639 | Test Acc: 0.700\n",
      " | Test Loss: 0.397 | Test Acc: 89.150\n",
      " | Test Loss: 4.226 | Test Acc: 10.400\n",
      " | Test Loss: 0.377 | Test Acc: 88.963\n",
      " | Test Loss: 12.538 | Test Acc: 0.550\n",
      " | Test Loss: 0.421 | Test Acc: 88.213\n",
      " | Test Loss: 14.083 | Test Acc: 0.600\n",
      " | Test Loss: 0.419 | Test Acc: 88.537\n",
      " | Test Loss: 14.679 | Test Acc: 0.550\n",
      " | Test Loss: 0.395 | Test Acc: 89.550\n",
      " | Test Loss: 3.383 | Test Acc: 19.750\n",
      " | Test Loss: 0.417 | Test Acc: 88.513\n",
      " | Test Loss: 13.229 | Test Acc: 0.650\n",
      " | Test Loss: 0.396 | Test Acc: 89.425\n",
      " | Test Loss: 3.912 | Test Acc: 1.950\n",
      " | Test Loss: 0.445 | Test Acc: 88.075\n",
      " | Test Loss: 12.566 | Test Acc: 0.700\n",
      " | Test Loss: 0.413 | Test Acc: 88.862\n",
      " | Test Loss: 12.625 | Test Acc: 0.600\n",
      " | Test Loss: 0.416 | Test Acc: 88.850\n",
      " | Test Loss: 12.985 | Test Acc: 0.700\n",
      " | Test Loss: 0.406 | Test Acc: 89.100\n",
      " | Test Loss: 12.873 | Test Acc: 0.700\n",
      " | Test Loss: 0.405 | Test Acc: 88.875\n",
      " | Test Loss: 12.940 | Test Acc: 0.650\n",
      " | Test Loss: 0.411 | Test Acc: 88.987\n",
      " | Test Loss: 12.994 | Test Acc: 0.750\n"
     ]
    }
   ],
   "source": [
    "degree_10_evaluate = eval_attack_over_weight_history(degree_10_switch_classes, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "switch_classes_attack_data = {\n",
    "    \"Deg 10: Misclassifications (Attacked Cases)\" : [x[1] for x in degree_10_evaluate],   \n",
    "    \"Deg 10: Accuracy on Remaining Classes\" : [x[0] for x in degree_10_evaluate],\n",
    "    \"Deg 6: Misclassifications (Attacked Cases)\" : [x[1] for x in degree_6_evaluate],   \n",
    "    \"Deg 6: Accuracy on Remaining Classes\" : [x[0] for x in degree_6_evaluate],\n",
    "    \"Baseline: Misclassifications (Attacked Cases)\" : [x[1] for x in no_def_evaluate],   \n",
    "    \"Baseline: Accuracy on Remaining Classes\" : [x[0] for x in no_def_evaluate]\n",
    "}\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(6, 4.5), dpi=80)\n",
    "make_plot_better(switch_classes_attack_data, 40, title=\"\", y_axis_lab=\"Test Accuracy (%)\", should_average=True,  add_attack_region =(9,21), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "degree_6_tracks = {\n",
    "    \"Misclassifications (Attacked Cases)\" : [x[1] for x in degree_6_evaluate],   \n",
    "    \"Accuracy on Remaining Classes\" : [x[0] for x in degree_6_evaluate]\n",
    "}\n",
    "make_plot_better(degree_6_tracks, 40, title=\"Misclassification Attack Chebyshev Degree 6 Defense\", y_axis_lab=\"%\", should_average=True,  add_attack_region =(10,21), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_def_tracks = {\n",
    "    \"Misclassifications (Attacked Cases)\" : [x[1] for x in no_def_evaluate],   \n",
    "    \"Accuracy on Remaining Classes\" : [x[0] for x in no_def_evaluate]\n",
    "}\n",
    "make_plot_better(no_def_tracks, 40, title=\"Misclassification Attack Chebyshev Degree No Defense Defense\", y_axis_lab=\"%\", should_average=True,  add_attack_region =(10,21), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = ['model.0.0.weight',\n",
    " 'model.1.0.weight',\n",
    " 'model.2.0.weight',\n",
    " 'model.3.0.weight',\n",
    " 'model.4.0.weight',\n",
    " 'model.5.0.weight',\n",
    " 'model.6.0.weight',\n",
    " 'model.7.0.weight',\n",
    " 'model.8.0.weight']\n",
    "big_noise_attack = noise_attack_factory(target_layers,-0.4,0.4)\n",
    "\n",
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-1.5,1.5),\n",
    "    'model.1.0.weight' : (-0.5,0.5),\n",
    "    'model.2.0.weight' : (-0.5,0.5),\n",
    "    'model.3.0.weight' : (-0.5,0.5),\n",
    "    'model.4.0.weight' : (-0.5,0.5),\n",
    "    'model.5.0.weight' : (-0.5,0.5),\n",
    "    'model.6.0.weight' : (-0.5,0.5),\n",
    "    'model.7.0.weight' : (-0.5,0.5),\n",
    "    'model.8.0.weight' : (-0.5,0.5),\n",
    "}\n",
    "cheby_10_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=0, sigmoid_function_on_ab=HatChebyshevApproximator)\n",
    "\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "noise_attack_cheby_10 = run_federated_test( agg_fn = cheby_10_aggregation,                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.1,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = None, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = big_noise_attack, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         output_filename = \"noise_attack_cheby_deg_10.pickle\",\n",
    "                                         multiple_attack_rounds=[100]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-1.5,1.5),\n",
    "    'model.1.0.weight' : (-0.5,0.5),\n",
    "    'model.2.0.weight' : (-0.5,0.5),\n",
    "    'model.3.0.weight' : (-0.5,0.5),\n",
    "    'model.4.0.weight' : (-0.5,0.5),\n",
    "    'model.5.0.weight' : (-0.5,0.5),\n",
    "    'model.6.0.weight' : (-0.5,0.5),\n",
    "    'model.7.0.weight' : (-0.5,0.5),\n",
    "    'model.8.0.weight' : (-0.5,0.5),\n",
    "}\n",
    "cheby_6_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=0, sigmoid_function_on_ab=(lambda a,b: HatChebyshevApproximator(a,b,deg=6)))\n",
    "\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "noise_attack_cheby_6 = run_federated_test( agg_fn = cheby_6_aggregation,                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.1,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = None, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = big_noise_attack, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         output_filename = \"noise_attack_cheby_deg_6.pickle\",\n",
    "                                         multiple_attack_rounds=[100]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-1.5,1.5),\n",
    "    'model.1.0.weight' : (-0.5,0.5),\n",
    "    'model.2.0.weight' : (-0.5,0.5),\n",
    "    'model.3.0.weight' : (-0.5,0.5),\n",
    "    'model.4.0.weight' : (-0.5,0.5),\n",
    "    'model.5.0.weight' : (-0.5,0.5),\n",
    "    'model.6.0.weight' : (-0.5,0.5),\n",
    "    'model.7.0.weight' : (-0.5,0.5),\n",
    "    'model.8.0.weight' : (-0.5,0.5),\n",
    "}\n",
    "cheby_10_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=0, sigmoid_function_on_ab=HatChebyshevApproximator)\n",
    "sca_zero_one = switch_classes_attack(0,1,25)\n",
    "\n",
    "\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "sc_attack_cheby_10 = run_federated_test(  agg_fn = cheby_10_aggregation, \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.02,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"sca_attack_cheby_deg_10.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         multiple_attack_rounds = [100,101,102,103,104,105,106,107,108,109, 110]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-1.5,1.5),\n",
    "    'model.1.0.weight' : (-0.5,0.5),\n",
    "    'model.2.0.weight' : (-0.5,0.5),\n",
    "    'model.3.0.weight' : (-0.5,0.5),\n",
    "    'model.4.0.weight' : (-0.5,0.5),\n",
    "    'model.5.0.weight' : (-0.5,0.5),\n",
    "    'model.6.0.weight' : (-0.5,0.5),\n",
    "    'model.7.0.weight' : (-0.5,0.5),\n",
    "    'model.8.0.weight' : (-0.5,0.5),\n",
    "}\n",
    "cheby_6_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=0, sigmoid_function_on_ab=(lambda a,b: HatChebyshevApproximator(a,b,deg=6)))\n",
    "\n",
    "# sca_zero_one = switch_classes_attack(0,1,25)\n",
    "\n",
    "\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "sc_attack_cheby_6 = run_federated_test(  agg_fn = cheby_6_aggregation, \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.02,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"sca_attack_cheby_deg_6.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         multiple_attack_rounds = [100,101,102,103,104,105,106,107,108,109, 110]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 18/Epoch 2) Train Loss: 1.825 | Train Acc: 31.110"
     ]
    }
   ],
   "source": [
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-1.5,1.5),\n",
    "    'model.1.0.weight' : (-0.5,0.5),\n",
    "    'model.2.0.weight' : (-0.5,0.5),\n",
    "    'model.3.0.weight' : (-0.5,0.5),\n",
    "    'model.4.0.weight' : (-0.5,0.5),\n",
    "    'model.5.0.weight' : (-0.5,0.5),\n",
    "    'model.6.0.weight' : (-0.5,0.5),\n",
    "    'model.7.0.weight' : (-0.5,0.5),\n",
    "    'model.8.0.weight' : (-0.5,0.5),\n",
    "}\n",
    "cheby_10_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=0, sigmoid_function_on_ab=HatChebyshevApproximator)\n",
    "\n",
    "# baseline = load_result(\"baseline.pickle\")\n",
    "cheby_training_scratch = run_federated_test( agg_fn = cheby_10_aggregation,                    \n",
    "                                         rounds = 50, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.1,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = None, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = None, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = None,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         snapshot = True, \n",
    "                                         output_filename = \"cheby_training_scratch.pickle\",) # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs187",
   "language": "python",
   "name": "cs187"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
