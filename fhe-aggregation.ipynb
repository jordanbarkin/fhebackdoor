{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure GPU (change if not M1 mac)\n",
    "mps = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "\n",
    "# Using CIFAR-10 again as in the programming assignments\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving And helpers\n",
    "\n",
    "def save_tracker(tracker, path):\n",
    "  np.savetxt(path, tracker, delimiter=',') \n",
    "\n",
    "def save_trackers(device, filename):\n",
    "  \"\"\"Save all trackers and current total_time to a file.\"\"\"\n",
    "  torch.save((device['train_loss_tracker'], device['train_acc_tracker'], device['test_loss_tracker'], device['test_acc_tracker'], total_time), filename)\n",
    "  print(\"Saved trackers to \" + filename)\n",
    "\n",
    "def moving_average(a, n=100):\n",
    "  '''Helper function used for visualization'''\n",
    "  ret = torch.cumsum(torch.Tensor(a), 0)\n",
    "  ret[n:] = ret[n:] - ret[:-n]\n",
    "  return ret[n - 1:] / n\n",
    "\n",
    "# Plotting helpers! \n",
    "def make_plot(trackers, num_epochs, title, y_axis_lab, should_average=False, legend=True, fix_ax=True):\n",
    "  avg_fn = moving_average if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  if fix_ax:\n",
    "    ax.set_ylim([0, 100])\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "\n",
    "  if legend:\n",
    "    _ = plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def make_plot_better(trackers, num_epochs, title, y_axis_lab, should_average=False, n = 100):\n",
    "  avg_fn = (lambda x : moving_average(x, n)) if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "  _ = plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "def iid_sampler(dataset, num_devices, data_pct):\n",
    "    '''\n",
    "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
    "    num_devices: integer number of devices to create subsets for\n",
    "    data_pct: percentalge of training samples to give each device\n",
    "              e.g., 0.1 represents 10%\n",
    "\n",
    "    return: a dictionary of the following format:\n",
    "      {\n",
    "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
    "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    iid (independent and identically distributed) means that the indexes\n",
    "    should be drawn independently in a uniformly random fashion.\n",
    "    '''\n",
    "    total_samples = len(dataset)\n",
    "    sampled = {}\n",
    "    number_samples = int((data_pct)*(total_samples)) \n",
    "\n",
    "    for i in range(num_devices):\n",
    "      sampled[i] = random.sample(range(total_samples), number_samples)\n",
    "        \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net definitions\n",
    "\n",
    "# Same ConvNet as in Assignment 2 and 3\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated learning helpers\n",
    "\n",
    "# The baseline `average' function. \n",
    "def average_weights(devices):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    max_magnitude = 0\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "      w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            max_magnitude = max(max_magnitude, abs(torch.max(state_dicts[i][k].type(torch.float32))))\n",
    "            w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg, max_magnitude\n",
    "\n",
    "\n",
    "def get_devices_for_round(devices, device_pct):  \n",
    "    return random.sample(devices, int(device_pct * len(devices)))\n",
    "\n",
    "def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n",
    "                  milestones=None, batch_size=128):\n",
    "    if milestones == None:\n",
    "        milestones = [25, 50, 75]\n",
    "\n",
    "    device_net = copy.deepcopy(net)\n",
    "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=milestones,\n",
    "                                                     gamma=0.1)\n",
    "    device_trainset = DatasetSplit(trainset, data_idxs)\n",
    "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "    return {\n",
    "        'net': device_net,\n",
    "        'id': device_id,\n",
    "        'dataloader': device_trainloader, \n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_tracker': [],\n",
    "        'train_acc_tracker': [],\n",
    "        'test_loss_tracker': [],\n",
    "        'test_acc_tracker': [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local device training and testing\n",
    "def train(epoch, device, criterion):\n",
    "    device['net'].train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
    "        inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "        device['optimizer'].zero_grad()\n",
    "        outputs = device['net'](inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        device['optimizer'].step()\n",
    "        train_loss += loss.item()\n",
    "        device['train_loss_tracker'].append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        dev_id = device['id']\n",
    "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "                         f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    device['train_acc_tracker'].append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(epoch, device, criterion):\n",
    "    device['net'].eval()\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "            outputs = device['net'](inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            device['test_loss_tracker'].append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()  \n",
    "    acc = 100.*correct/total\n",
    "    device['test_acc_tracker'].append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function tests (the main experiment routine)\n",
    "\n",
    "# Given two different sets of aggregated weights, \n",
    "# gives a value representing the difference between them,\n",
    "# as a way to measure the direct cost of using our aggregated function\n",
    "def diff_aggregated_weights(strat, baseline):\n",
    "    result = 0 \n",
    "    for k in strat.keys():\n",
    "        result += torch.linalg.norm(strat[k] - baseline[k])\n",
    "    return result\n",
    "\n",
    "# A data type for experiment results\n",
    "BackdoorResult = namedtuple(\"BackdoorResult\", [\"scheme_loss\", \"test_accuracy\", \"backdoor_success\", \"devices\", \"avg_weight_history\"])\n",
    "\n",
    "# The main routine! \n",
    "def run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),  # Pass in aggregation function, \n",
    "                                                  # Device list -> aggregated weights \n",
    "                       rounds = 10,               # Rounds of FL\n",
    "                       local_epochs = 4,          # Epochs per device                      \n",
    "                       num_devices = 50,          # Total # devices\n",
    "                       device_pct = 0.1,          # % of devices per round\n",
    "                       data_pct = 0.1,            # % of data each device gets\n",
    "                       net = ConvNet().to(mps),   # Net design; make sure on mps\n",
    "                       evil_round = None,         # If None, no attack; else attacker will mount attack on this round\n",
    "                       attacker_strategy = None,  # device --> void --- set up the local weights on the attacker\n",
    "                       evil_device_id = None,     # Which device attacks? \n",
    "                       evaluate_attack = None,    # Function devices --> <any> measuring how well the attack worked at the end\n",
    "                       output_filename = None, \n",
    "                       snapshot = True, \n",
    "                       resume_from_snap = None):   \n",
    "    def lighten_device(d):\n",
    "        return {\n",
    "            k: d[k] for k in ( 'id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker')\n",
    "        }                                   \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "    devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "    \n",
    "    scheme_loss = []\n",
    "    max_magnitudes = []\n",
    "    avg_weight_history = []\n",
    "    _starting_round_num = 0\n",
    "    \n",
    "    if resume_from_snap: \n",
    "        print(\"Resuming from snapshot!\\n\")\n",
    "        # Load what we can\n",
    "        result = resume_from_snap\n",
    "        scheme_loss = result.scheme_loss\n",
    "        avg_weight_history = result.avg_weight_history\n",
    "        partial_devices = result.devices\n",
    "\n",
    "        # Fresh data and devices\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "        devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "\n",
    "        # Restore the devices with the info from the trackers\n",
    "        def restore_device(old_device, new_device):\n",
    "            for k in ('id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker'):\n",
    "                new_device[k] = old_device[k]\n",
    "            return new_device\n",
    "        \n",
    "        print(\"Restoring devices\\n\")\n",
    "        devices = [restore_device(partial_devices[i], devices[i]) for i in range(len(devices))]\n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(avg_weight_history[-1])\n",
    "        \n",
    "        _starting_round_num = len(result.test_accuracy)\n",
    "        print(\"Finished restoring\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "    ## IID Federated Learning\n",
    "    start_time = time.time()\n",
    "    for round_num in range(_starting_round_num, rounds):\n",
    "        round_start_time = time.time()\n",
    "        # Part 1.3: Implement getting devices for each round here\n",
    "        round_devices = get_devices_for_round(devices, device_pct)\n",
    "\n",
    "        print('Round: ', round_num)\n",
    "        # Train locally \n",
    "        for device in round_devices:\n",
    "            for local_epoch in range(local_epochs):\n",
    "                train(local_epoch, device, criterion)\n",
    "        \n",
    "        # One device becomes evil if required\n",
    "        if (evil_round and round_num == evil_round):\n",
    "            assert (evil_device_id is not None)\n",
    "            assert (attacker_strategy is not None)\n",
    "            print(\"Attacking!\\n\")\n",
    "            \n",
    "            attacker_strategy(devices[evil_device_id])\n",
    "            # Make sure evil guy gets averaged in \n",
    "            if evil_device_id not in round_devices:\n",
    "                round_devices.append(devices[evil_device_id])\n",
    "            \n",
    "        \n",
    "        # Weight averaging\n",
    "        w_baseline, max_magnitude = average_weights(round_devices)\n",
    "        w_avg = agg_fn(round_devices)\n",
    "        max_magnitudes.append(max_magnitude)\n",
    "        \n",
    "        # Track the difference between the two; should be 0 if straight average\n",
    "        scheme_loss.append((float(diff_aggregated_weights(w_avg, w_baseline))))\n",
    "        \n",
    "        avg_weight_history.append(copy.deepcopy(w_avg))\n",
    "        \n",
    "        # Gradients         \n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(w_avg)\n",
    "            device['optimizer'].zero_grad()\n",
    "            device['optimizer'].step()\n",
    "            device['scheduler'].step()\n",
    "\n",
    "        # test accuracy after aggregation\n",
    "        # device 0 is the unique device with all of the \n",
    "        # test accuracies and losses in its tracker\n",
    "        test(round_num, devices[0], criterion)\n",
    "        \n",
    "        print(f\"\\nDiff: {scheme_loss[-1]}\\n\")\n",
    "        print(f\"Round time: {time.time() - start_time} \\n\")\n",
    "        if snapshot:\n",
    "            intermediate_result = BackdoorResult(\n",
    "                scheme_loss = scheme_loss, \n",
    "                test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "                backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "                devices = [lighten_device(d) for d in devices], \n",
    "                avg_weight_history = avg_weight_history\n",
    "            )\n",
    "\n",
    "\n",
    "            if output_filename is not None: \n",
    "                print(\"Writing snapshot\\n\")\n",
    "                with open(f\"snapshot_{output_filename}\", 'wb') as file: \n",
    "                    pickle.dump(intermediate_result, file)\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    print('Total training time: {} seconds'.format(total_time))\n",
    "    \n",
    "    # Pack up everything we care about and the devices for good measure\n",
    "    result = BackdoorResult(\n",
    "        scheme_loss = scheme_loss, \n",
    "        test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "        backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "        devices = [lighten_device(d) for d in devices], \n",
    "        avg_weight_history = avg_weight_history\n",
    "    )\n",
    "\n",
    "    \n",
    "    if output_filename is not None: \n",
    "        with open(output_filename, 'wb') as file: \n",
    "            print(\"Writing file\\n\")\n",
    "            pickle.dump(result, file)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Load output files back into memory\n",
    "def load_result(filename):\n",
    "    with open(filename, 'rb') as file: \n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 7/Epoch 0) Train Loss: 2.090 | Train Acc: 23.100 | Test Loss: 9.358 | Test Acc: 12.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 18.737525701522827 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  1\n",
      "(Device 1/Epoch 0) Train Loss: 1.950 | Train Acc: 29.040 | Test Loss: 1.859 | Test Acc: 27.280\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 38.49649667739868 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 19.779122829437256 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "results = run_federated_test(local_epochs=1, num_devices = 10, rounds = 2, output_filename = \"testout.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Can also load it back\n",
    "# results = load_result(\"snapshot_testout.pickle\")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A silly attack that just sends massive weights all of magnitude 10000\n",
    "def sample_attack(device):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    weights = device[\"net\"].state_dict().copy()\n",
    "    \n",
    "    for w in weights.keys():\n",
    "        weights[w] = torch.full(weights[w].size(), 10000)\n",
    "    \n",
    "    device['net'].load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function that squashes any weights with magnitude over 1000 (except on the first device but whatever)\n",
    "def super_smart_aggregation(devices):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights, excluding huge updates.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            if (torch.max(state_dicts[i][k].type(torch.float32)) <= 1000): \n",
    "                w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 4/Epoch 0) Train Loss: 2.034 | Train Acc: 23.760 | Test Loss: 2.223 | Test Acc: 17.630\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  1\n",
      "(Device 9/Epoch 0) Train Loss: 1.864 | Train Acc: 28.560Attacking!\n",
      "\n",
      " | Test Loss: nan | Test Acc: 9.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  2\n",
      "(Device 0/Epoch 0) Train Loss: inf | Train Acc: 10.420 | Test Loss: 2.430 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Total training time: 60.91644906997681 seconds\n"
     ]
    }
   ],
   "source": [
    "# Example of how this works\n",
    "\n",
    "# Here, we carry out the attack but use the ordinary average\n",
    "results_straight_avg = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline_trivial_attack.pickle\")  \n",
    "# Observe that it absolutely destroys our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 7/Epoch 0) Train Loss: 2.018 | Train Acc: 23.180 | Test Loss: 18.179 | Test Acc: 12.360\n",
      "\n",
      "Diff: 84.85118865966797\n",
      "\n",
      "Round:  1\n",
      "(Device 9/Epoch 0) Train Loss: 1.873 | Train Acc: 29.680Attacking!\n",
      "\n",
      " | Test Loss: 3.511 | Test Acc: 17.340\n",
      "\n",
      "Diff: 10114592.0\n",
      "\n",
      "Round:  2\n",
      "(Device 2/Epoch 0) Train Loss: 1.789 | Train Acc: 32.900 | Test Loss: 2.722 | Test Acc: 23.900\n",
      "\n",
      "Diff: 46.39040756225586\n",
      "\n",
      "Total training time: 56.98331904411316 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now we do it again but with the aggregation that rejects huge updates\n",
    "results_reject_huge = run_federated_test(agg_fn = super_smart_aggregation,                    \n",
    "                                         rounds = 3,              \n",
    "                                         local_epochs = 1,                             \n",
    "                                         num_devices = 10,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 1,        \n",
    "                                         attacker_strategy = sample_attack,  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None,\n",
    "                                         output_filename = \"simple_attack_and_defense.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round:  0\n",
      "(Device 2/Epoch 3) Train Loss: 1.727 | Train Acc: 34.8200 | Test Loss: 2.821 | Test Acc: 10.200\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  1\n",
      "(Device 24/Epoch 3) Train Loss: 1.562 | Train Acc: 41.420 | Test Loss: 1.498 | Test Acc: 44.420\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  2\n",
      "(Device 21/Epoch 3) Train Loss: 1.314 | Train Acc: 50.700 | Test Loss: 1.242 | Test Acc: 54.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  3\n",
      "(Device 9/Epoch 3) Train Loss: 1.142 | Train Acc: 59.3200 | Test Loss: 1.086 | Test Acc: 60.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  4\n",
      "(Device 15/Epoch 3) Train Loss: 1.108 | Train Acc: 59.800 | Test Loss: 0.965 | Test Acc: 65.690\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  5\n",
      "(Device 1/Epoch 3) Train Loss: 1.012 | Train Acc: 63.1400 | Test Loss: 0.869 | Test Acc: 69.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  6\n",
      "(Device 28/Epoch 3) Train Loss: 0.883 | Train Acc: 68.640 | Test Loss: 0.815 | Test Acc: 71.450\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  7\n",
      "(Device 10/Epoch 3) Train Loss: 0.804 | Train Acc: 72.000 | Test Loss: 0.757 | Test Acc: 73.380\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  8\n",
      "(Device 41/Epoch 3) Train Loss: 0.766 | Train Acc: 73.160 | Test Loss: 0.683 | Test Acc: 76.610\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  9\n",
      "(Device 16/Epoch 3) Train Loss: 0.682 | Train Acc: 75.940 | Test Loss: 0.648 | Test Acc: 77.530\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  10\n",
      "(Device 24/Epoch 3) Train Loss: 0.643 | Train Acc: 77.680 | Test Loss: 0.602 | Test Acc: 79.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  11\n",
      "(Device 27/Epoch 3) Train Loss: 0.578 | Train Acc: 79.520 | Test Loss: 0.647 | Test Acc: 77.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  12\n",
      "(Device 27/Epoch 3) Train Loss: 0.583 | Train Acc: 79.420 | Test Loss: 0.595 | Test Acc: 79.950\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  13\n",
      "(Device 23/Epoch 3) Train Loss: 0.551 | Train Acc: 80.640 | Test Loss: 0.548 | Test Acc: 81.380\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  14\n",
      "(Device 24/Epoch 3) Train Loss: 0.537 | Train Acc: 81.080 | Test Loss: 0.521 | Test Acc: 82.400\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  15\n",
      "(Device 13/Epoch 3) Train Loss: 0.447 | Train Acc: 84.500 | Test Loss: 0.510 | Test Acc: 82.860\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  16\n",
      "(Device 41/Epoch 3) Train Loss: 0.539 | Train Acc: 80.840 | Test Loss: 0.504 | Test Acc: 83.430\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  17\n",
      "(Device 41/Epoch 3) Train Loss: 0.435 | Train Acc: 85.920 | Test Loss: 0.505 | Test Acc: 83.600\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  18\n",
      "(Device 36/Epoch 3) Train Loss: 0.414 | Train Acc: 85.340 | Test Loss: 0.469 | Test Acc: 84.710\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  19\n",
      "(Device 19/Epoch 3) Train Loss: 0.429 | Train Acc: 85.720 | Test Loss: 0.461 | Test Acc: 85.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  20\n",
      "(Device 15/Epoch 3) Train Loss: 0.356 | Train Acc: 87.620 | Test Loss: 0.449 | Test Acc: 85.460\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  21\n",
      "(Device 12/Epoch 3) Train Loss: 0.379 | Train Acc: 86.120 | Test Loss: 0.431 | Test Acc: 85.900\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  22\n",
      "(Device 28/Epoch 3) Train Loss: 0.382 | Train Acc: 87.000 | Test Loss: 0.442 | Test Acc: 85.790\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  23\n",
      "(Device 20/Epoch 3) Train Loss: 0.321 | Train Acc: 88.840 | Test Loss: 0.413 | Test Acc: 86.540\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  24\n",
      "(Device 15/Epoch 3) Train Loss: 0.270 | Train Acc: 90.920 | Test Loss: 0.419 | Test Acc: 86.480\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  25\n",
      "(Device 39/Epoch 3) Train Loss: 0.236 | Train Acc: 91.420 | Test Loss: 0.391 | Test Acc: 87.190\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  26\n",
      "(Device 36/Epoch 3) Train Loss: 0.245 | Train Acc: 92.040 | Test Loss: 0.390 | Test Acc: 87.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  27\n",
      "(Device 19/Epoch 3) Train Loss: 0.221 | Train Acc: 92.440 | Test Loss: 0.388 | Test Acc: 87.520\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  28\n",
      "(Device 19/Epoch 3) Train Loss: 0.235 | Train Acc: 92.680 | Test Loss: 0.386 | Test Acc: 87.520\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  29\n",
      "(Device 30/Epoch 3) Train Loss: 0.221 | Train Acc: 92.260 | Test Loss: 0.387 | Test Acc: 87.370\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  30\n",
      "(Device 26/Epoch 3) Train Loss: 0.221 | Train Acc: 92.580 | Test Loss: 0.386 | Test Acc: 87.480\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  31\n",
      "(Device 49/Epoch 3) Train Loss: 0.195 | Train Acc: 93.360 | Test Loss: 0.386 | Test Acc: 87.650\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  32\n",
      "(Device 34/Epoch 3) Train Loss: 0.216 | Train Acc: 93.480 | Test Loss: 0.389 | Test Acc: 87.620\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  33\n",
      "(Device 7/Epoch 3) Train Loss: 0.246 | Train Acc: 91.8800 | Test Loss: 0.384 | Test Acc: 87.560\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  34\n",
      "(Device 38/Epoch 3) Train Loss: 0.186 | Train Acc: 94.140 | Test Loss: 0.385 | Test Acc: 87.580\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  35\n",
      "(Device 44/Epoch 3) Train Loss: 0.189 | Train Acc: 93.860 | Test Loss: 0.387 | Test Acc: 87.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  36\n",
      "(Device 25/Epoch 3) Train Loss: 0.210 | Train Acc: 93.400 | Test Loss: 0.387 | Test Acc: 87.630\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  37\n",
      "(Device 38/Epoch 3) Train Loss: 0.184 | Train Acc: 94.600 | Test Loss: 0.389 | Test Acc: 87.620\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  38\n",
      "(Device 10/Epoch 3) Train Loss: 0.209 | Train Acc: 92.800 | Test Loss: 0.388 | Test Acc: 87.800\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  39\n",
      "(Device 47/Epoch 3) Train Loss: 0.180 | Train Acc: 93.820 | Test Loss: 0.388 | Test Acc: 87.760\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  40\n",
      "(Device 2/Epoch 3) Train Loss: 0.192 | Train Acc: 93.3200 | Test Loss: 0.390 | Test Acc: 87.920\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  41\n",
      "(Device 11/Epoch 3) Train Loss: 0.198 | Train Acc: 93.520 | Test Loss: 0.388 | Test Acc: 87.820\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  42\n",
      "(Device 11/Epoch 3) Train Loss: 0.218 | Train Acc: 93.460 | Test Loss: 0.391 | Test Acc: 87.870\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  43\n",
      "(Device 48/Epoch 3) Train Loss: 0.186 | Train Acc: 94.340 | Test Loss: 0.389 | Test Acc: 87.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  44\n",
      "(Device 28/Epoch 3) Train Loss: 0.175 | Train Acc: 93.680 | Test Loss: 0.390 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  45\n",
      "(Device 26/Epoch 3) Train Loss: 0.187 | Train Acc: 93.960 | Test Loss: 0.391 | Test Acc: 87.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  46\n",
      "(Device 29/Epoch 3) Train Loss: 0.171 | Train Acc: 93.880 | Test Loss: 0.388 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  47\n",
      "(Device 34/Epoch 3) Train Loss: 0.164 | Train Acc: 94.000 | Test Loss: 0.393 | Test Acc: 87.850\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  48\n",
      "(Device 40/Epoch 3) Train Loss: 0.191 | Train Acc: 94.680 | Test Loss: 0.393 | Test Acc: 87.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  49\n",
      "(Device 30/Epoch 3) Train Loss: 0.162 | Train Acc: 94.740 | Test Loss: 0.392 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  50\n",
      "(Device 48/Epoch 3) Train Loss: 0.177 | Train Acc: 93.800 | Test Loss: 0.391 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  51\n",
      "(Device 9/Epoch 3) Train Loss: 0.173 | Train Acc: 94.1000 | Test Loss: 0.390 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  52\n",
      "(Device 24/Epoch 3) Train Loss: 0.175 | Train Acc: 93.900 | Test Loss: 0.390 | Test Acc: 88.010\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  53\n",
      "(Device 49/Epoch 3) Train Loss: 0.174 | Train Acc: 94.080 | Test Loss: 0.390 | Test Acc: 87.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  54\n",
      "(Device 7/Epoch 3) Train Loss: 0.185 | Train Acc: 93.3800 | Test Loss: 0.389 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  55\n",
      "(Device 28/Epoch 3) Train Loss: 0.190 | Train Acc: 94.320 | Test Loss: 0.389 | Test Acc: 88.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  56\n",
      "(Device 37/Epoch 3) Train Loss: 0.165 | Train Acc: 94.220 | Test Loss: 0.388 | Test Acc: 87.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  57\n",
      "(Device 17/Epoch 3) Train Loss: 0.199 | Train Acc: 93.800 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  58\n",
      "(Device 37/Epoch 3) Train Loss: 0.170 | Train Acc: 94.200 | Test Loss: 0.387 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  59\n",
      "(Device 18/Epoch 3) Train Loss: 0.162 | Train Acc: 94.760 | Test Loss: 0.387 | Test Acc: 88.000\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  60\n",
      "(Device 7/Epoch 3) Train Loss: 0.184 | Train Acc: 93.3600 | Test Loss: 0.388 | Test Acc: 88.170\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  61\n",
      "(Device 40/Epoch 3) Train Loss: 0.170 | Train Acc: 94.060 | Test Loss: 0.387 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  62\n",
      "(Device 30/Epoch 3) Train Loss: 0.173 | Train Acc: 94.480 | Test Loss: 0.389 | Test Acc: 88.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  63\n",
      "(Device 18/Epoch 3) Train Loss: 0.164 | Train Acc: 94.420 | Test Loss: 0.388 | Test Acc: 88.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  64\n",
      "(Device 23/Epoch 3) Train Loss: 0.166 | Train Acc: 94.000 | Test Loss: 0.388 | Test Acc: 88.080\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  65\n",
      "(Device 46/Epoch 3) Train Loss: 0.162 | Train Acc: 94.620 | Test Loss: 0.388 | Test Acc: 87.990\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  66\n",
      "(Device 22/Epoch 3) Train Loss: 0.159 | Train Acc: 94.440 | Test Loss: 0.389 | Test Acc: 88.100\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  67\n",
      "(Device 27/Epoch 3) Train Loss: 0.180 | Train Acc: 95.020 | Test Loss: 0.387 | Test Acc: 88.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  68\n",
      "(Device 25/Epoch 3) Train Loss: 0.182 | Train Acc: 93.740 | Test Loss: 0.387 | Test Acc: 88.130\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Device 11/Epoch 3) Train Loss: 0.174 | Train Acc: 94.300 | Test Loss: 0.386 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  70\n",
      "(Device 0/Epoch 3) Train Loss: 0.190 | Train Acc: 94.0800 | Test Loss: 0.386 | Test Acc: 88.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  71\n",
      "(Device 31/Epoch 3) Train Loss: 0.166 | Train Acc: 94.200 | Test Loss: 0.386 | Test Acc: 88.120\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round:  72\n",
      "(Device 1/Epoch 1) Train Loss: 0.164 | Train Acc: 94.3820"
     ]
    }
   ],
   "source": [
    "# Here, we compute a real baseline over 100 epochs and typical averaging. Good starting point for future \n",
    "# tests; can resume from this snapshot.\n",
    "baseline = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 100,              \n",
    "                                         local_epochs = 4,                             \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = load_result(\"baseline.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, now can get things like\n",
    "# baseline.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 27/Epoch 3) Train Loss: 0.324 | Train Acc: 88.320 | Test Loss: 0.409 | Test Acc: 86.340\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 664.8843719959259 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 667.9921019077301 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can resume from a snapshot:\n",
    "# NOTE : keep all the basic params (local epochs, num devices, net, etc) the same\n",
    "# when resuming from a snapshot \n",
    "baseline2 = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "                                         rounds = 101,              \n",
    "                                         local_epochs = 4,                             \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evaluate_attack = None, \n",
    "                                         output_filename = \"baseline2.pickle\", \n",
    "                                         resume_from_snap = baseline )  # we pass in a results object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline2_loaded = load_result(\"baseline2.pickle\")\n",
    "baseline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can move on to a real defense. \n",
    "\n",
    "# We create factory functions that let us parametrize a double sigmoid using threshholds\n",
    "\n",
    "def torch_scaled_sigmoid_factory(a, c = 0.005, left = False):\n",
    "    \n",
    "    def f(x):\n",
    "        scaled = torch.sub(x, a)\n",
    "\n",
    "        divisor = c if left else -1*c\n",
    "\n",
    "        scaled = torch.div(scaled, divisor)\n",
    "\n",
    "        return torch.sigmoid(scaled)\n",
    "    return f\n",
    "\n",
    "def torch_double_sigmoid_factory(a,b,c = 0.005):\n",
    "    left = torch_scaled_sigmoid_factory(a, left = True)\n",
    "    right = torch_scaled_sigmoid_factory(b, left = False) \n",
    "    def f(x):\n",
    "        return x*(left(x)*right(x))\n",
    "    return f\n",
    "\n",
    "my_sigmoid = torch_double_sigmoid_factory(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it works\n",
    "# torch_double_sigmoid_factory(-2,2)(torch.Tensor([-5,-4,-3,-2,-1.5,-1.4,1.3,-1,0,1,1.5,1.6,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a sigmoid defense,\n",
    "#     keys_to_range_mapping maps each layer name in the state dict to the sigmoid bounds for processing it\n",
    "#     All other layers just get identity function-ed\n",
    "def make_sigmoid_defense(keys_to_range_mapping, stickiness = 0):\n",
    "    \n",
    "    sigmoid_dict = defaultdict(lambda : (lambda x: x))\n",
    "    \n",
    "    for k,(a,b) in keys_to_range_mapping.items():\n",
    "        sigmoid_dict[k] = torch_double_sigmoid_factory(a,b)\n",
    "    \n",
    "    def f(devices):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights preprocessed by sigmoid\n",
    "        '''\n",
    "        state_dicts = [device['net'].state_dict() for device in devices]\n",
    "        # initialize w_avg to tensors from device 0\n",
    "        w_avg = copy.deepcopy(state_dicts[0])\n",
    "        for k in w_avg.keys():\n",
    "            sig = sigmoid_dict[k]\n",
    "            w_avg[k] = sig(w_avg[k].type(torch.float32))\n",
    "            for i in range(stickiness):\n",
    "                w_avg[k] += sig((state_dicts[0][k].type(torch.float32)))                \n",
    "\n",
    "        # for each model param\n",
    "        for k in w_avg.keys():\n",
    "            sig = sigmoid_dict[k]\n",
    "            # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "            for i in range(1, len(devices)):\n",
    "                w_avg[k] += sig((state_dicts[i][k].type(torch.float32)))\n",
    "            # compute average\n",
    "            w_avg[k] /= float(len(devices) + stickiness)\n",
    "        return w_avg\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.0.0.weight',\n",
       " 'model.1.0.weight',\n",
       " 'model.2.0.weight',\n",
       " 'model.3.0.weight',\n",
       " 'model.4.0.weight',\n",
       " 'model.5.0.weight',\n",
       " 'model.6.0.weight',\n",
       " 'model.7.0.weight',\n",
       " 'model.8.0.weight']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the actual layer weights that we care about\n",
    "[x for x in baseline.avg_weight_history[-1].keys() if '.0.weight' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets set up a potential sigmoid defense\n",
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-2,2),\n",
    "    'model.1.0.weight' : (-2,2),\n",
    "    'model.2.0.weight' : (-.7,.7),\n",
    "    'model.3.0.weight' : (-.6,.6),\n",
    "    'model.4.0.weight' : (-0.7,0.7),\n",
    "    'model.5.0.weight' : (-0.5,0.5),\n",
    "    'model.6.0.weight' : (-0.3,0.3),\n",
    "    'model.7.0.weight' : (-0.2,0.2),\n",
    "    'model.8.0.weight' : (-0.2,0.2),\n",
    "}\n",
    "sigmoid_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid_resumption = run_federated_test(agg_fn = sigmoid_accuracy,                    \n",
    "#                                          rounds = 110,              \n",
    "#                                          local_epochs = 1,                             \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.05,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evaluate_attack = None, \n",
    "#                                          output_filename = None,\n",
    "#                                          snapshot = False,\n",
    "#                                          resume_from_snap = baseline )  # we pass in a results object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make an attack\n",
    "# Makes an attack that puts noise in all of target layers. \n",
    "# Noise is uniform over a,b\n",
    "def noise_attack_factory(target_layers, a,b):\n",
    "    def attack(device):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights.\n",
    "        '''\n",
    "        weights = device[\"net\"].state_dict().copy()\n",
    "\n",
    "        for w in weights.keys():\n",
    "            if w in target_layers:\n",
    "                weights[w] = (a - b) * torch.rand(weights[w].size()) + b\n",
    "\n",
    "        device['net'].load_state_dict(weights)\n",
    "    return attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We attack these layers with weights ranging from -4,4\n",
    "target_layers = ['model.0.0.weight',\n",
    " 'model.1.0.weight',\n",
    " 'model.2.0.weight',\n",
    " 'model.3.0.weight',\n",
    " 'model.4.0.weight',\n",
    " 'model.5.0.weight',\n",
    " 'model.6.0.weight',\n",
    " 'model.7.0.weight',\n",
    " 'model.8.0.weight']\n",
    "big_noise_attack = noise_attack_factory(target_layers,-3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll resume from our baseline model\n",
    "baseline = load_result(\"baseline.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 21/Epoch 2) Train Loss: 0.308 | Train Acc: 88.867"
     ]
    }
   ],
   "source": [
    "sigmoid_against_noise_attack = run_federated_test(agg_fn = sigmoid_aggregation,                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = big_noise_attack, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"sigmoid_against_noise_attack_1.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_sigmoid_on_cutoff(a,c=0.005, left = False):\n",
    "#     def f(x):\n",
    "#         if(not left and x > (a + 2)):\n",
    "#             return 0\n",
    "#         if(left and x < (a - 2)):\n",
    "#             return 0\n",
    "#         exponent = (-(x-a))/(c if left else -1*c)\n",
    "#         return 1/(1 + (math.e**(exponent)))\n",
    "#     return f \n",
    "\n",
    "# def make_double_sigmoid(a,b):\n",
    "#     left = make_sigmoid_on_cutoff(a, left = True)\n",
    "#     right = make_sigmoid_on_cutoff(b, left = False)\n",
    "#     def f(x):\n",
    "#         return left(x)*right(x)\n",
    "#     return f\n",
    "# baseline2.devices[0]['net']\n",
    "\n",
    "\n",
    "#         data_idxs = iid_sampler(trainset, 1, .1)\n",
    "\n",
    "#         devices = [create_device(ConvNet().to(mps), i, trainset, data_idxs[i]) for i in range(1)]\n",
    "    \n",
    "# device=  devices[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_noise_attack(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device['net'].state_dict()['model.0.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs187",
   "language": "python",
   "name": "cs187"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
