{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random \n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure GPU (change if not M1 mac)\n",
    "mps = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "\n",
    "# Using CIFAR-10 again as in the programming assignments\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving And helpers\n",
    "\n",
    "def save_tracker(tracker, path):\n",
    "  np.savetxt(path, tracker, delimiter=',') \n",
    "\n",
    "def save_trackers(device, filename):\n",
    "  \"\"\"Save all trackers and current total_time to a file.\"\"\"\n",
    "  torch.save((device['train_loss_tracker'], device['train_acc_tracker'], device['test_loss_tracker'], device['test_acc_tracker'], total_time), filename)\n",
    "  print(\"Saved trackers to \" + filename)\n",
    "\n",
    "def moving_average(a, n=100):\n",
    "  '''Helper function used for visualization'''\n",
    "  ret = torch.cumsum(torch.Tensor(a), 0)\n",
    "  ret[n:] = ret[n:] - ret[:-n]\n",
    "  return ret[n - 1:] / n\n",
    "\n",
    "# Plotting helpers! \n",
    "def make_plot(trackers, num_epochs, title, y_axis_lab, should_average=False, legend=True, fix_ax=True):\n",
    "  avg_fn = moving_average if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  if fix_ax:\n",
    "    ax.set_ylim([0, 100])\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "\n",
    "  if legend:\n",
    "    _ = plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def make_plot_better(trackers, num_epochs, title, y_axis_lab, should_average=False, n = 100):\n",
    "  avg_fn = (lambda x : moving_average(x, n)) if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "  _ = plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return image, torch.tensor(label)\n",
    "\n",
    "def iid_sampler(dataset, num_devices, data_pct):\n",
    "    '''\n",
    "    dataset: PyTorch Dataset (e.g., CIFAR-10 training set)\n",
    "    num_devices: integer number of devices to create subsets for\n",
    "    data_pct: percentalge of training samples to give each device\n",
    "              e.g., 0.1 represents 10%\n",
    "\n",
    "    return: a dictionary of the following format:\n",
    "      {\n",
    "        0: [3, 65, 2233, ..., 22] // device 0 sample indexes\n",
    "        1: [0, 2, 4, ..., 583] // device 1 sample indexes\n",
    "        ...\n",
    "      }\n",
    "\n",
    "    iid (independent and identically distributed) means that the indexes\n",
    "    should be drawn independently in a uniformly random fashion.\n",
    "    '''\n",
    "    total_samples = len(dataset)\n",
    "    sampled = {}\n",
    "    number_samples = int((data_pct)*(total_samples)) \n",
    "\n",
    "    for i in range(num_devices):\n",
    "      sampled[i] = random.sample(range(total_samples), number_samples)\n",
    "        \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net definitions\n",
    "\n",
    "# Same ConvNet as in Assignment 2 and 3\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federated learning helpers\n",
    "\n",
    "# The baseline `average' function. \n",
    "def average_weights(devices,*args, **kwargs):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "#     max_magnitude = 0\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "      w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "#             max_magnitude = max(max_magnitude, abs(torch.max(state_dicts[i][k].type(torch.float32))))\n",
    "            w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg\n",
    "\n",
    "\n",
    "def get_devices_for_round(devices, device_pct):  \n",
    "    return random.sample(devices, int(device_pct * len(devices)))\n",
    "\n",
    "def create_device(net, device_id, trainset, data_idxs, lr=0.1,\n",
    "                  milestones=None, batch_size=128):\n",
    "    if milestones == None:\n",
    "        milestones = [25, 50, 75]\n",
    "\n",
    "    device_net = copy.deepcopy(net)\n",
    "    optimizer = torch.optim.SGD(device_net.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                     milestones=milestones,\n",
    "                                                     gamma=0.1)\n",
    "    device_trainset = DatasetSplit(trainset, data_idxs)\n",
    "    device_trainloader = torch.utils.data.DataLoader(device_trainset,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     shuffle=True)\n",
    "    return {\n",
    "        'net': device_net,\n",
    "        'id': device_id,\n",
    "        'dataloader': device_trainloader, \n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_tracker': [],\n",
    "        'train_acc_tracker': [],\n",
    "        'test_loss_tracker': [],\n",
    "        'test_acc_tracker': [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local device training and testing\n",
    "def train(epoch, device, criterion):\n",
    "    device['net'].train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(device['dataloader']):\n",
    "        inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "        device['optimizer'].zero_grad()\n",
    "        outputs = device['net'](inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        device['optimizer'].step()\n",
    "        train_loss += loss.item()\n",
    "        device['train_loss_tracker'].append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        dev_id = device['id']\n",
    "        sys.stdout.write(f'\\r(Device {dev_id}/Epoch {epoch}) ' + \n",
    "                         f'Train Loss: {loss:.3f} | Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    device['train_acc_tracker'].append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(epoch, device, criterion, testloader = testloader):\n",
    "    device['net'].eval()\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(mps), targets.to(mps)\n",
    "            outputs = device['net'](inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            device['test_loss_tracker'].append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()  \n",
    "    acc = 100.*correct/total\n",
    "    device['test_acc_tracker'].append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function tests (the main experiment routine)\n",
    "\n",
    "# Given two different sets of aggregated weights, \n",
    "# gives a value representing the difference between them,\n",
    "# as a way to measure the direct cost of using our aggregated function\n",
    "def diff_aggregated_weights(strat, baseline):\n",
    "    result = 0 \n",
    "    for k in strat.keys():\n",
    "        result += torch.linalg.norm(strat[k] - baseline[k])\n",
    "    return result\n",
    "\n",
    "# A data type for experiment results\n",
    "BackdoorResult = namedtuple(\"BackdoorResult\", [\"scheme_loss\", \"test_accuracy\", \"backdoor_success\", \"devices\", \"avg_weight_history\"])\n",
    "\n",
    "\n",
    "def run_federated_test(agg_fn = average_weights,  # Pass in aggregation function, \n",
    "                                                  #     device list -> aggregated weights \n",
    "                       rounds = 10,               # Rounds of FL\n",
    "                       local_epochs = 4,          # Epochs per device                      \n",
    "                       num_devices = 50,          # Total # devices\n",
    "                       device_pct = 0.2,          # % of devices per round\n",
    "                       data_pct = 0.1,            # % of data each device gets\n",
    "                       net = ConvNet().to(mps),   # Network object; make sure on mps backend\n",
    "                       evil_round = None,         # integer list option; attacker will mount attack during these rounds\n",
    "                       attacker_strategy = None,  # device -> void; Sets up attacker weight update on device \n",
    "                       evil_device_id = None,     # Which device id should perform the attack\n",
    "                       evaluate_attack = None,    # Evaluated on the set of devices after each round and logged.\n",
    "                       output_filename = None,    # Write results to disk at this path\n",
    "                       snapshot = True,           # Snapshot to disk after each round\n",
    "                       resume_from_snap = None,   # Optionally resume from a prior snapshot.\n",
    "                       multiple_attack_rounds = []):   \n",
    "    '''\n",
    "    Returns instance of BackdoorResult, with following state: \n",
    "        scheme_loss        : difference between a straight average and aggregated weights after each round\n",
    "        test_accuracy      : classification accuracy after each round\n",
    "        backdoor_success   : result of evaluate_attack(devices) after each round\n",
    "        devices            : snapshot of devices after each round\n",
    "        avg_weight_history : snapshot of central server weights after each round\n",
    "    '''\n",
    "    \n",
    "    def lighten_device(d):\n",
    "        return {\n",
    "            k: d[k] for k in ( 'id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker')\n",
    "        }                                   \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "    devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "    \n",
    "    scheme_loss = []\n",
    "#     max_magnitudes = []\n",
    "    avg_weight_history = []\n",
    "    _starting_round_num = 0\n",
    "    \n",
    "    if resume_from_snap: \n",
    "        print(\"Resuming from snapshot!\\n\")\n",
    "        # Load what we can\n",
    "        result = resume_from_snap\n",
    "        scheme_loss = result.scheme_loss\n",
    "        avg_weight_history = result.avg_weight_history\n",
    "        partial_devices = result.devices\n",
    "\n",
    "        # Fresh data and devices\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        data_idxs = iid_sampler(trainset, num_devices, data_pct)\n",
    "\n",
    "        devices = [create_device(net, i, trainset, data_idxs[i]) for i in range(num_devices)]\n",
    "\n",
    "        # Restore the devices with the info from the trackers\n",
    "        def restore_device(old_device, new_device):\n",
    "            for k in ('id', 'train_loss_tracker', 'train_acc_tracker', 'test_loss_tracker', 'test_acc_tracker'):\n",
    "                new_device[k] = old_device[k]\n",
    "            return new_device\n",
    "        \n",
    "        print(\"Restoring devices\\n\")\n",
    "        devices = [restore_device(partial_devices[i], devices[i]) for i in range(len(devices))]\n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(avg_weight_history[-1])\n",
    "        \n",
    "        _starting_round_num = len(result.test_accuracy)\n",
    "        print(\"Finished restoring\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "    ## IID Federated Learning\n",
    "    start_time = time.time()\n",
    "    for round_num in range(_starting_round_num, rounds):\n",
    "        round_start_time = time.time()\n",
    "        # Part 1.3: Implement getting devices for each round here\n",
    "        round_devices = get_devices_for_round(devices, device_pct)\n",
    "\n",
    "        print('Round: ', round_num)\n",
    "        # Train locally \n",
    "        for device in round_devices:\n",
    "            for local_epoch in range(local_epochs):\n",
    "                train(local_epoch, device, criterion)\n",
    "        \n",
    "        # One device becomes evil if required\n",
    "        if ((evil_round and round_num == evil_round) or round_num in multiple_attack_rounds):\n",
    "            assert (evil_device_id is not None)\n",
    "            assert (attacker_strategy is not None)\n",
    "            print(\"Attacking!\\n\")\n",
    "            \n",
    "            attacker_strategy(devices[evil_device_id])\n",
    "            # Make sure evil guy gets averaged in \n",
    "            if evil_device_id not in round_devices:\n",
    "                round_devices.append(devices[evil_device_id])\n",
    "            print(\"Finished attacking\\n\")\n",
    "            \n",
    "        \n",
    "        # Weight averaging\n",
    "        w_baseline = average_weights(round_devices)\n",
    "        w_avg = agg_fn(round_devices, (avg_weight_history[-1] if avg_weight_history != [] else None))\n",
    "#         max_magnitudes.append(max_magnitude)\n",
    "        \n",
    "        # Track the difference between the two; should be 0 if straight average\n",
    "        scheme_loss.append((float(diff_aggregated_weights(w_avg, w_baseline))))\n",
    "        \n",
    "        avg_weight_history.append(copy.deepcopy(w_avg))\n",
    "        \n",
    "        # Gradients         \n",
    "        for device in devices:\n",
    "            device['net'].load_state_dict(w_avg)\n",
    "            device['optimizer'].zero_grad()\n",
    "            device['optimizer'].step()\n",
    "            device['scheduler'].step()\n",
    "\n",
    "        # test accuracy after aggregation\n",
    "        # device 0 is the unique device with all of the \n",
    "        # test accuracies and losses in its tracker\n",
    "        test(round_num, devices[0], criterion)\n",
    "        \n",
    "        print(f\"\\nDiff: {scheme_loss[-1]}\\n\")\n",
    "        print(f\"Round time: {time.time() - start_time} \\n\")\n",
    "        if snapshot:\n",
    "            intermediate_result = BackdoorResult(\n",
    "                scheme_loss = scheme_loss, \n",
    "                test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "                backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "                devices = [lighten_device(d) for d in devices], \n",
    "                avg_weight_history = avg_weight_history\n",
    "            )\n",
    "\n",
    "\n",
    "            if output_filename is not None: \n",
    "                print(\"Writing snapshot\\n\")\n",
    "                with open(f\"snapshot_{output_filename}\", 'wb') as file: \n",
    "                    pickle.dump(intermediate_result, file)\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    print('Total training time: {} seconds'.format(total_time))\n",
    "    \n",
    "    # Pack up everything we care about and the devices for good measure\n",
    "    result = BackdoorResult(\n",
    "        scheme_loss = scheme_loss, \n",
    "        test_accuracy = devices[0][\"test_acc_tracker\"], \n",
    "        backdoor_success = evaluate_attack(devices) if evaluate_attack is not None else None, \n",
    "        devices = [lighten_device(d) for d in devices], \n",
    "        avg_weight_history = avg_weight_history\n",
    "    )\n",
    "\n",
    "    \n",
    "    if output_filename is not None: \n",
    "        with open(output_filename, 'wb') as file: \n",
    "            print(\"Writing file\\n\")\n",
    "            pickle.dump(result, file)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Load output files back into memory\n",
    "def load_result(filename):\n",
    "    with open(filename, 'rb') as file: \n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# results = run_federated_test(local_epochs=1, num_devices = 10, rounds = 2, output_filename = \"testout.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Can also load it back\n",
    "# results = load_result(\"snapshot_testout.pickle\")\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A silly attack that just sends massive weights all of magnitude 10000\n",
    "def sample_attack(device):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights.\n",
    "    '''\n",
    "    weights = device[\"net\"].state_dict().copy()\n",
    "    \n",
    "    for w in weights.keys():\n",
    "        weights[w] = torch.full(weights[w].size(), 10000)\n",
    "    \n",
    "    device['net'].load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation function that squashes any weights with magnitude over 1000 (except on the first device but whatever)\n",
    "def super_smart_aggregation(devices):\n",
    "    '''\n",
    "    devices: a list of devices generated by create_devices\n",
    "    Returns an the average of the weights, excluding huge updates.\n",
    "    '''\n",
    "    state_dicts = [device['net'].state_dict() for device in devices]\n",
    "    # initialize w_avg to tensors from device 0\n",
    "    w_avg = copy.deepcopy(state_dicts[0])\n",
    "    for k in w_avg.keys():\n",
    "        w_avg[k] = w_avg[k].type(torch.float32)\n",
    "\n",
    "    # for each model param\n",
    "    for k in w_avg.keys():\n",
    "        # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "        for i in range(1, len(devices)):\n",
    "            if (torch.max(state_dicts[i][k].type(torch.float32)) <= 1000): \n",
    "                w_avg[k] += (state_dicts[i][k].type(torch.float32))\n",
    "        # compute average\n",
    "        w_avg[k] /= float(len(devices))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how this works\n",
    "\n",
    "# Here, we carry out the attack but use the ordinary average\n",
    "# results_straight_avg = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "#                                          rounds = 3,              \n",
    "#                                          local_epochs = 1,                             \n",
    "#                                          num_devices = 10,         \n",
    "#                                          device_pct = 0.2,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evil_round = 1,        \n",
    "#                                          attacker_strategy = sample_attack,  \n",
    "#                                          evil_device_id = 2,     \n",
    "#                                          evaluate_attack = None, \n",
    "#                                          output_filename = \"baseline_trivial_attack.pickle\")  \n",
    "# Observe that it absolutely destroys our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we do it again but with the aggregation that rejects huge updates\n",
    "# results_reject_huge = run_federated_test(agg_fn = super_smart_aggregation,                    \n",
    "#                                          rounds = 3,              \n",
    "#                                          local_epochs = 1,                             \n",
    "#                                          num_devices = 10,         \n",
    "#                                          device_pct = 0.2,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evil_round = 1,        \n",
    "#                                          attacker_strategy = sample_attack,  \n",
    "#                                          evil_device_id = 2,     \n",
    "#                                          evaluate_attack = None,\n",
    "#                                          output_filename = \"simple_attack_and_defense.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here, we compute a real baseline over 100 epochs and typical averaging. Good starting point for future \n",
    "# tests; can resume from this snapshot.\n",
    "# baseline = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "#                                          rounds = 100,              \n",
    "#                                          local_epochs = 4,                             \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.2,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evaluate_attack = None, \n",
    "#                                          output_filename = \"baseline.pickle\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline = load_result(\"baseline.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, now can get things like\n",
    "# baseline.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can resume from a snapshot:\n",
    "# NOTE : keep all the basic params (local epochs, num devices, net, etc) the same\n",
    "# when resuming from a snapshot \n",
    "# baseline2 = run_federated_test(agg_fn = (lambda x : average_weights(x)[0]),                    \n",
    "#                                          rounds = 101,              \n",
    "#                                          local_epochs = 4,                             \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.2,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evaluate_attack = None, \n",
    "#                                          output_filename = \"baseline2.pickle\", \n",
    "#                                          resume_from_snap = baseline )  # we pass in a results object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline2_loaded = load_result(\"baseline2.pickle\")\n",
    "# baseline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can move on to a real defense. \n",
    "\n",
    "# We create factory functions that let us parametrize a double sigmoid using threshholds\n",
    "\n",
    "def torch_scaled_sigmoid_factory(a, c = 0.005, left = False):\n",
    "    \n",
    "    def f(x):\n",
    "        scaled = torch.sub(x, a)\n",
    "\n",
    "        divisor = c if left else -1*c\n",
    "\n",
    "        scaled = torch.div(scaled, divisor)\n",
    "\n",
    "        return torch.sigmoid(scaled)\n",
    "    return f\n",
    "\n",
    "def torch_double_sigmoid_factory(a,b,c = 0.005):\n",
    "    left = torch_scaled_sigmoid_factory(a, left = True)\n",
    "    right = torch_scaled_sigmoid_factory(b, left = False) \n",
    "    def f(x):\n",
    "        return (left(x)*right(x))\n",
    "    return f\n",
    "\n",
    "my_sigmoid = torch_double_sigmoid_factory(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_double_sigmoid_factory(-1,6)(torch.tensor(list(range(-5,10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure it works\n",
    "# torch_double_sigmoid_factory(-2,2)(torch.Tensor([-5,-4,-3,-2,-1.5,-1.4,1.3,-1,0,1,1.5,1.6,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a sigmoid defense,\n",
    "#     keys_to_range_mapping maps each layer name in the state dict to the sigmoid bounds for processing it\n",
    "#     All other layers just get identity function-ed\n",
    "# watcher = {}\n",
    "def make_sigmoid_defense(keys_to_range_mapping, stickiness = 0):\n",
    "    \n",
    "    sigmoid_dict = defaultdict(lambda : (lambda x: 1))\n",
    "    \n",
    "    for k,(a,b) in keys_to_range_mapping.items():\n",
    "        sigmoid_dict[k] = torch_double_sigmoid_factory(a,b)\n",
    "    \n",
    "    def f(devices, previous_round_weights):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights preprocessed by sigmoid\n",
    "        '''\n",
    "        state_dicts = [device['net'].state_dict() for device in devices]\n",
    "        # initialize w_avg to tensors from device 0\n",
    "        w_avg = copy.deepcopy(state_dicts[0])\n",
    "        for k in w_avg.keys():\n",
    "            sig = sigmoid_dict[k]\n",
    "#             prev_round_x = previous_round_weights[k].type(torch.float32) if  previous_round_weights is not None else x\n",
    "            w_avg[k] = w_avg[k].type(torch.float32)\n",
    "#             for i in range(stickiness):\n",
    "#                 w_avg[k] += x              \n",
    "\n",
    "        # for each model param\n",
    "        for k in w_avg.keys():\n",
    "            sig = sigmoid_dict[k]\n",
    "            # for each remaining device i, add tensor state_dicts[i][k] to w_avg[k]\n",
    "            for i in range(1, len(devices)):\n",
    "                new_weight = (state_dicts[i][k].type(torch.float32))\n",
    "                prev_round_x = previous_round_weights[k].type(torch.float32)  if (previous_round_weights is not None) else new_weight\n",
    "#                 if i == 1:\n",
    "#                     watcher[k] = (new_weight - prev_round_x)\n",
    "#                 print((())))\n",
    "#                 print((len(new_weight)))\n",
    "\n",
    "\n",
    "                w_avg[k] += new_weight*(sig(torch.sub(new_weight, prev_round_x)))\n",
    "            # compute average\n",
    "            w_avg[k] /= float(len(devices) + stickiness)\n",
    "        return w_avg\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_70486/611164091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# These are the actual layer weights that we care about\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_weight_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.0.weight'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline' is not defined"
     ]
    }
   ],
   "source": [
    "# These are the actual layer weights that we care about\n",
    "[x for x in baseline.avg_weight_history[-1].keys() if '.0.weight' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets set up a potential sigmoid defense\n",
    "# empirical_cutoffs = {\n",
    "#     'model.0.0.weight' : (-2,2),\n",
    "#     'model.1.0.weight' : (-2,2),\n",
    "#     'model.2.0.weight' : (-.7,.7),\n",
    "#     'model.3.0.weight' : (-.6,.6),\n",
    "#     'model.4.0.weight' : (-0.7,0.7),\n",
    "#     'model.5.0.weight' : (-0.5,0.5),\n",
    "#     'model.6.0.weight' : (-0.3,0.3),\n",
    "#     'model.7.0.weight' : (-0.2,0.2),\n",
    "#     'model.8.0.weight' : (-0.2,0.2),\n",
    "# }\n",
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-0.3,0.3),\n",
    "    'model.1.0.weight' : (-0.3,0.3),\n",
    "    'model.2.0.weight' : (-0.3,0.3),\n",
    "    'model.3.0.weight' : (-0.3,0.3),\n",
    "    'model.4.0.weight' : (-0.3,0.3),\n",
    "    'model.5.0.weight' : (-0.3,0.3),\n",
    "    'model.6.0.weight' : (-0.3,0.3),\n",
    "    'model.7.0.weight' : (-0.1,0.1),\n",
    "    'model.8.0.weight' : (-0.1,0.1),\n",
    "}\n",
    "sigmoid_aggregation = make_sigmoid_defense(empirical_cutoffs, stickiness=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid_resumption = run_federated_test(agg_fn = sigmoid_accuracy,                    \n",
    "#                                          rounds = 110,              \n",
    "#                                          local_epochs = 1,                             \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.05,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evaluate_attack = None, \n",
    "#                                          output_filename = None,\n",
    "#                                          snapshot = False,\n",
    "#                                          resume_from_snap = baseline )  # we pass in a results object here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets make an attack\n",
    "# Makes an attack that puts noise in all of target layers. \n",
    "# Noise is uniform over a,b\n",
    "def noise_attack_factory(target_layers, a,b):\n",
    "    def attack(device):\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights.\n",
    "        '''\n",
    "        weights = device[\"net\"].state_dict().copy()\n",
    "\n",
    "        for w in weights.keys():\n",
    "            if w in target_layers:\n",
    "                weights[w] = (a - b) * torch.rand(weights[w].size()) + b\n",
    "\n",
    "        device['net'].load_state_dict(weights)\n",
    "    return attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We attack these layers with weights ranging from -4,4\n",
    "target_layers = ['model.0.0.weight',\n",
    " 'model.1.0.weight',\n",
    " 'model.2.0.weight',\n",
    " 'model.3.0.weight',\n",
    " 'model.4.0.weight',\n",
    " 'model.5.0.weight',\n",
    " 'model.6.0.weight',\n",
    " 'model.7.0.weight',\n",
    " 'model.8.0.weight']\n",
    "big_noise_attack = noise_attack_factory(target_layers,-3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll resume from our baseline model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline = load_result(\"baseline.pickle\")\n",
    "# determine_cutoffs = run_federated_test( agg_fn = sigmoid_aggregation,                    \n",
    "#                                          rounds = 101, # go 30 pounds past where the baseline left off              \n",
    "#                                          local_epochs = 0,        # all else the same                     \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.05,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evil_round = None, # attack after 10 rounds more training with the new acc fn        \n",
    "#                                          attacker_strategy = None, # device 2 will carry out big noise attack  \n",
    "#                                          evil_device_id = None,     \n",
    "#                                          evaluate_attack = None, # we will evaluate manually afterwards \n",
    "#                                          output_filename = None,\n",
    "#                                          resume_from_snap = baseline, #pick up where baseline left off  \n",
    "#                                          snapshot = False ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_sigmoid_on_cutoff(a,c=0.005, left = False):\n",
    "#     def f(x):\n",
    "#         if(not left and x > (a + 2)):\n",
    "#             return 0\n",
    "#         if(left and x < (a - 2)):\n",
    "#             return 0\n",
    "#         exponent = (-(x-a))/(c if left else -1*c)\n",
    "#         return 1/(1 + (math.e**(exponent)))\n",
    "#     return f \n",
    "\n",
    "# def make_double_sigmoid(a,b):\n",
    "#     left = make_sigmoid_on_cutoff(a, left = True)\n",
    "#     right = make_sigmoid_on_cutoff(b, left = False)\n",
    "#     def f(x):\n",
    "#         return left(x)*right(x)\n",
    "#     return f\n",
    "# baseline2.devices[0]['net']\n",
    "\n",
    "def make_test_device(trainset):\n",
    "    data_idxs = iid_sampler(trainset, 1, .1)\n",
    "    return create_device(ConvNet().to(mps), 0, trainset, data_idxs[0]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_noise_attack(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device['net'].state_dict()['model.0.0.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = [x[1] for x in trainset if x[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_classes_dataset(s, a, b):\n",
    "    s = list(s)\n",
    "    for i in range(len(s)):\n",
    "        if s[i][1] == a:\n",
    "            s[i] = s[i][0],b\n",
    "        elif s[i][1] == b:\n",
    "            s[i] = s[i][0],a \n",
    "    \n",
    "    return s\n",
    "\n",
    "def limit_test_set(s, labels):\n",
    "    s = list(s)\n",
    "    s = [s for s in s if s[1] in labels]\n",
    "    return s\n",
    "\n",
    "\n",
    "# Make switch classes attack\n",
    "def switch_classes_attack(a,b, train_epochs):\n",
    "    transform_train = transforms.Compose([                                   \n",
    "        transforms.RandomCrop(32, padding=4),                                       \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    evil_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                            download=True,\n",
    "                                            transform=transform_train)\n",
    "    print(len(evil_trainset))\n",
    "\n",
    "\n",
    "#     evil_trainset = limit_test_set(evil_trainset, [a,b])\n",
    "    evil_trainset = swap_classes_dataset(evil_trainset, a,b)\n",
    "    print(len(evil_trainset))\n",
    "\n",
    "\n",
    "    print(\"swapping and limiting\\n\")\n",
    "    \n",
    "    # Load testing data\n",
    "    transform_test = transforms.Compose([                                           \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    evil_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                           download=True,\n",
    "                                           transform=transform_test)\n",
    "    evil_testset = limit_test_set(evil_testset, [a,b])\n",
    "\n",
    "    \n",
    "    evil_testset = swap_classes_dataset(evil_testset, a,b)\n",
    "    \n",
    "    evil_testloader = torch.utils.data.DataLoader(evil_testset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    attack_idx = iid_sampler(evil_trainset, 1, 1)[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    evil_device_trainset = DatasetSplit(evil_trainset, attack_idx)\n",
    "    evil_device_trainloader = torch.utils.data.DataLoader(evil_device_trainset,\n",
    "                                                     batch_size=128,\n",
    "                                                     shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    logged_state_dict = None\n",
    "\n",
    "    class attack_class:\n",
    "        def __init__(self):\n",
    "            self.logged_state_dict = None\n",
    "        '''\n",
    "        devices: a list of devices generated by create_devices\n",
    "        Returns an the average of the weights.\n",
    "        '''\n",
    "        def __call__(self, device):\n",
    "            device['dataloader'] = evil_device_trainloader\n",
    "\n",
    "            if self.logged_state_dict is not None:\n",
    "                print(\"Using memoized attack\\n\")\n",
    "                device['net'].load_state_dict(self.logged_state_dict)\n",
    "            else:\n",
    "                for local_epoch in range(train_epochs):\n",
    "                    train(local_epoch, device, criterion)\n",
    "                    test(0, device, criterion,testloader =  evil_testloader)\n",
    "\n",
    "\n",
    "            print(\"Confirm the attack worked. (This should be high)\")  \n",
    "            test(0, device, criterion,testloader =  evil_testloader)\n",
    "\n",
    "\n",
    "            self.logged_state_dict = copy.deepcopy(device['net'].state_dict())\n",
    "\n",
    "        \n",
    "\n",
    "    return attack_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n",
      "50000\n",
      "swapping and limiting\n",
      "\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Now let's make an instance of this attack that performs 25 epochs of training on the evil device with the misclassified data.\n",
    "# Specifically, we switch cars and airplanes\n",
    "sca_01 = switch_classes_attack(0,1,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# baseline = load_result(\"baseline.pickle\")\n",
    "# switch_classes_no_defense = run_federated_test(                    \n",
    "#                                          rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "#                                          local_epochs = 4,        # all else the same                     \n",
    "#                                          num_devices = 50,         \n",
    "#                                          device_pct = 0.2,          \n",
    "#                                          data_pct = 0.1,           \n",
    "#                                          net = ConvNet().to(mps),  \n",
    "#                                          evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "#                                          attacker_strategy = sca2, # device 2 will carry out big noise attack  \n",
    "#                                          evil_device_id = 2,     \n",
    "#                                          evaluate_attack = None, # we will evaluate manually afterwards \n",
    "#                                          output_filename = \"switch_classes_no_defense.pickle\",\n",
    "#                                          resume_from_snap = baseline, #pick up where baseline left off  \n",
    "#                                          snapshot = True ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch_classes_no_defense = load_result(\"switch_classes_no_defense.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch_classes_no_defense.test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def test(epoch, device, criterion, testloader = testloader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(0, test_device, nn.CrossEntropyLoss(), testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_test_set(s, labels):\n",
    "    s = list(s)\n",
    "    s = [s for s in s if s[1] in labels]\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def make_testloader_subset(labels, a, b):\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "    \n",
    "    restricted_test_set = limit_test_set(testset, labels)\n",
    "#     print(len(restricted_test_set))\n",
    "    restricted_test_set = swap_classes_dataset(restricted_test_set, a, b)\n",
    "    \n",
    "    restricted_testloader = torch.utils.data.DataLoader(restricted_test_set, batch_size=128, shuffle=False)\n",
    "    \n",
    "    return restricted_testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x[1] for x in checker_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(0, test_device, nn.CrossEntropyLoss(), checker_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_70486/1373342214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mall_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax_magnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_avg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmax_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweight_history_to_layer_max_magnitude_and_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_weight_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_batch_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmax_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_history\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mby_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mk/ydf2207j325_0l9ph1swkbzc0000gq/T/ipykernel_70486/1373342214.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mall_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax_magnitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_avg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmax_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweight_history_to_layer_max_magnitude_and_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_weight_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_batch_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmax_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_history\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mby_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'baseline' is not defined"
     ]
    }
   ],
   "source": [
    "def weight_history_to_layer_max_magnitude_and_means(w_avg, include_batch_norm = True):\n",
    "    weight_keys = [x for x in w_avg.keys() if \".0.weight\" in x]\n",
    "    if include_batch_norm:\n",
    "        weight_keys += [x for x in w_avg.keys() if \".1.weight\" in x]\n",
    "    def max_magnitude(t):\n",
    "        return torch.max(torch.abs(t))\n",
    "    def mean_magnitude(t):\n",
    "        return torch.mean(torch.abs(t))\n",
    "    all_means = [mean_magnitude(w_avg[w]) for w in weight_keys]\n",
    "    all_max = [max_magnitude(w_avg[w]) for w in weight_keys]\n",
    "    return all_means, all_max\n",
    "max_history = [weight_history_to_layer_max_magnitude_and_means(baseline.avg_weight_history[t], include_batch_norm = False) for t in range(100)]\n",
    "max_history = [x[1] for x in max_history]\n",
    "by_layer = [[max_history[k][l].cpu() for k in range(len(max_history))] for l in range(len(max_history[0]))]\n",
    "\n",
    "[plt.plot([x for x in range(100)],by_layer[l],  '-k', color='blue') for l in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_history = [weight_history_to_layer_max_magnitude_and_means(watcher, include_batch_norm = False) for t in range(1)]\n",
    "# max_history = [(x[1]) for x in max_history]\n",
    "# by_layer = [[max_history[k][l].cpu() for k in range(len(max_history))] for l in range(len(max_history[0]))]\n",
    "\n",
    "# [plt.plot([x for x in range(1)],by_layer[l],  '-k', color=['blue','red','green','blue', 'red', 'green', 'blue', 'red', 'green'][l]) for l in range(5,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 34/Epoch 3) Train Loss: 0.353 | Train Acc: 88.640 | Test Loss: 0.403 | Test Acc: 86.770\n",
      "\n",
      "Diff: 0.21199172735214233\n",
      "\n",
      "Round time: 338.0134241580963 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  101\n",
      "(Device 49/Epoch 3) Train Loss: 0.347 | Train Acc: 88.020 | Test Loss: 0.381 | Test Acc: 87.730\n",
      "\n",
      "Diff: 2.324102933926042e-05\n",
      "\n",
      "Round time: 662.2266662120819 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  102\n",
      "(Device 2/Epoch 3) Train Loss: 0.240 | Train Acc: 91.2600 | Test Loss: 0.387 | Test Acc: 87.700\n",
      "\n",
      "Diff: 7.141364767448977e-05\n",
      "\n",
      "Round time: 984.8954751491547 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  103\n",
      "(Device 3/Epoch 3) Train Loss: 0.303 | Train Acc: 89.3000 | Test Loss: 0.378 | Test Acc: 87.960\n",
      "\n",
      "Diff: 2.7878273613168858e-05\n",
      "\n",
      "Round time: 1270.7090492248535 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  104\n",
      "(Device 16/Epoch 3) Train Loss: 0.387 | Train Acc: 87.040 | Test Loss: 0.386 | Test Acc: 87.480\n",
      "\n",
      "Diff: 0.00034004563349299133\n",
      "\n",
      "Round time: 1559.5193321704865 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  105\n",
      "(Device 33/Epoch 3) Train Loss: 0.210 | Train Acc: 92.800 | Test Loss: 0.379 | Test Acc: 88.100\n",
      "\n",
      "Diff: 2.031960138992872e-05\n",
      "\n",
      "Round time: 1745.902722120285 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  106\n",
      "(Device 49/Epoch 3) Train Loss: 0.208 | Train Acc: 93.200 | Test Loss: 0.363 | Test Acc: 88.450\n",
      "\n",
      "Diff: 0.0014024514239281416\n",
      "\n",
      "Round time: 1924.789852142334 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  107\n",
      "(Device 32/Epoch 3) Train Loss: 0.274 | Train Acc: 89.960 | Test Loss: 0.358 | Test Acc: 88.570\n",
      "\n",
      "Diff: 0.0001915216853376478\n",
      "\n",
      "Round time: 2103.1979172229767 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  108\n",
      "(Device 12/Epoch 3) Train Loss: 0.261 | Train Acc: 90.920 | Test Loss: 0.376 | Test Acc: 88.280\n",
      "\n",
      "Diff: 9.961040632333606e-05\n",
      "\n",
      "Round time: 2283.013152360916 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  109\n",
      "(Device 2/Epoch 3) Train Loss: 0.190 | Train Acc: 93.7200 | Test Loss: 0.378 | Test Acc: 88.440\n",
      "\n",
      "Diff: 6.921531894477084e-05\n",
      "\n",
      "Round time: 2461.277685403824 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  110\n",
      "(Device 42/Epoch 3) Train Loss: 0.404 | Train Acc: 86.580Attacking!\n",
      "\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.569 | Test Acc: 81.700\n",
      "\n",
      "Diff: 411.1285705566406\n",
      "\n",
      "Round time: 2639.307859182358 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  111\n",
      "(Device 23/Epoch 3) Train Loss: 0.143 | Train Acc: 95.000 | Test Loss: 0.364 | Test Acc: 88.570\n",
      "\n",
      "Diff: 0.01019325852394104\n",
      "\n",
      "Round time: 2817.9722332954407 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  112\n",
      "(Device 49/Epoch 3) Train Loss: 0.424 | Train Acc: 85.380 | Test Loss: 0.394 | Test Acc: 87.780\n",
      "\n",
      "Diff: 0.012016956694424152\n",
      "\n",
      "Round time: 3001.2249562740326 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  113\n",
      "(Device 35/Epoch 3) Train Loss: 0.216 | Train Acc: 92.740 | Test Loss: 0.361 | Test Acc: 88.720\n",
      "\n",
      "Diff: 4.299101419746876e-05\n",
      "\n",
      "Round time: 3183.949806213379 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  114\n",
      "(Device 13/Epoch 3) Train Loss: 0.281 | Train Acc: 90.020 | Test Loss: 0.354 | Test Acc: 89.130\n",
      "\n",
      "Diff: 0.0002179448347305879\n",
      "\n",
      "Round time: 3370.5046911239624 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  115\n",
      "(Device 5/Epoch 3) Train Loss: 0.131 | Train Acc: 95.3400 | Test Loss: 0.348 | Test Acc: 89.280\n",
      "\n",
      "Diff: 0.00028175555053167045\n",
      "\n",
      "Round time: 3557.6465661525726 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  116\n",
      "(Device 24/Epoch 3) Train Loss: 0.255 | Train Acc: 90.940 | Test Loss: 0.361 | Test Acc: 88.830\n",
      "\n",
      "Diff: 0.00160264503210783\n",
      "\n",
      "Round time: 3740.7970910072327 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  117\n",
      "(Device 19/Epoch 3) Train Loss: 0.239 | Train Acc: 91.840 | Test Loss: 0.349 | Test Acc: 89.140\n",
      "\n",
      "Diff: 0.007501357700675726\n",
      "\n",
      "Round time: 3922.2070519924164 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  118\n",
      "(Device 19/Epoch 3) Train Loss: 0.177 | Train Acc: 94.700 | Test Loss: 0.386 | Test Acc: 88.380\n",
      "\n",
      "Diff: 0.035732515156269073\n",
      "\n",
      "Round time: 4109.553030252457 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  119\n",
      "(Device 39/Epoch 3) Train Loss: 0.226 | Train Acc: 92.220 | Test Loss: 0.398 | Test Acc: 88.050\n",
      "\n",
      "Diff: 0.003836588002741337\n",
      "\n",
      "Round time: 4295.968320131302 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  120\n",
      "(Device 6/Epoch 3) Train Loss: 0.132 | Train Acc: 95.38000 | Test Loss: 0.378 | Test Acc: 88.970\n",
      "\n",
      "Diff: 0.06036344915628433\n",
      "\n",
      "Round time: 4487.158486127853 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  121\n",
      "(Device 36/Epoch 3) Train Loss: 0.176 | Train Acc: 94.740 | Test Loss: 0.347 | Test Acc: 89.510\n",
      "\n",
      "Diff: 0.0031616499181836843\n",
      "\n",
      "Round time: 4674.330126047134 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  122\n",
      "(Device 47/Epoch 3) Train Loss: 0.143 | Train Acc: 94.820 | Test Loss: 0.391 | Test Acc: 88.670\n",
      "\n",
      "Diff: 0.0014756062300875783\n",
      "\n",
      "Round time: 4860.598829030991 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  123\n",
      "(Device 28/Epoch 3) Train Loss: 0.127 | Train Acc: 96.260 | Test Loss: 0.345 | Test Acc: 89.610\n",
      "\n",
      "Diff: 0.0018236682517454028\n",
      "\n",
      "Round time: 5046.52060008049 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  124\n",
      "(Device 24/Epoch 3) Train Loss: 0.255 | Train Acc: 91.920 | Test Loss: 0.354 | Test Acc: 89.580\n",
      "\n",
      "Diff: 0.0036055194213986397\n",
      "\n",
      "Round time: 5232.379231214523 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  125\n",
      "(Device 15/Epoch 3) Train Loss: 0.094 | Train Acc: 96.7800 | Test Loss: 0.340 | Test Acc: 90.020\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5418.352782011032 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  126\n",
      "(Device 0/Epoch 3) Train Loss: 0.074 | Train Acc: 97.7200 | Test Loss: 0.338 | Test Acc: 90.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5604.615499019623 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  127\n",
      "(Device 46/Epoch 3) Train Loss: 0.089 | Train Acc: 97.0200 | Test Loss: 0.336 | Test Acc: 90.310\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5792.045459270477 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  128\n",
      "(Device 23/Epoch 3) Train Loss: 0.077 | Train Acc: 97.780 | Test Loss: 0.340 | Test Acc: 90.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5982.049333333969 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  129\n",
      "(Device 39/Epoch 3) Train Loss: 0.062 | Train Acc: 98.2800 | Test Loss: 0.342 | Test Acc: 90.150\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6169.599514245987 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 6172.737816095352 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = load_result(\"baseline.pickle\")\n",
    "sigmoid_against_noise_attack = run_federated_test(agg_fn = sigmoid_aggregation,                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = big_noise_attack, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"sigmoid_against_noise_attack_final.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "swapping\n",
      "\n",
      "Files already downloaded and verified\n",
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 44/Epoch 3) Train Loss: 0.285 | Train Acc: 90.480Attacking!\n",
      "\n",
      "(Device 2/Epoch 24) Train Loss: 0.014 | Train Acc: 99.5660Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.963 | Test Acc: 85.330\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.411 | Test Acc: 86.850\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1250.4205939769745 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  101\n",
      "(Device 15/Epoch 3) Train Loss: 0.311 | Train Acc: 89.640Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.255 | Test Acc: 78.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.404 | Test Acc: 86.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1435.8249740600586 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  102\n",
      "(Device 39/Epoch 3) Train Loss: 0.269 | Train Acc: 90.700Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.810 | Test Acc: 78.590\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.395 | Test Acc: 87.440\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1619.1456139087677 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  103\n",
      "(Device 26/Epoch 3) Train Loss: 0.269 | Train Acc: 90.600Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.669 | Test Acc: 82.810\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.433 | Test Acc: 86.750\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 1956.4879717826843 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  104\n",
      "(Device 28/Epoch 3) Train Loss: 0.262 | Train Acc: 90.780Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.872 | Test Acc: 79.930\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.454 | Test Acc: 86.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2142.7575058937073 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  105\n",
      "(Device 49/Epoch 3) Train Loss: 0.208 | Train Acc: 92.940Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.681 | Test Acc: 84.050\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.457 | Test Acc: 86.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2479.3429250717163 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  106\n",
      "(Device 33/Epoch 3) Train Loss: 0.357 | Train Acc: 88.180Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.893 | Test Acc: 79.500\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.391 | Test Acc: 87.650\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2666.9391298294067 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  107\n",
      "(Device 43/Epoch 3) Train Loss: 0.242 | Train Acc: 91.540Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.757 | Test Acc: 78.930\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.380 | Test Acc: 88.490\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 2851.377774000168 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  108\n",
      "(Device 7/Epoch 3) Train Loss: 0.245 | Train Acc: 91.6000Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.944 | Test Acc: 79.410\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.366 | Test Acc: 88.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3037.1944308280945 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  109\n",
      "(Device 39/Epoch 3) Train Loss: 0.277 | Train Acc: 90.760Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 2.009 | Test Acc: 79.570\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.359 | Test Acc: 88.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3224.3572149276733 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  110\n",
      "(Device 29/Epoch 3) Train Loss: 0.188 | Train Acc: 93.160Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.961 | Test Acc: 79.520\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.374 | Test Acc: 88.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3419.0711059570312 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  111\n",
      "(Device 36/Epoch 3) Train Loss: 0.257 | Train Acc: 91.040 | Test Loss: 0.355 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3606.8760719299316 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  112\n",
      "(Device 4/Epoch 3) Train Loss: 0.280 | Train Acc: 90.54000 | Test Loss: 0.361 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3792.78076171875 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  113\n",
      "(Device 29/Epoch 3) Train Loss: 0.178 | Train Acc: 93.660 | Test Loss: 0.357 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 3987.803965806961 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  114\n",
      "(Device 44/Epoch 3) Train Loss: 0.172 | Train Acc: 94.860 | Test Loss: 0.370 | Test Acc: 89.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4351.923414945602 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  115\n",
      "(Device 39/Epoch 3) Train Loss: 0.117 | Train Acc: 96.060 | Test Loss: 0.359 | Test Acc: 89.340\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4539.46182179451 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  116\n",
      "(Device 9/Epoch 3) Train Loss: 0.147 | Train Acc: 95.0600 | Test Loss: 0.351 | Test Acc: 89.390\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4729.595947980881 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  117\n",
      "(Device 31/Epoch 3) Train Loss: 0.133 | Train Acc: 96.1200 | Test Loss: 0.356 | Test Acc: 89.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 4929.9669868946075 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  118\n",
      "(Device 10/Epoch 3) Train Loss: 0.194 | Train Acc: 93.400 | Test Loss: 0.366 | Test Acc: 89.050\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5118.179821968079 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  119\n",
      "(Device 25/Epoch 3) Train Loss: 0.228 | Train Acc: 92.120 | Test Loss: 0.351 | Test Acc: 89.900\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5306.937247037888 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  120\n",
      "(Device 17/Epoch 3) Train Loss: 0.160 | Train Acc: 95.120 | Test Loss: 0.366 | Test Acc: 89.460\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5502.8072509765625 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  121\n",
      "(Device 7/Epoch 3) Train Loss: 0.115 | Train Acc: 95.8200 | Test Loss: 0.345 | Test Acc: 89.970\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 5692.020018815994 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  122\n",
      "(Device 9/Epoch 3) Train Loss: 0.223 | Train Acc: 92.6000 | Test Loss: 0.361 | Test Acc: 89.340\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6040.297296762466 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  123\n",
      "(Device 42/Epoch 3) Train Loss: 0.085 | Train Acc: 97.3400 | Test Loss: 0.382 | Test Acc: 89.360\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6230.666080951691 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  124\n",
      "(Device 18/Epoch 3) Train Loss: 0.162 | Train Acc: 94.6400 | Test Loss: 0.349 | Test Acc: 89.790\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6418.991494894028 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  125\n",
      "(Device 1/Epoch 3) Train Loss: 0.096 | Train Acc: 96.9400 | Test Loss: 0.347 | Test Acc: 90.320\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6606.251834869385 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  126\n",
      "(Device 17/Epoch 3) Train Loss: 0.065 | Train Acc: 97.9600 | Test Loss: 0.349 | Test Acc: 90.280\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 6948.046301841736 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  127\n",
      "(Device 19/Epoch 3) Train Loss: 0.073 | Train Acc: 97.740 | Test Loss: 0.353 | Test Acc: 90.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7138.234809875488 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  128\n",
      "(Device 34/Epoch 3) Train Loss: 0.074 | Train Acc: 97.8000 | Test Loss: 0.355 | Test Acc: 90.170\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7327.037197113037 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  129\n",
      "(Device 18/Epoch 3) Train Loss: 0.058 | Train Acc: 98.4400 | Test Loss: 0.357 | Test Acc: 90.190\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7516.31902384758 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 7520.757854938507 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sca_zero_one = switch_classes_attack(0,1,25)\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_no_defense = run_federated_test(                    \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_no_defense_multiple_final.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         multiple_attack_rounds = [100,101,102,103,104,105,106,107,108,109]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "swapping\n",
      "\n",
      "Files already downloaded and verified\n",
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 7/Epoch 3) Train Loss: 0.354 | Train Acc: 88.2400Attacking!\n",
      "\n",
      "(Device 2/Epoch 24) Train Loss: 0.019 | Train Acc: 99.3360Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.802 | Test Acc: 86.410\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.539 | Test Acc: 82.430\n",
      "\n",
      "Diff: 10.120282173156738\n",
      "\n",
      "Round time: 1260.700845003128 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  101\n",
      "(Device 10/Epoch 3) Train Loss: 0.304 | Train Acc: 90.080Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.646 | Test Acc: 84.470\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.394 | Test Acc: 88.150\n",
      "\n",
      "Diff: 1.0120688676834106\n",
      "\n",
      "Round time: 1605.5601961612701 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  102\n",
      "(Device 9/Epoch 3) Train Loss: 0.250 | Train Acc: 91.8800Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.160 | Test Acc: 79.510\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.369 | Test Acc: 88.710\n",
      "\n",
      "Diff: 0.000545351707842201\n",
      "\n",
      "Round time: 1804.8574283123016 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  103\n",
      "(Device 33/Epoch 3) Train Loss: 0.320 | Train Acc: 89.140Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.775 | Test Acc: 79.150\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.371 | Test Acc: 88.350\n",
      "\n",
      "Diff: 0.2273254096508026\n",
      "\n",
      "Round time: 1994.2636981010437 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  104\n",
      "(Device 34/Epoch 3) Train Loss: 0.249 | Train Acc: 91.060Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 2.010 | Test Acc: 78.710\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.361 | Test Acc: 88.480\n",
      "\n",
      "Diff: 0.0004312241217121482\n",
      "\n",
      "Round time: 2182.455514192581 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  105\n",
      "(Device 20/Epoch 3) Train Loss: 0.362 | Train Acc: 88.500Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.828 | Test Acc: 79.160\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.365 | Test Acc: 88.620\n",
      "\n",
      "Diff: 0.0171380452811718\n",
      "\n",
      "Round time: 2383.003970146179 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  106\n",
      "(Device 42/Epoch 3) Train Loss: 0.172 | Train Acc: 94.240Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.648 | Test Acc: 84.760\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.482 | Test Acc: 85.970\n",
      "\n",
      "Diff: 0.1966441571712494\n",
      "\n",
      "Round time: 2731.1573972702026 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  107\n",
      "(Device 32/Epoch 3) Train Loss: 0.162 | Train Acc: 94.440Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.760 | Test Acc: 80.810\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.394 | Test Acc: 88.290\n",
      "\n",
      "Diff: 0.011711034923791885\n",
      "\n",
      "Round time: 2932.9708671569824 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  108\n",
      "(Device 35/Epoch 3) Train Loss: 0.233 | Train Acc: 91.920Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.720 | Test Acc: 79.780\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.390 | Test Acc: 88.160\n",
      "\n",
      "Diff: 0.00041811345727182925\n",
      "\n",
      "Round time: 3125.602886199951 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  109\n",
      "(Device 20/Epoch 3) Train Loss: 0.157 | Train Acc: 94.680Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.893 | Test Acc: 79.880\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.377 | Test Acc: 88.420\n",
      "\n",
      "Diff: 0.09476998448371887\n",
      "\n",
      "Round time: 3326.7698023319244 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  110\n",
      "(Device 40/Epoch 3) Train Loss: 0.179 | Train Acc: 94.260Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 1.971 | Test Acc: 79.760\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.361 | Test Acc: 88.740\n",
      "\n",
      "Diff: 0.00030925663304515183\n",
      "\n",
      "Round time: 3524.3542079925537 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  111\n",
      "(Device 31/Epoch 3) Train Loss: 0.210 | Train Acc: 92.840 | Test Loss: 0.363 | Test Acc: 89.040\n",
      "\n",
      "Diff: 0.009649098850786686\n",
      "\n",
      "Round time: 3734.484055995941 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  112\n",
      "(Device 19/Epoch 3) Train Loss: 0.179 | Train Acc: 94.4800 | Test Loss: 0.354 | Test Acc: 89.220\n",
      "\n",
      "Diff: 0.015375320799648762\n",
      "\n",
      "Round time: 3962.994415998459 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  113\n",
      "(Device 10/Epoch 3) Train Loss: 0.140 | Train Acc: 95.120 | Test Loss: 0.355 | Test Acc: 89.300\n",
      "\n",
      "Diff: 0.0003145982918795198\n",
      "\n",
      "Round time: 4179.241348981857 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  114\n",
      "(Device 35/Epoch 3) Train Loss: 0.299 | Train Acc: 89.4000 | Test Loss: 0.371 | Test Acc: 89.040\n",
      "\n",
      "Diff: 0.03675477206707001\n",
      "\n",
      "Round time: 4586.433605194092 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  115\n",
      "(Device 8/Epoch 3) Train Loss: 0.184 | Train Acc: 93.54000 | Test Loss: 0.352 | Test Acc: 89.510\n",
      "\n",
      "Diff: 0.005422140937298536\n",
      "\n",
      "Round time: 4800.521832227707 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  116\n",
      "(Device 49/Epoch 3) Train Loss: 0.261 | Train Acc: 91.340 | Test Loss: 0.371 | Test Acc: 89.130\n",
      "\n",
      "Diff: 0.005069469567388296\n",
      "\n",
      "Round time: 5021.807204008102 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  117\n",
      "(Device 23/Epoch 3) Train Loss: 0.229 | Train Acc: 92.220 | Test Loss: 0.348 | Test Acc: 89.850\n",
      "\n",
      "Diff: 0.0009505321504548192\n",
      "\n",
      "Round time: 5247.358416080475 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  118\n",
      "(Device 14/Epoch 3) Train Loss: 0.201 | Train Acc: 93.520 | Test Loss: 0.350 | Test Acc: 89.540\n",
      "\n",
      "Diff: 0.008713014423847198\n",
      "\n",
      "Round time: 5470.169590950012 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  119\n",
      "(Device 11/Epoch 3) Train Loss: 0.199 | Train Acc: 92.940 | Test Loss: 0.350 | Test Acc: 89.640\n",
      "\n",
      "Diff: 0.036510057747364044\n",
      "\n",
      "Round time: 5865.939716100693 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  120\n",
      "(Device 29/Epoch 3) Train Loss: 0.274 | Train Acc: 90.9800 | Test Loss: 0.355 | Test Acc: 89.910\n",
      "\n",
      "Diff: 0.2861802577972412\n",
      "\n",
      "Round time: 6092.064857006073 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  121\n",
      "(Device 38/Epoch 3) Train Loss: 0.092 | Train Acc: 97.2200 | Test Loss: 0.352 | Test Acc: 90.010\n",
      "\n",
      "Diff: 0.0017421803204342723\n",
      "\n",
      "Round time: 6312.076562166214 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  122\n",
      "(Device 13/Epoch 3) Train Loss: 0.093 | Train Acc: 97.3600 | Test Loss: 0.363 | Test Acc: 89.570\n",
      "\n",
      "Diff: 0.03876407817006111\n",
      "\n",
      "Round time: 6530.912013053894 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  123\n",
      "(Device 23/Epoch 3) Train Loss: 0.179 | Train Acc: 94.280 | Test Loss: 0.350 | Test Acc: 90.160\n",
      "\n",
      "Diff: 0.03298230469226837\n",
      "\n",
      "Round time: 6758.608757972717 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  124\n",
      "(Device 29/Epoch 3) Train Loss: 0.077 | Train Acc: 97.380 | Test Loss: 0.363 | Test Acc: 89.800\n",
      "\n",
      "Diff: 0.05205897241830826\n",
      "\n",
      "Round time: 6983.1133069992065 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  125\n",
      "(Device 43/Epoch 3) Train Loss: 0.066 | Train Acc: 98.120 | Test Loss: 0.343 | Test Acc: 90.330\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7208.319562196732 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  126\n",
      "(Device 42/Epoch 3) Train Loss: 0.066 | Train Acc: 97.8400 | Test Loss: 0.342 | Test Acc: 90.470\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7426.290471076965 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  127\n",
      "(Device 20/Epoch 3) Train Loss: 0.060 | Train Acc: 98.3000 | Test Loss: 0.343 | Test Acc: 90.310\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 7622.184158086777 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  128\n",
      "(Device 13/Epoch 3) Train Loss: 0.066 | Train Acc: 98.120 | Test Loss: 0.346 | Test Acc: 90.440\n",
      "\n",
      "Diff: 1.1847208014614807e-07\n",
      "\n",
      "Round time: 7976.075016260147 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Round:  129\n",
      "(Device 38/Epoch 3) Train Loss: 0.096 | Train Acc: 97.8200 | Test Loss: 0.349 | Test Acc: 90.470\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 8180.8768701553345 \n",
      "\n",
      "Writing snapshot\n",
      "\n",
      "Total training time: 8186.628128051758 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sca_zero_one_2 = switch_classes_attack(0,1,25)\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_sigmoid_defense = run_federated_test(  agg_fn = sigmoid_aggregation, \n",
    "                                         rounds = 130, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 4,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.2,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one_2, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_sigmoid_defense_multiple_final.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = True, \n",
    "                                         multiple_attack_rounds = [100,101,102,103,104,105,106,107,108,109]) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.0.0.weight',\n",
       "              tensor([[[[ 7.4600e-01,  8.4820e-01,  4.7823e-01],\n",
       "                        [ 5.1573e-01,  1.0049e+00,  1.2566e+00],\n",
       "                        [-5.9251e-01, -3.1604e-01, -1.8427e-01]],\n",
       "              \n",
       "                       [[-5.3358e-01, -8.8284e-01, -5.9071e-01],\n",
       "                        [-1.0610e+00, -8.9894e-01,  4.1919e-01],\n",
       "                        [-1.0492e+00, -1.1256e+00, -3.9162e-01]],\n",
       "              \n",
       "                       [[ 1.2420e-01, -1.3487e-01,  8.3084e-04],\n",
       "                        [-1.8890e-01,  3.6324e-01,  1.1430e+00],\n",
       "                        [ 1.9642e-01,  6.0267e-01,  1.1399e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9969e-01,  3.3645e-01,  3.9868e-01],\n",
       "                        [-1.1443e+00, -3.7165e-01,  1.0054e+00],\n",
       "                        [-9.7183e-02, -2.2539e-02,  3.8900e-01]],\n",
       "              \n",
       "                       [[-4.9338e-01, -3.5729e-01,  2.5315e-01],\n",
       "                        [-1.0529e+00, -6.9401e-01,  9.6998e-01],\n",
       "                        [-1.2148e-01,  2.0725e-01,  6.5304e-01]],\n",
       "              \n",
       "                       [[-2.0915e-01, -1.8893e-01,  2.2833e-01],\n",
       "                        [-9.4088e-01, -3.4339e-01,  8.9907e-01],\n",
       "                        [-2.1204e-03,  3.0896e-01,  4.9953e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.1321e-01,  5.4262e-02,  1.8721e-01],\n",
       "                        [ 7.7245e-01, -4.2362e-01, -9.7571e-01],\n",
       "                        [ 5.8087e-01,  2.3426e-01, -4.4087e-01]],\n",
       "              \n",
       "                       [[ 4.8934e-01, -5.9026e-02,  1.6839e-03],\n",
       "                        [ 6.8533e-01, -4.8358e-01, -1.3530e+00],\n",
       "                        [ 5.4588e-01,  3.4553e-01, -5.9782e-01]],\n",
       "              \n",
       "                       [[ 8.5630e-02,  2.2176e-01,  5.3444e-01],\n",
       "                        [ 3.4456e-01, -4.1069e-01, -6.1995e-01],\n",
       "                        [ 2.2876e-01,  1.3409e-01, -5.0664e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.2215e-01, -6.2452e-01, -4.0303e-01],\n",
       "                        [-6.1050e-01, -7.5069e-01, -7.8922e-01],\n",
       "                        [-5.7783e-01, -9.8292e-01, -8.4384e-01]],\n",
       "              \n",
       "                       [[-1.5789e-01,  1.2242e-01, -2.9747e-01],\n",
       "                        [ 3.8840e-01,  6.6292e-01,  3.0778e-01],\n",
       "                        [ 6.8307e-02,  1.0262e-01,  2.9979e-02]],\n",
       "              \n",
       "                       [[ 1.3978e-01,  5.4648e-01,  5.9003e-02],\n",
       "                        [ 5.8368e-01,  1.4628e+00,  8.6412e-01],\n",
       "                        [ 3.7575e-01,  8.5173e-01,  4.0163e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.3140e-01,  6.5279e-01, -4.1369e-01],\n",
       "                        [-6.2593e-01, -1.2069e+00,  7.2996e-02],\n",
       "                        [ 3.0293e-01,  3.2063e-01,  8.2402e-01]],\n",
       "              \n",
       "                       [[ 8.4682e-01,  1.0004e+00,  1.0241e-01],\n",
       "                        [-8.5225e-01, -1.4023e+00,  4.8998e-02],\n",
       "                        [ 4.3616e-02, -3.0539e-01,  7.6269e-01]],\n",
       "              \n",
       "                       [[ 6.5153e-01,  6.1098e-01,  3.9530e-02],\n",
       "                        [-3.9860e-01, -1.0256e+00, -1.0201e-01],\n",
       "                        [-3.6981e-02, -5.1075e-01,  3.8620e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.8512e-01,  5.6056e-01,  1.9931e-01],\n",
       "                        [-7.1396e-01, -7.4205e-01,  7.5849e-02],\n",
       "                        [ 3.0334e-01,  8.7610e-02,  1.6386e-01]],\n",
       "              \n",
       "                       [[ 3.0663e-01,  8.5636e-01,  5.9673e-01],\n",
       "                        [-8.3291e-01, -7.5372e-01, -3.6590e-01],\n",
       "                        [ 5.2356e-03, -2.3271e-02, -1.1739e-01]],\n",
       "              \n",
       "                       [[ 7.4793e-01,  8.3830e-01,  2.3682e-01],\n",
       "                        [-4.7219e-01, -4.7703e-01, -1.9997e-01],\n",
       "                        [ 9.9404e-02, -3.7972e-01, -4.1654e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4389e-01, -1.6766e-01, -1.9358e-01],\n",
       "                        [-6.5485e-01, -5.7184e-01, -7.9434e-01],\n",
       "                        [-6.7295e-01, -5.9633e-01, -5.9251e-01]],\n",
       "              \n",
       "                       [[-1.0399e-01,  4.6378e-01,  1.6142e-02],\n",
       "                        [-1.9374e-01, -8.2373e-02, -1.3812e-01],\n",
       "                        [-2.2333e-01, -2.1542e-01, -2.5140e-01]],\n",
       "              \n",
       "                       [[ 5.3864e-01,  1.1065e+00,  7.2999e-01],\n",
       "                        [ 5.7934e-01,  1.0594e+00,  6.2575e-01],\n",
       "                        [-3.3914e-02,  4.3139e-01,  3.0039e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.5557e-01,  7.7835e-01, -1.7814e-01],\n",
       "                        [ 1.9155e-01,  4.1013e-01,  3.6364e-01],\n",
       "                        [ 2.6347e-02, -2.8235e-02,  2.2341e-01]],\n",
       "              \n",
       "                       [[-5.0296e-01, -3.9266e-01, -1.8949e-01],\n",
       "                        [-8.2804e-01, -6.1499e-01,  1.1121e-02],\n",
       "                        [-3.1366e-01, -1.7471e-01,  5.4434e-01]],\n",
       "              \n",
       "                       [[-4.5348e-01, -2.3656e-01, -4.0574e-01],\n",
       "                        [-6.1831e-01, -4.2694e-01, -3.8332e-02],\n",
       "                        [-2.6471e-01,  1.9310e-01,  3.0584e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8455e-01,  1.2126e+00,  8.0503e-01],\n",
       "                        [ 1.4647e+00, -1.2733e+00,  1.1694e-01],\n",
       "                        [-1.1123e+00, -1.0796e+00,  9.6822e-01]],\n",
       "              \n",
       "                       [[-8.3310e-01, -1.0898e-01,  6.5875e-02],\n",
       "                        [ 1.4889e+00, -2.1803e+00, -2.3472e-01],\n",
       "                        [-5.5331e-02, -3.7314e-01,  1.6295e+00]],\n",
       "              \n",
       "                       [[-3.3531e-01,  5.5317e-01, -4.9394e-01],\n",
       "                        [ 1.5348e+00, -1.1421e+00, -8.6518e-01],\n",
       "                        [ 6.2617e-01, -5.4433e-02,  4.6411e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.4604e-01,  1.2608e+00,  1.0239e+00],\n",
       "                        [ 6.0905e-02,  2.5489e-01,  1.5757e-01],\n",
       "                        [-7.9464e-01, -9.7786e-01, -9.5878e-01]],\n",
       "              \n",
       "                       [[ 5.0220e-01,  4.0124e-01,  4.4711e-01],\n",
       "                        [ 4.7901e-03, -1.1536e-01, -5.1425e-02],\n",
       "                        [-2.7524e-01, -7.8621e-01, -5.0263e-01]],\n",
       "              \n",
       "                       [[ 3.3087e-01,  1.4517e-01, -7.0901e-02],\n",
       "                        [-4.0793e-02,  2.4547e-01,  1.8638e-02],\n",
       "                        [-1.9316e-01, -1.1762e-01, -4.6944e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.1189e-01, -1.3706e-01,  1.5707e-01],\n",
       "                        [-2.3585e-03, -1.0508e+00, -5.0788e-01],\n",
       "                        [ 3.5409e-01,  8.6983e-02, -7.2599e-02]],\n",
       "              \n",
       "                       [[ 3.8382e-01, -9.1621e-02,  1.8528e-01],\n",
       "                        [ 1.1166e-01, -6.7314e-01, -2.4659e-01],\n",
       "                        [ 1.8720e-01,  2.8836e-01,  2.8809e-01]],\n",
       "              \n",
       "                       [[ 1.1892e-01, -1.9946e-01,  2.2136e-01],\n",
       "                        [-5.0872e-01, -8.2646e-01, -4.7349e-01],\n",
       "                        [-2.0345e-01, -3.7981e-01, -4.1044e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0992e+00, -3.6817e-03,  1.0181e+00],\n",
       "                        [-3.5721e-01,  3.9177e-02,  1.1431e-01],\n",
       "                        [ 1.0578e+00,  6.4090e-02, -8.5409e-01]],\n",
       "              \n",
       "                       [[-1.0481e+00, -1.6901e-01,  1.0465e+00],\n",
       "                        [-1.7580e-01,  2.1051e-01,  1.4377e-01],\n",
       "                        [ 9.2165e-01, -3.2907e-01, -8.3878e-01]],\n",
       "              \n",
       "                       [[-7.5452e-01,  3.0559e-01,  6.2661e-01],\n",
       "                        [-2.5981e-01,  3.1321e-01,  2.6233e-01],\n",
       "                        [ 7.7262e-01, -2.7381e-01, -8.9623e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2306e-01, -9.2307e-01,  4.5236e-01],\n",
       "                        [-3.3498e-01, -1.0396e+00, -2.1117e-01],\n",
       "                        [ 8.3307e-01,  8.2997e-01,  9.0825e-01]],\n",
       "              \n",
       "                       [[-4.3742e-01, -6.6041e-01,  1.1552e-01],\n",
       "                        [-1.5698e-01, -8.8968e-01, -1.3198e-01],\n",
       "                        [ 9.4637e-01,  7.2481e-01,  7.0995e-01]],\n",
       "              \n",
       "                       [[ 1.1793e-02, -5.7150e-01,  3.5219e-01],\n",
       "                        [ 2.2607e-02, -9.5980e-01, -2.1904e-01],\n",
       "                        [ 6.5610e-01,  3.8355e-01,  2.2455e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.2842e-01, -1.3555e-01, -2.0949e-01],\n",
       "                        [-1.2356e-01,  4.2564e-01,  4.5131e-01],\n",
       "                        [ 7.0156e-01,  1.1215e+00,  7.0723e-01]],\n",
       "              \n",
       "                       [[-4.6227e-01, -2.9525e-01, -6.7845e-01],\n",
       "                        [-9.5757e-01, -8.7562e-01, -1.0047e+00],\n",
       "                        [-4.4320e-01, -5.2527e-01, -7.1877e-01]],\n",
       "              \n",
       "                       [[ 8.4654e-01,  8.8862e-01,  7.5659e-01],\n",
       "                        [ 1.1400e-01,  2.7547e-01,  2.3232e-01],\n",
       "                        [-1.9108e-01,  5.0945e-02, -1.6371e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.5764e-01,  7.8496e-02,  7.0316e-02],\n",
       "                        [-9.8618e-03,  3.5874e-01,  1.4442e-01],\n",
       "                        [ 9.6966e-02,  1.8458e-01,  9.3510e-03]],\n",
       "              \n",
       "                       [[-1.9471e-01,  2.5681e-01, -1.8786e-01],\n",
       "                        [ 1.4716e-01,  9.6748e-02, -8.6126e-02],\n",
       "                        [ 3.5148e-02,  3.1066e-01, -6.5107e-02]],\n",
       "              \n",
       "                       [[ 2.5573e-01,  4.7144e-01,  1.9082e-01],\n",
       "                        [ 3.6060e-01,  5.8935e-01,  8.1626e-02],\n",
       "                        [-1.7770e-02,  1.2095e-01, -9.5245e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-3.0492e-01, -7.9451e-01, -2.0177e-01],\n",
       "                        [ 2.9974e-01,  9.3592e-01,  5.7752e-01],\n",
       "                        [ 4.5517e-01,  5.7255e-01,  1.0055e-01]],\n",
       "              \n",
       "                       [[-3.9410e-01, -9.5116e-01, -4.0499e-01],\n",
       "                        [-7.1705e-02,  4.2422e-01,  2.7775e-01],\n",
       "                        [-1.3216e-01, -2.8865e-01, -3.6781e-01]],\n",
       "              \n",
       "                       [[-9.0501e-02, -2.5837e-01,  1.6859e-01],\n",
       "                        [ 2.0695e-01,  7.0540e-01,  3.1135e-01],\n",
       "                        [-3.1081e-02,  5.2900e-02, -1.8616e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.1950e-01, -4.1508e-01, -2.8218e-01],\n",
       "                        [-1.4278e-02, -1.2342e-01, -7.2811e-02],\n",
       "                        [-2.5550e-01,  2.3747e-02,  4.0854e-02]],\n",
       "              \n",
       "                       [[-2.0806e-01,  1.3843e-02, -3.3811e-01],\n",
       "                        [ 2.6848e-03,  2.5554e-01, -8.4292e-02],\n",
       "                        [-5.0058e-01, -4.9409e-01, -4.0431e-01]],\n",
       "              \n",
       "                       [[ 2.5918e-01,  5.0689e-01,  2.3340e-01],\n",
       "                        [ 2.8925e-01,  6.9060e-01,  2.6207e-01],\n",
       "                        [-1.8518e-01, -7.2043e-02, -2.1737e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.2315e-01, -9.4369e-01, -7.2165e-01],\n",
       "                        [-5.0508e-02,  9.0636e-02, -1.1688e-01],\n",
       "                        [ 4.9442e-01,  6.7985e-01,  6.4325e-01]],\n",
       "              \n",
       "                       [[-3.7517e-01, -5.2720e-01, -4.6969e-01],\n",
       "                        [ 4.9999e-01,  3.3864e-01,  3.6457e-01],\n",
       "                        [ 1.5075e-01,  8.9029e-01,  7.5332e-01]],\n",
       "              \n",
       "                       [[-1.0619e-01, -9.4547e-01, -5.8482e-01],\n",
       "                        [ 7.8396e-02,  3.2259e-03,  1.5844e-02],\n",
       "                        [ 2.9331e-01,  1.3868e-01,  3.6770e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.6490e-01, -2.8646e-01,  3.7080e-02],\n",
       "                        [-4.6360e-01, -8.8054e-01, -2.2498e-01],\n",
       "                        [ 1.1127e-01, -6.8173e-02,  3.6203e-01]],\n",
       "              \n",
       "                       [[ 2.8686e-01, -1.3965e-02,  2.5676e-01],\n",
       "                        [-2.0552e-01, -4.3999e-01, -2.6020e-01],\n",
       "                        [ 1.2199e-01,  5.1533e-02,  2.1905e-01]],\n",
       "              \n",
       "                       [[ 2.5427e-01, -2.5339e-01,  1.3233e-01],\n",
       "                        [-4.4425e-01, -7.9445e-01, -2.3966e-01],\n",
       "                        [ 5.9852e-02, -1.1686e-01, -8.3175e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1581e+00,  1.5876e+00,  1.2343e+00],\n",
       "                        [ 1.4999e-01,  2.3932e-02,  9.7031e-02],\n",
       "                        [-1.0668e+00, -1.6777e+00, -1.2232e+00]],\n",
       "              \n",
       "                       [[-2.6281e-01, -3.1214e-01, -4.1979e-01],\n",
       "                        [-6.6602e-02, -2.8669e-01, -2.1713e-02],\n",
       "                        [ 4.5412e-01,  1.3624e-01,  5.0685e-01]],\n",
       "              \n",
       "                       [[-1.0695e+00, -1.3316e+00, -1.2191e+00],\n",
       "                        [-3.3425e-01,  2.9614e-01,  2.0976e-02],\n",
       "                        [ 1.0264e+00,  1.4344e+00,  9.6358e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.1699e-01, -8.9804e-02, -4.8738e-01],\n",
       "                        [-1.0412e+00,  2.2035e-01, -2.7585e-02],\n",
       "                        [-1.8318e-01,  4.6138e-02,  1.6011e-01]],\n",
       "              \n",
       "                       [[ 4.5050e-02,  8.1832e-01,  4.8944e-01],\n",
       "                        [-6.2886e-01,  1.2733e+00,  1.1358e+00],\n",
       "                        [-9.8908e-02,  3.2021e-01,  3.9740e-01]],\n",
       "              \n",
       "                       [[-3.2753e-01,  2.7570e-01, -3.1633e-01],\n",
       "                        [-8.7083e-01,  3.8136e-01, -1.0155e-01],\n",
       "                        [-4.9953e-01, -1.0893e-01,  2.0734e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.8644e-02, -3.2624e-01, -2.2315e-01],\n",
       "                        [ 3.7384e-01,  3.1365e-01,  2.7697e-01],\n",
       "                        [ 2.1826e-01,  2.5302e-01,  2.2848e-01]],\n",
       "              \n",
       "                       [[-2.3903e-01, -2.9286e-01, -3.7492e-01],\n",
       "                        [ 2.2696e-01,  3.0064e-01,  4.3780e-02],\n",
       "                        [ 1.6319e-01,  9.1570e-02, -4.1368e-02]],\n",
       "              \n",
       "                       [[-5.3434e-02, -5.6940e-02, -3.6186e-01],\n",
       "                        [ 4.6369e-01,  3.0135e-01, -1.4473e-01],\n",
       "                        [ 3.5113e-01,  1.4846e-01, -1.3136e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.0714e-01, -7.0433e-01, -5.0340e-01],\n",
       "                        [ 7.2643e-02,  5.7143e-02,  3.3262e-02],\n",
       "                        [ 5.6978e-01,  7.9702e-01,  6.5755e-01]],\n",
       "              \n",
       "                       [[ 5.5029e-03, -2.8522e-01, -2.0949e-01],\n",
       "                        [-1.4425e-01,  2.7532e-01,  2.0440e-01],\n",
       "                        [ 1.8113e-01,  3.5645e-01,  1.7561e-01]],\n",
       "              \n",
       "                       [[ 9.9449e-01,  1.0417e+00,  5.4901e-01],\n",
       "                        [ 3.8781e-02, -1.3406e-01, -2.5410e-01],\n",
       "                        [-5.8594e-01, -1.1834e+00, -1.1219e+00]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.7754e-02, -2.0948e-01,  1.2229e-01],\n",
       "                        [ 3.5918e-01,  2.3606e-01,  8.2641e-01],\n",
       "                        [ 1.8891e-01, -3.9553e-01,  2.4130e-01]],\n",
       "              \n",
       "                       [[ 3.8047e-01, -3.5316e-02,  4.0284e-02],\n",
       "                        [ 2.7422e-01, -4.9840e-01, -7.2813e-02],\n",
       "                        [-4.5077e-01, -1.0116e+00, -5.7418e-01]],\n",
       "              \n",
       "                       [[ 4.2405e-01,  4.4689e-01,  5.1046e-01],\n",
       "                        [ 6.5073e-01,  1.3184e-01,  2.2423e-01],\n",
       "                        [-2.9629e-01, -9.2983e-01, -3.9708e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.7836e-02, -7.5087e-01,  1.3452e+00],\n",
       "                        [-6.2575e-01,  2.6040e+00, -1.3689e+00],\n",
       "                        [-1.2604e+00,  5.9851e-02,  1.0685e-01]],\n",
       "              \n",
       "                       [[ 4.4690e-02, -5.2580e-01,  1.4804e-01],\n",
       "                        [-2.8576e-01,  2.5846e+00, -2.2666e+00],\n",
       "                        [-4.4783e-01,  5.1641e-01, -3.2163e-02]],\n",
       "              \n",
       "                       [[ 3.0412e-01, -8.0242e-01,  4.2518e-01],\n",
       "                        [-4.9551e-01,  1.5338e+00, -1.7846e+00],\n",
       "                        [ 1.7764e-01,  4.2812e-01,  3.4481e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 8.6875e-02,  5.1701e-01, -4.1698e-01],\n",
       "                        [-1.0267e+00,  7.3840e-01,  8.0301e-01],\n",
       "                        [-6.2991e-01,  7.0737e-02, -2.1441e-01]],\n",
       "              \n",
       "                       [[-5.2782e-01, -7.1171e-03,  1.7121e-02],\n",
       "                        [-1.5023e+00,  5.9908e-01,  1.2550e+00],\n",
       "                        [-6.6325e-01,  3.4872e-01,  3.2508e-01]],\n",
       "              \n",
       "                       [[-3.8546e-01,  8.9735e-02, -4.4784e-02],\n",
       "                        [-1.3276e+00,  7.2525e-01,  9.7362e-01],\n",
       "                        [-6.6382e-01,  5.0661e-01,  2.8737e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.0297e-01, -4.4385e-01, -3.7963e-01],\n",
       "                        [ 5.0906e-02, -2.2639e-01, -5.6638e-02],\n",
       "                        [-8.5377e-01, -1.6898e+00, -6.8952e-01]],\n",
       "              \n",
       "                       [[ 7.9777e-01,  1.0436e+00,  5.7461e-01],\n",
       "                        [ 1.6498e+00,  1.7265e+00,  1.4330e+00],\n",
       "                        [ 7.9440e-01,  8.3967e-02,  5.4361e-01]],\n",
       "              \n",
       "                       [[-7.7544e-01, -1.0937e+00, -8.4160e-01],\n",
       "                        [ 5.5908e-02, -6.1526e-01, -1.2425e-01],\n",
       "                        [-1.9594e-01, -8.8816e-01,  6.3752e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.5261e-01,  2.7538e-01, -2.0202e-01],\n",
       "                        [ 1.0844e+00,  2.6398e-01, -1.1756e+00],\n",
       "                        [ 1.3704e-01, -1.7433e-01, -2.9942e-01]],\n",
       "              \n",
       "                       [[ 7.7017e-01, -1.3015e-01, -1.2435e+00],\n",
       "                        [ 1.4542e+00, -1.1615e-01, -1.6577e+00],\n",
       "                        [ 4.1894e-01,  3.4633e-03, -5.3822e-01]],\n",
       "              \n",
       "                       [[ 4.9902e-01, -1.3109e-01, -8.7425e-01],\n",
       "                        [ 1.4743e+00,  3.6337e-01, -1.2042e+00],\n",
       "                        [ 8.6177e-01,  1.4009e-02, -4.8786e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.1483e+00,  1.4821e+00,  7.9972e-01],\n",
       "                        [ 8.8295e-01, -1.0935e+00,  8.8492e-01],\n",
       "                        [-3.6108e-01, -1.7023e+00, -1.5850e+00]],\n",
       "              \n",
       "                       [[-4.0459e-01, -2.9197e-01, -4.1407e-01],\n",
       "                        [-3.1370e-03, -1.6041e+00,  1.1043e+00],\n",
       "                        [ 5.1098e-01, -1.2983e-01,  5.8607e-01]],\n",
       "              \n",
       "                       [[-7.6655e-01, -5.9248e-01, -1.0845e+00],\n",
       "                        [ 4.7828e-02, -8.3389e-01,  6.7056e-01],\n",
       "                        [ 1.0259e+00,  9.6670e-01,  7.5338e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.7627e-02, -9.3956e-01, -6.5185e-01],\n",
       "                        [ 4.4882e-01,  9.8119e-01,  5.2181e-01],\n",
       "                        [-4.9687e-02,  2.4900e-01,  9.4613e-02]],\n",
       "              \n",
       "                       [[-3.7113e-01, -7.7517e-01, -6.7337e-01],\n",
       "                        [ 2.0036e-01,  9.5474e-01,  3.9141e-01],\n",
       "                        [-3.6652e-01,  4.6237e-02, -3.3025e-01]],\n",
       "              \n",
       "                       [[-2.4663e-02, -5.6934e-01, -3.3191e-01],\n",
       "                        [ 1.7403e-01,  9.3419e-01,  5.2080e-01],\n",
       "                        [-1.9306e-01,  7.5581e-02, -1.6303e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.4526e-01, -4.7319e-01, -5.8144e-02],\n",
       "                        [ 5.2575e-01,  2.0558e-01, -7.9737e-01],\n",
       "                        [ 6.0704e-01,  3.6191e-01, -3.0978e-01]],\n",
       "              \n",
       "                       [[-1.0660e-01, -1.8084e-01,  1.4867e-01],\n",
       "                        [ 8.7418e-01,  4.1051e-01, -5.4904e-01],\n",
       "                        [ 3.5131e-01,  4.0586e-02, -5.6816e-01]],\n",
       "              \n",
       "                       [[ 5.2015e-03, -1.3211e-03, -1.5475e-01],\n",
       "                        [ 9.4173e-01,  2.8285e-01, -6.4030e-01],\n",
       "                        [ 4.9779e-01, -2.2143e-01, -7.4964e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.3121e-01, -7.5906e-01, -4.7272e-01],\n",
       "                        [-3.3155e-01, -5.1259e-01, -1.3841e-01],\n",
       "                        [-2.5138e-01, -9.3561e-02,  8.6543e-02]],\n",
       "              \n",
       "                       [[-6.1921e-01, -7.8019e-01, -4.7794e-01],\n",
       "                        [ 1.3205e-01, -1.3983e-01,  7.7905e-02],\n",
       "                        [ 3.3254e-01,  1.7180e-01,  1.6117e-01]],\n",
       "              \n",
       "                       [[-3.0669e-01, -5.3661e-01, -4.6193e-01],\n",
       "                        [ 4.1804e-01,  5.2455e-01,  4.3928e-01],\n",
       "                        [ 6.3286e-01,  5.7358e-01,  4.0605e-01]]]], device='mps:0')),\n",
       "             ('model.0.1.weight',\n",
       "              tensor([1.0030, 1.2417, 1.1108, 0.7086, 1.4484, 1.0021, 0.7617, 0.9483, 2.0205,\n",
       "                      1.0026, 1.6280, 1.0919, 1.1475, 0.7983, 1.0918, 0.7695, 0.9342, 0.9284,\n",
       "                      1.6964, 1.3543, 0.8664, 0.4545, 1.1657, 0.8296, 2.8554, 1.4328, 1.1043,\n",
       "                      1.4243, 1.5086, 1.2256, 0.8222, 1.2479], device='mps:0')),\n",
       "             ('model.0.1.bias',\n",
       "              tensor([ 0.4378,  0.3951,  0.3621,  0.4648,  0.6802,  0.6909, -0.0453, -0.1896,\n",
       "                       1.9524,  0.5220, -0.6369,  0.4615,  0.5035,  0.1235, -0.6217,  0.3958,\n",
       "                      -0.6410,  0.4458, -1.2411, -0.4490,  0.5662,  0.2558, -0.6704,  0.0869,\n",
       "                       2.5992,  0.5860,  0.2017,  0.6434,  1.4005,  0.5692,  0.4218, -0.2638],\n",
       "                     device='mps:0')),\n",
       "             ('model.0.1.running_mean',\n",
       "              tensor([-0.1925,  0.1269,  0.1261, -0.1591,  0.2431,  0.1259,  0.0644,  0.5090,\n",
       "                       0.1148, -0.1223,  0.9110,  0.0363,  0.1858,  0.3026, -0.8746, -0.2723,\n",
       "                       0.3888, -0.1681,  0.8223,  0.0382, -0.1881, -0.4679, -0.0647, -0.0038,\n",
       "                      -0.1742, -0.1371, -0.0841, -0.0358,  0.1297, -0.2176, -0.0398,  0.6656],\n",
       "                     device='mps:0')),\n",
       "             ('model.0.1.running_var',\n",
       "              tensor([12.2908, 21.1225, 15.0946, 18.5407, 10.2946, 11.1816, 16.6319, 10.5193,\n",
       "                       6.9239, 24.3742, 17.8641,  4.8731, 22.7847,  4.5885, 14.5112,  7.1435,\n",
       "                       4.8605, 22.8186, 13.8453,  2.8103, 10.1067,  7.4839,  1.3455,  9.2337,\n",
       "                       9.1040, 23.1241, 11.0842, 46.6388,  5.3780, 10.1471, 11.1330, 20.9485],\n",
       "                     device='mps:0')),\n",
       "             ('model.0.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.1.0.weight',\n",
       "              tensor([[[[-2.0450e-01, -2.3559e-01, -1.6379e-01],\n",
       "                        [-2.3276e-01, -2.4086e-01, -2.4306e-01],\n",
       "                        [-9.4205e-02, -8.1977e-02, -2.6264e-02]],\n",
       "              \n",
       "                       [[-8.4158e-02,  4.9218e-02,  1.4698e-01],\n",
       "                        [-1.6226e-01, -3.4957e-02,  5.6308e-02],\n",
       "                        [-3.4209e-01, -2.1271e-01, -7.5631e-02]],\n",
       "              \n",
       "                       [[ 5.9199e-02, -1.3700e-02, -3.3085e-02],\n",
       "                        [ 2.5260e-02, -6.1533e-02, -4.0967e-02],\n",
       "                        [-5.4628e-02, -2.2659e-01, -1.9803e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3319e-01, -8.3192e-02, -1.4808e-01],\n",
       "                        [-2.0992e-02, -1.0417e-01, -8.4455e-02],\n",
       "                        [-2.8770e-02, -1.0738e-01, -9.2791e-02]],\n",
       "              \n",
       "                       [[-5.6517e-02, -8.5416e-02, -9.4544e-02],\n",
       "                        [ 5.2510e-02, -8.7249e-04,  2.9981e-02],\n",
       "                        [ 7.4321e-02,  3.7569e-02, -8.6443e-02]],\n",
       "              \n",
       "                       [[ 1.2159e-01,  9.9381e-02,  6.2586e-02],\n",
       "                        [ 2.9087e-01,  4.0082e-01,  3.3885e-01],\n",
       "                        [ 2.1024e-01,  3.1226e-01,  2.0942e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.7163e-02,  8.4086e-02,  9.8936e-03],\n",
       "                        [-2.8634e-01, -6.9121e-02,  3.9750e-02],\n",
       "                        [ 1.8067e-01,  6.6750e-02, -1.2808e-01]],\n",
       "              \n",
       "                       [[-7.3003e-02, -4.6267e-02, -7.5809e-02],\n",
       "                        [-2.8704e-01, -2.5646e-03,  1.8111e-01],\n",
       "                        [-1.3710e-01, -3.4417e-03,  2.3700e-02]],\n",
       "              \n",
       "                       [[ 7.2436e-03, -5.5749e-02, -5.1142e-02],\n",
       "                        [ 1.6388e-01, -9.9758e-03, -2.7778e-01],\n",
       "                        [-5.1662e-03,  1.9875e-01,  6.9679e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-6.4422e-03,  1.1803e-01, -1.5709e-01],\n",
       "                        [-3.0361e-01, -3.8535e-01, -2.0324e-01],\n",
       "                        [ 3.2971e-01,  5.8092e-01,  1.0326e-01]],\n",
       "              \n",
       "                       [[-9.8443e-02,  1.5720e-01,  7.9685e-02],\n",
       "                        [ 9.2909e-02, -1.2188e-01, -2.8326e-01],\n",
       "                        [-1.5366e-01,  1.3486e-01,  2.7730e-01]],\n",
       "              \n",
       "                       [[ 2.7207e-02,  1.5755e-01,  1.4364e-02],\n",
       "                        [-1.9243e-01, -8.5793e-02, -1.1409e-01],\n",
       "                        [-1.6731e-01,  3.4023e-02, -1.7437e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-5.7813e-01, -6.9504e-01, -4.2501e-01],\n",
       "                        [-1.2750e-01, -4.3643e-02,  1.0075e-01],\n",
       "                        [ 5.6347e-02,  4.3477e-01,  5.2736e-01]],\n",
       "              \n",
       "                       [[-1.6504e-01, -1.2353e-01, -1.2828e-02],\n",
       "                        [-1.8011e-01, -3.3459e-01, -3.7747e-02],\n",
       "                        [ 8.0111e-02,  9.7101e-02,  1.8796e-01]],\n",
       "              \n",
       "                       [[-4.7334e-02, -9.8294e-03,  1.4581e-03],\n",
       "                        [-1.5802e-01, -2.2482e-01,  1.4797e-01],\n",
       "                        [ 9.8968e-02, -7.6019e-02,  9.6128e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.8537e-02,  3.4886e-02, -9.0443e-02],\n",
       "                        [-1.4379e-03,  1.2311e-01,  4.8882e-02],\n",
       "                        [-2.1723e-01, -1.4267e-01,  1.2544e-01]],\n",
       "              \n",
       "                       [[ 1.5352e-01,  9.9329e-02,  1.5618e-01],\n",
       "                        [ 1.1465e-04, -6.1684e-03,  1.0449e-01],\n",
       "                        [-5.4194e-02, -2.0709e-01, -1.6001e-01]],\n",
       "              \n",
       "                       [[-3.0162e-02,  1.1974e-01, -2.4478e-02],\n",
       "                        [-1.7273e-01, -9.5262e-02, -1.8966e-01],\n",
       "                        [-1.8664e-01, -1.7581e-01, -1.9986e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-3.9211e-01, -6.6578e-02,  5.4359e-01],\n",
       "                        [-3.9380e-01, -3.5574e-01,  4.5573e-01],\n",
       "                        [-3.0933e-01, -3.1804e-01,  1.7506e-01]],\n",
       "              \n",
       "                       [[-4.7914e-01, -1.8475e-02,  1.8818e-01],\n",
       "                        [-3.6681e-01, -1.6110e-01,  4.8769e-01],\n",
       "                        [-2.2666e-01, -3.7882e-01,  1.4193e-01]],\n",
       "              \n",
       "                       [[ 1.1715e-01,  2.2990e-01, -2.9764e-01],\n",
       "                        [ 5.6628e-03,  4.7946e-01, -4.0796e-01],\n",
       "                        [-2.8893e-01,  3.0915e-01, -1.7982e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.4583e-01, -2.9147e-02,  1.8430e-01],\n",
       "                        [ 1.6447e-01, -2.2349e-01, -3.5572e-02],\n",
       "                        [ 2.0772e-01, -2.0713e-02, -1.7547e-01]],\n",
       "              \n",
       "                       [[ 1.9872e-01,  1.5174e-01, -3.9253e-01],\n",
       "                        [ 1.6320e-02,  2.8003e-01, -3.9971e-01],\n",
       "                        [-1.0956e-01,  3.0948e-01, -3.2189e-02]],\n",
       "              \n",
       "                       [[ 5.8455e-02, -3.5647e-02,  5.8517e-02],\n",
       "                        [ 7.2825e-02, -7.1414e-02, -3.7585e-02],\n",
       "                        [ 1.2763e-01,  2.1051e-02,  2.1819e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.5174e-02,  1.3353e-01,  6.8893e-02],\n",
       "                        [-3.3124e-02,  9.8053e-02, -1.6580e-02],\n",
       "                        [-1.2421e-01, -1.5969e-01, -1.9190e-01]],\n",
       "              \n",
       "                       [[-8.6922e-02,  1.1312e-01,  1.5660e-01],\n",
       "                        [ 6.0805e-02,  1.3355e-01, -9.8768e-03],\n",
       "                        [-2.7388e-02, -7.5958e-03,  1.0256e-02]],\n",
       "              \n",
       "                       [[-5.9784e-04,  7.7569e-02, -1.4120e-01],\n",
       "                        [ 4.2412e-02,  1.3326e-02, -3.5752e-02],\n",
       "                        [ 1.8106e-01,  1.4062e-01,  4.2871e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-8.0071e-02, -5.2341e-02, -3.7222e-03],\n",
       "                        [ 1.9100e-01,  3.3520e-01,  2.1077e-01],\n",
       "                        [-1.1907e-02, -1.8068e-04,  1.8417e-02]],\n",
       "              \n",
       "                       [[-8.2724e-02, -1.5755e-02, -9.2832e-02],\n",
       "                        [ 1.5520e-02,  1.2935e-01,  8.7191e-02],\n",
       "                        [ 8.9260e-02,  3.1560e-02,  8.9415e-02]],\n",
       "              \n",
       "                       [[-8.5277e-02,  1.3269e-01,  6.2698e-02],\n",
       "                        [-3.3066e-03,  1.2319e-01,  6.2084e-02],\n",
       "                        [-1.1390e-01, -1.2787e-01, -1.2984e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.3697e-02, -1.0072e-01, -6.1971e-03],\n",
       "                        [-1.5723e-01, -3.0556e-01, -1.1410e-01],\n",
       "                        [-2.4133e-01, -2.7589e-01, -1.9675e-01]],\n",
       "              \n",
       "                       [[-1.3003e-01, -1.4654e-04,  1.6108e-01],\n",
       "                        [-1.9634e-01, -7.3402e-02,  8.6209e-02],\n",
       "                        [-1.5153e-01, -8.3117e-02,  2.6136e-01]],\n",
       "              \n",
       "                       [[-6.2059e-02, -1.6364e-01, -2.6964e-01],\n",
       "                        [ 9.4567e-03, -6.1158e-02, -1.3125e-01],\n",
       "                        [ 2.9274e-01,  1.2319e-01, -5.1836e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3252e-02,  4.9279e-02,  1.3983e-02],\n",
       "                        [-2.2418e-01, -2.0307e-01, -1.7296e-01],\n",
       "                        [ 1.5339e-01,  1.2269e-01,  9.4176e-02]],\n",
       "              \n",
       "                       [[-7.9548e-02, -1.2242e-01, -1.1164e-01],\n",
       "                        [-2.3198e-02, -7.0765e-02, -1.0083e-01],\n",
       "                        [ 1.8194e-01,  9.9496e-02, -1.4201e-02]],\n",
       "              \n",
       "                       [[-1.0527e-01, -2.1383e-02, -9.0091e-02],\n",
       "                        [-3.6708e-01, -3.3485e-01, -3.5718e-01],\n",
       "                        [-2.2739e-01, -2.0871e-01, -2.3488e-01]]]], device='mps:0')),\n",
       "             ('model.1.1.weight',\n",
       "              tensor([1.2075, 1.6137, 1.5039, 1.1208, 1.3842, 1.8038, 1.8866, 1.1497, 2.3041,\n",
       "                      1.7560, 1.3912, 1.3800, 1.8007, 1.0066, 1.3157, 1.4913, 1.0902, 2.0610,\n",
       "                      1.0382, 2.1025, 1.1388, 1.5548, 1.8249, 1.8082, 1.3678, 2.6456, 2.4558,\n",
       "                      1.4036, 1.2387, 1.8133, 1.1103, 1.3620], device='mps:0')),\n",
       "             ('model.1.1.bias',\n",
       "              tensor([ 0.1179,  0.1031, -0.2335, -0.2701, -0.3128, -0.5125,  0.5676, -0.1560,\n",
       "                      -0.2460, -0.5754, -0.2256,  0.6038, -0.1898, -0.4951, -0.6099, -0.7462,\n",
       "                      -0.3689, -0.5736, -0.3172, -0.4241, -0.2158, -0.0523, -0.1862, -0.9262,\n",
       "                      -0.1928, -0.1471,  0.9170, -0.4284, -0.3327, -0.1152, -0.3112, -0.0748],\n",
       "                     device='mps:0')),\n",
       "             ('model.1.1.running_mean',\n",
       "              tensor([ -9.4253,  -2.8421,  -7.0705,  -4.4638,  -4.1854,  -7.7021,  -9.6240,\n",
       "                       -8.3632,  -4.1775,  -4.0232, -11.2920,  -4.9486,  -8.0370,   6.3452,\n",
       "                       -5.7452,   3.9910,  -1.4648,  -6.1724,  -3.3224,  -8.0714,  -7.5331,\n",
       "                       -4.3516,  -1.3795,  10.8637,  -9.4199,  -3.7977,  -5.9999,  -6.1545,\n",
       "                        7.2919,  -2.9348,   3.2052,  -7.9581], device='mps:0')),\n",
       "             ('model.1.1.running_var',\n",
       "              tensor([ 29.1162,  34.1855,  20.8430,  18.1495,  40.8200,  38.2013,  42.4680,\n",
       "                       34.3650, 242.1780,  51.0480,  40.0988,  44.0444, 105.4031,  13.3313,\n",
       "                       23.3735,  26.5661,  17.5198,  56.2843,  28.1791, 126.2668,  29.5352,\n",
       "                       65.6716,  92.5008,  48.2277,  25.6542, 410.7781,  56.3970,  38.1616,\n",
       "                       33.2006, 148.6181,  21.2483,  29.4248], device='mps:0')),\n",
       "             ('model.1.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.2.0.weight',\n",
       "              tensor([[[[-0.2047, -0.1127, -0.2001],\n",
       "                        [ 0.1811,  0.1478,  0.1489],\n",
       "                        [ 0.0889,  0.0572,  0.1019]],\n",
       "              \n",
       "                       [[ 0.1447, -0.0318, -0.1641],\n",
       "                        [-0.2170,  0.0112,  0.1703],\n",
       "                        [-0.2715, -0.5288, -0.3099]],\n",
       "              \n",
       "                       [[ 0.1018,  0.2345,  0.3516],\n",
       "                        [-0.0803, -0.2030, -0.1575],\n",
       "                        [-0.0365,  0.0885, -0.0690]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1357, -0.1432,  0.0248],\n",
       "                        [-0.1570, -0.0315, -0.0737],\n",
       "                        [ 0.0488,  0.0523,  0.0546]],\n",
       "              \n",
       "                       [[-0.1076, -0.0442,  0.0269],\n",
       "                        [-0.0019, -0.0290,  0.0323],\n",
       "                        [ 0.1646,  0.1136,  0.1026]],\n",
       "              \n",
       "                       [[-0.0303, -0.0221, -0.0988],\n",
       "                        [-0.1768, -0.0711, -0.0264],\n",
       "                        [-0.0498, -0.0046,  0.0040]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0866, -0.1529, -0.0138],\n",
       "                        [ 0.0116, -0.1068, -0.0356],\n",
       "                        [ 0.0731,  0.0089, -0.0824]],\n",
       "              \n",
       "                       [[ 0.0918, -0.0516, -0.1196],\n",
       "                        [ 0.0061,  0.1402, -0.0640],\n",
       "                        [-0.1016, -0.0927,  0.0177]],\n",
       "              \n",
       "                       [[-0.1282,  0.0934,  0.1125],\n",
       "                        [-0.0618, -0.0905,  0.1292],\n",
       "                        [ 0.0087, -0.0483, -0.0514]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0770, -0.2613, -0.1909],\n",
       "                        [ 0.0834,  0.0418,  0.0565],\n",
       "                        [ 0.1051,  0.2371,  0.2921]],\n",
       "              \n",
       "                       [[ 0.0201,  0.0307,  0.1518],\n",
       "                        [-0.0197, -0.0188,  0.0354],\n",
       "                        [-0.1265,  0.0069,  0.0107]],\n",
       "              \n",
       "                       [[ 0.0052, -0.0188,  0.0846],\n",
       "                        [-0.0449, -0.1434, -0.1113],\n",
       "                        [ 0.0011, -0.0100, -0.1220]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1060,  0.0847,  0.0848],\n",
       "                        [ 0.1148,  0.1475,  0.1645],\n",
       "                        [-0.0387,  0.0526,  0.0013]],\n",
       "              \n",
       "                       [[ 0.0229,  0.0112,  0.0490],\n",
       "                        [ 0.1034,  0.0858, -0.0886],\n",
       "                        [ 0.0517,  0.0633,  0.1276]],\n",
       "              \n",
       "                       [[ 0.0126, -0.0448, -0.0514],\n",
       "                        [ 0.1116,  0.0011, -0.0918],\n",
       "                        [ 0.2544,  0.1428,  0.0274]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0638, -0.0745,  0.0011],\n",
       "                        [-0.2179, -0.2851, -0.1804],\n",
       "                        [-0.0477, -0.3101, -0.1535]],\n",
       "              \n",
       "                       [[-0.0757, -0.1224, -0.1786],\n",
       "                        [-0.0625,  0.0166,  0.0569],\n",
       "                        [-0.0148,  0.0368, -0.0032]],\n",
       "              \n",
       "                       [[-0.1264, -0.0981, -0.0872],\n",
       "                        [ 0.0349, -0.0211,  0.0758],\n",
       "                        [-0.0213,  0.0303,  0.1087]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1678,  0.1229, -0.0932],\n",
       "                        [ 0.1106,  0.0357, -0.1184],\n",
       "                        [ 0.3243,  0.1664, -0.0230]],\n",
       "              \n",
       "                       [[ 0.2427, -0.0276, -0.3133],\n",
       "                        [ 0.5886,  0.1057, -0.4671],\n",
       "                        [ 0.1340, -0.1473, -0.5652]],\n",
       "              \n",
       "                       [[ 0.0940,  0.0701, -0.3001],\n",
       "                        [ 0.4073,  0.2877, -0.0375],\n",
       "                        [ 0.2765,  0.3032, -0.0733]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0824, -0.0833,  0.1645],\n",
       "                        [-0.0165,  0.1692,  0.3578],\n",
       "                        [ 0.0216,  0.2752,  0.2568]],\n",
       "              \n",
       "                       [[ 0.2045, -0.0729, -0.1481],\n",
       "                        [ 0.2564, -0.0909, -0.2747],\n",
       "                        [ 0.1700, -0.1715, -0.2892]],\n",
       "              \n",
       "                       [[ 0.1707,  0.0988, -0.1609],\n",
       "                        [ 0.1244, -0.0649, -0.2620],\n",
       "                        [ 0.2649,  0.0546, -0.1681]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0117, -0.0740, -0.0565],\n",
       "                        [-0.0262, -0.0954, -0.0321],\n",
       "                        [-0.1090,  0.0018,  0.0836]],\n",
       "              \n",
       "                       [[ 0.2174, -0.0588, -0.2400],\n",
       "                        [ 0.2047, -0.1886, -0.0908],\n",
       "                        [-0.0807, -0.0451, -0.0545]],\n",
       "              \n",
       "                       [[ 0.0508, -0.1656, -0.0206],\n",
       "                        [ 0.0406, -0.0241, -0.0577],\n",
       "                        [ 0.1356, -0.1414, -0.0403]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1350,  0.0061, -0.0161],\n",
       "                        [ 0.0854, -0.0183, -0.1447],\n",
       "                        [ 0.0770, -0.1295, -0.0736]],\n",
       "              \n",
       "                       [[-0.1838,  0.0188,  0.0858],\n",
       "                        [-0.0660,  0.0651,  0.0555],\n",
       "                        [-0.1312, -0.0774, -0.1413]],\n",
       "              \n",
       "                       [[-0.0256, -0.0832,  0.0044],\n",
       "                        [-0.0884, -0.1029,  0.0329],\n",
       "                        [ 0.0342, -0.0391,  0.0804]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0467, -0.0580,  0.1572],\n",
       "                        [ 0.0072, -0.0206,  0.1267],\n",
       "                        [ 0.0245,  0.0372,  0.2685]],\n",
       "              \n",
       "                       [[-0.0505, -0.2868, -0.1865],\n",
       "                        [ 0.1293, -0.1056, -0.2029],\n",
       "                        [ 0.2643,  0.0313, -0.0833]],\n",
       "              \n",
       "                       [[-0.0836, -0.2319, -0.2564],\n",
       "                        [ 0.0273, -0.2309, -0.1155],\n",
       "                        [-0.0040, -0.1378, -0.1510]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2878, -0.0362, -0.2228],\n",
       "                        [ 0.1939, -0.0413, -0.2647],\n",
       "                        [ 0.2101, -0.0301, -0.1748]],\n",
       "              \n",
       "                       [[-0.0894, -0.2128, -0.1667],\n",
       "                        [ 0.0366, -0.0243, -0.1481],\n",
       "                        [ 0.1919, -0.0343, -0.0528]],\n",
       "              \n",
       "                       [[ 0.0331,  0.0574,  0.0927],\n",
       "                        [ 0.0194,  0.0018,  0.0250],\n",
       "                        [ 0.0826, -0.0610, -0.0608]]]], device='mps:0')),\n",
       "             ('model.2.1.weight',\n",
       "              tensor([1.9256, 1.1403, 1.7446, 1.2133, 1.1309, 1.2049, 1.3980, 1.0988, 1.2684,\n",
       "                      1.2076, 1.0705, 1.2139, 1.4536, 1.6127, 1.3342, 1.1467, 1.1203, 1.3695,\n",
       "                      1.2475, 1.1073, 1.3371, 1.0271, 1.1530, 1.3951, 1.4328, 1.1859, 1.2219,\n",
       "                      1.0953, 1.0390, 1.0514, 1.7910, 0.9668, 1.2804, 1.6110, 1.2575, 0.9457,\n",
       "                      1.8649, 1.2512, 1.2944, 1.4385, 1.2408, 1.1781, 1.2670, 1.5709, 1.2379,\n",
       "                      1.6553, 1.2537, 1.6272, 1.2396, 1.4157, 1.3694, 1.4025, 1.1961, 1.3674,\n",
       "                      1.3943, 1.2592, 1.3740, 1.2671, 1.4051, 0.8829, 1.2363, 1.2853, 1.2570,\n",
       "                      1.3252], device='mps:0')),\n",
       "             ('model.2.1.bias',\n",
       "              tensor([-0.4465, -0.3081, -0.0381,  0.0147, -0.1504, -0.1281, -0.3135, -0.2718,\n",
       "                      -0.4213, -0.1064, -0.1362,  0.0986, -0.2400, -0.3684, -0.7908, -0.2050,\n",
       "                      -0.3550, -0.6338, -0.2392, -0.0587, -0.4594, -0.1533, -0.3746, -0.3429,\n",
       "                      -0.1735, -0.2453, -0.2189, -0.6403,  0.1461, -0.3892, -0.3638,  0.1967,\n",
       "                      -0.3444, -0.3732, -0.2118, -0.1732, -0.7756, -0.7076, -0.0654, -0.3057,\n",
       "                      -0.2316, -0.2749, -0.0192, -0.5112, -0.3534, -0.3142, -0.4422, -0.6408,\n",
       "                      -0.1298, -0.2533, -0.3032, -0.4673, -0.2066, -0.2858, -0.4452, -0.5167,\n",
       "                      -0.5261,  0.3231, -0.3390, -0.1759, -0.2902, -0.2319, -0.4067, -0.2068],\n",
       "                     device='mps:0')),\n",
       "             ('model.2.1.running_mean',\n",
       "              tensor([-1.9928,  3.0382, -6.5124, -2.5793, -2.6497, -2.0214, -1.1960, -0.7739,\n",
       "                      -3.1130, -2.6572, -2.8864, -3.6639, -2.5891, -1.7849, -0.1971, -1.2064,\n",
       "                      -4.4809, -3.9984, -4.9632, -0.7046, -2.1570,  0.9328, -0.2520, -2.5753,\n",
       "                      -3.9227,  2.5348, -1.7117,  3.9908, -1.9985,  0.1702, -4.8052, -4.0665,\n",
       "                      -2.4480, -1.6426, -3.2465,  3.1451, -7.5892,  0.2519, -1.9659,  5.7323,\n",
       "                      -0.5691, -1.1463, -3.8796, -2.2772,  0.0522, -3.3473, -3.8480, -6.5169,\n",
       "                      -0.8059, -1.1953, -1.6095, -4.3841, -1.7210, -5.1408, -4.3451, -3.5907,\n",
       "                      -2.9697, -2.5128, -2.1638, -0.7995, -1.1497, -1.0447, -0.7016, -2.9561],\n",
       "                     device='mps:0')),\n",
       "             ('model.2.1.running_var',\n",
       "              tensor([55.7063, 14.0940, 65.2682, 22.1385, 20.2429, 17.2499, 24.1205, 12.7838,\n",
       "                      15.0807, 19.4452, 17.7245, 19.9584, 29.9409, 28.9750, 13.0230, 15.5467,\n",
       "                      14.6561, 27.8253, 15.9239, 13.6248, 16.4803,  8.8682, 21.2981, 31.1923,\n",
       "                      32.0190, 18.9847, 21.8575, 12.7613, 24.6558, 10.7394, 49.6963, 22.5372,\n",
       "                      15.1605, 30.8096, 22.0495, 10.7526, 42.3788, 12.0234, 16.2590, 33.4382,\n",
       "                      23.1852, 22.7367, 26.1346, 32.4730, 22.2370, 30.9052, 25.3398, 21.3355,\n",
       "                      19.1165, 30.5844, 31.7406, 18.4297, 16.4332, 16.6414, 22.3340, 18.9612,\n",
       "                      22.1248, 31.1446, 25.2996, 15.0442, 16.4801,  8.6418, 24.4000, 20.9368],\n",
       "                     device='mps:0')),\n",
       "             ('model.2.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.3.0.weight',\n",
       "              tensor([[[[-0.0083,  0.1500, -0.0054],\n",
       "                        [-0.0723,  0.0230, -0.2313],\n",
       "                        [-0.0227, -0.0838, -0.0619]],\n",
       "              \n",
       "                       [[ 0.1004,  0.0312, -0.1434],\n",
       "                        [-0.0500, -0.0283, -0.2366],\n",
       "                        [-0.0978, -0.1703, -0.0821]],\n",
       "              \n",
       "                       [[-0.0550, -0.0554, -0.0895],\n",
       "                        [-0.0345,  0.0335,  0.0371],\n",
       "                        [ 0.0686,  0.1515,  0.0998]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1164,  0.1141,  0.1143],\n",
       "                        [-0.1796, -0.0766, -0.1126],\n",
       "                        [-0.0150, -0.0876,  0.0009]],\n",
       "              \n",
       "                       [[ 0.1904, -0.0516,  0.1672],\n",
       "                        [ 0.1991,  0.2198,  0.1382],\n",
       "                        [-0.0008,  0.0580, -0.1694]],\n",
       "              \n",
       "                       [[ 0.1730,  0.3803,  0.1922],\n",
       "                        [-0.2188,  0.0720, -0.0495],\n",
       "                        [-0.0767, -0.1826, -0.2539]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0702,  0.1338,  0.0045],\n",
       "                        [ 0.1851,  0.1291,  0.0475],\n",
       "                        [ 0.1103,  0.1706,  0.0817]],\n",
       "              \n",
       "                       [[-0.0374, -0.0801, -0.0948],\n",
       "                        [-0.0886, -0.1439, -0.1945],\n",
       "                        [-0.0125, -0.0558, -0.0249]],\n",
       "              \n",
       "                       [[ 0.0395,  0.0319, -0.1479],\n",
       "                        [ 0.0409,  0.0687, -0.1958],\n",
       "                        [ 0.1202,  0.1357, -0.1844]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0822,  0.0975, -0.0858],\n",
       "                        [-0.0877,  0.1262, -0.0104],\n",
       "                        [-0.1335,  0.1006,  0.1042]],\n",
       "              \n",
       "                       [[ 0.0050, -0.0523, -0.0383],\n",
       "                        [-0.0202, -0.1341, -0.1067],\n",
       "                        [-0.0758, -0.2085, -0.1334]],\n",
       "              \n",
       "                       [[ 0.0784,  0.0547, -0.0939],\n",
       "                        [-0.0526, -0.1416, -0.1999],\n",
       "                        [ 0.1080, -0.1683, -0.1837]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1442,  0.1356, -0.0660],\n",
       "                        [ 0.1100,  0.0578, -0.1165],\n",
       "                        [ 0.0167, -0.1166, -0.1031]],\n",
       "              \n",
       "                       [[-0.0341,  0.0712,  0.0497],\n",
       "                        [-0.0460, -0.0800, -0.0388],\n",
       "                        [ 0.0139, -0.0669, -0.0230]],\n",
       "              \n",
       "                       [[ 0.0204, -0.0644, -0.1453],\n",
       "                        [ 0.2527,  0.1273,  0.0850],\n",
       "                        [ 0.0836,  0.0221,  0.0422]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0093,  0.0234,  0.0585],\n",
       "                        [ 0.0175, -0.0381, -0.0210],\n",
       "                        [ 0.0135, -0.0540, -0.0571]],\n",
       "              \n",
       "                       [[ 0.0590,  0.0646,  0.0917],\n",
       "                        [-0.0856,  0.0141,  0.1569],\n",
       "                        [-0.0221, -0.0038,  0.1658]],\n",
       "              \n",
       "                       [[-0.1884, -0.0609,  0.1747],\n",
       "                        [-0.0564, -0.0908,  0.1538],\n",
       "                        [ 0.1046,  0.0723,  0.1130]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0940,  0.1750, -0.0022],\n",
       "                        [-0.1133,  0.0053,  0.0422],\n",
       "                        [-0.0602,  0.1732,  0.0832]],\n",
       "              \n",
       "                       [[ 0.0394,  0.1203,  0.0671],\n",
       "                        [-0.1050, -0.0093, -0.0752],\n",
       "                        [-0.1130,  0.1176, -0.0034]],\n",
       "              \n",
       "                       [[-0.3203, -0.0097,  0.1765],\n",
       "                        [-0.4532,  0.0232,  0.3331],\n",
       "                        [-0.2294, -0.1019,  0.1312]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0563,  0.0013,  0.0709],\n",
       "                        [ 0.2010, -0.1721,  0.0615],\n",
       "                        [ 0.1396, -0.0913,  0.2144]],\n",
       "              \n",
       "                       [[ 0.0123, -0.1960, -0.1536],\n",
       "                        [ 0.3914,  0.0433, -0.2460],\n",
       "                        [ 0.1856,  0.0222, -0.2020]],\n",
       "              \n",
       "                       [[-0.1993, -0.1158, -0.1832],\n",
       "                        [-0.0041,  0.2234, -0.0272],\n",
       "                        [-0.2288, -0.1128, -0.0563]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0531,  0.0115,  0.0063],\n",
       "                        [ 0.1512,  0.0624,  0.1500],\n",
       "                        [-0.0294,  0.0015,  0.0682]],\n",
       "              \n",
       "                       [[-0.1657, -0.2169, -0.0378],\n",
       "                        [-0.1453, -0.1703, -0.0532],\n",
       "                        [-0.1080, -0.1303, -0.0451]],\n",
       "              \n",
       "                       [[-0.0166,  0.0583,  0.0124],\n",
       "                        [-0.1330, -0.0136, -0.1286],\n",
       "                        [-0.1505, -0.0036, -0.0315]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0193, -0.0110,  0.0438],\n",
       "                        [ 0.1742,  0.0646,  0.1578],\n",
       "                        [-0.0017, -0.0186,  0.1116]],\n",
       "              \n",
       "                       [[ 0.0292, -0.0349, -0.0704],\n",
       "                        [-0.0202,  0.0031, -0.2036],\n",
       "                        [-0.0212, -0.1400, -0.1568]],\n",
       "              \n",
       "                       [[-0.0624, -0.0781, -0.1796],\n",
       "                        [-0.0022, -0.1193, -0.1671],\n",
       "                        [-0.0821, -0.2195, -0.1771]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1056,  0.0142, -0.2399],\n",
       "                        [-0.0049,  0.0453,  0.1754],\n",
       "                        [ 0.0466, -0.0077, -0.1130]],\n",
       "              \n",
       "                       [[ 0.0567,  0.1212,  0.1499],\n",
       "                        [-0.0321, -0.1286,  0.2002],\n",
       "                        [-0.0457,  0.0176, -0.0976]],\n",
       "              \n",
       "                       [[-0.0215,  0.0314,  0.0674],\n",
       "                        [-0.1464, -0.1990, -0.0037],\n",
       "                        [ 0.0149, -0.1820, -0.0784]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0761, -0.0619, -0.1314],\n",
       "                        [ 0.0161,  0.0051,  0.0543],\n",
       "                        [-0.1800, -0.0262, -0.0177]],\n",
       "              \n",
       "                       [[ 0.0266,  0.0476,  0.0704],\n",
       "                        [ 0.0892,  0.0362, -0.0326],\n",
       "                        [-0.1064,  0.0497, -0.0992]],\n",
       "              \n",
       "                       [[ 0.0980, -0.1993, -0.1468],\n",
       "                        [ 0.1409,  0.0760, -0.3261],\n",
       "                        [-0.1185, -0.0214,  0.0533]]]], device='mps:0')),\n",
       "             ('model.3.1.weight',\n",
       "              tensor([1.3190, 1.1180, 1.2382, 1.2546, 1.1259, 1.0365, 1.1074, 1.0645, 1.2794,\n",
       "                      1.2908, 1.2159, 1.2696, 1.2822, 1.3349, 1.3393, 1.2548, 1.4102, 1.4814,\n",
       "                      1.6202, 1.6525, 1.2932, 1.3967, 1.2795, 1.6835, 1.1727, 1.1932, 1.0921,\n",
       "                      1.0750, 1.2434, 1.5235, 1.0360, 1.1725, 1.4504, 1.3967, 1.0869, 1.2597,\n",
       "                      1.4732, 1.3651, 1.3401, 1.2821, 1.2864, 1.1884, 1.7954, 1.4907, 1.5965,\n",
       "                      1.6143, 1.4476, 1.3842, 1.2697, 0.9808, 1.0397, 1.4917, 1.4099, 1.2984,\n",
       "                      1.5375, 1.3735, 1.5544, 1.1962, 1.4156, 1.4147, 1.1754, 1.5876, 1.1456,\n",
       "                      1.1748], device='mps:0')),\n",
       "             ('model.3.1.bias',\n",
       "              tensor([-0.4854, -0.2668, -0.5873, -0.4109, -0.4938, -0.6440, -0.5770, -0.0908,\n",
       "                      -0.1735, -0.3019, -0.5167, -0.7163, -0.3336, -0.7762, -0.7664, -0.5621,\n",
       "                      -0.6404, -0.4426, -0.2865, -0.3600, -0.6672, -0.3504, -0.5030, -0.5217,\n",
       "                      -0.3071, -0.5105, -0.2730, -0.3122, -0.5838, -0.5779, -0.3074, -0.5794,\n",
       "                      -0.9491, -0.5583, -0.4573, -0.4525, -0.7482, -0.3548, -0.7252, -0.6638,\n",
       "                      -0.3966, -0.4201, -0.6708, -0.5003, -0.4045, -0.7621, -0.5259, -0.4067,\n",
       "                      -0.2220, -0.3420, -0.3101, -0.3821, -0.3612, -0.5837, -0.6755, -0.5457,\n",
       "                      -0.5992, -0.2263, -0.3358, -0.2267, -0.1568, -0.6493, -0.3560, -0.6185],\n",
       "                     device='mps:0')),\n",
       "             ('model.3.1.running_mean',\n",
       "              tensor([ -0.4888,  -5.6021,  -0.8027,  -4.3722,   3.8333,  -2.1104,  -4.5647,\n",
       "                        0.1274,  -5.2746,  -4.1091,  -0.5473,  -3.4791,  -2.7392,  -7.1562,\n",
       "                       -3.0521,  -2.5922,  -2.7966,  -5.7435,  -0.7540,  -3.5287,  -1.3683,\n",
       "                       -2.3207,  -1.3927,  -4.2081,  -3.2862,  -2.8587,  -4.6630,  -2.5314,\n",
       "                       -2.9189,  -2.7521,  -3.3458,   1.2015,  -4.6294,  -2.7083,   0.5240,\n",
       "                       -4.2818,  -6.4446,  -3.2725,  -6.1555,  -3.1081,  -5.0032,   0.0339,\n",
       "                       -1.4064,  -6.2180,  -5.2362,  -1.5057,  -6.0688,  -1.3179,  -2.7370,\n",
       "                       -3.2648,  -1.4364,  -3.8325,  -4.3731,  -4.9827,  -5.5363,  -2.1743,\n",
       "                       -3.5827,  -3.9385,  -4.7900, -10.1379,  -5.0204,  -2.8411,  -7.1180,\n",
       "                       -1.6577], device='mps:0')),\n",
       "             ('model.3.1.running_var',\n",
       "              tensor([16.8949, 12.5606,  9.8843, 16.4332, 16.1254, 10.8427, 10.5227, 11.8037,\n",
       "                      13.3581, 12.0905, 13.1394, 12.7650, 15.4342, 13.9630, 10.2298,  9.4212,\n",
       "                      11.5822, 16.5965, 25.2216, 19.5119, 13.0804, 15.1732, 16.8932, 20.7458,\n",
       "                      12.7237,  6.6458, 10.7897, 10.4430, 11.8725, 13.3705,  9.0570, 11.9949,\n",
       "                      11.5323, 12.3209, 14.3645, 11.6610, 12.7421, 15.6724, 12.5693, 10.1125,\n",
       "                       9.3820,  9.3522, 14.4353, 11.6209, 27.1242, 15.4302, 12.3543, 15.8935,\n",
       "                      11.2553,  7.3714,  9.8187, 20.9656, 12.3698, 13.3801, 16.7866, 16.1229,\n",
       "                      12.5393, 12.8755,  9.3663, 27.3126, 14.5344, 14.2208, 12.8565, 15.5489],\n",
       "                     device='mps:0')),\n",
       "             ('model.3.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.4.0.weight',\n",
       "              tensor([[[[-0.1176,  0.0107, -0.0429],\n",
       "                        [ 0.1977,  0.2009,  0.1522],\n",
       "                        [ 0.1119,  0.0461, -0.1063]],\n",
       "              \n",
       "                       [[-0.0538, -0.0435, -0.2097],\n",
       "                        [ 0.0731,  0.0501,  0.0081],\n",
       "                        [-0.0064,  0.0285,  0.0931]],\n",
       "              \n",
       "                       [[ 0.0086,  0.0426,  0.0070],\n",
       "                        [ 0.0221, -0.0206,  0.0221],\n",
       "                        [-0.0106, -0.0497, -0.0731]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0120, -0.0125, -0.0387],\n",
       "                        [-0.0222, -0.0578,  0.1192],\n",
       "                        [-0.0322,  0.0282,  0.1220]],\n",
       "              \n",
       "                       [[-0.0295, -0.1214, -0.1559],\n",
       "                        [-0.0158, -0.0017,  0.0271],\n",
       "                        [ 0.1386,  0.2121,  0.1496]],\n",
       "              \n",
       "                       [[ 0.2929,  0.3589,  0.1960],\n",
       "                        [ 0.2914,  0.4306,  0.2696],\n",
       "                        [ 0.0058,  0.0098, -0.0213]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0259, -0.0383, -0.1965],\n",
       "                        [ 0.0822,  0.0561,  0.1431],\n",
       "                        [-0.1519, -0.0313,  0.2850]],\n",
       "              \n",
       "                       [[ 0.2061,  0.1306, -0.1561],\n",
       "                        [ 0.0655, -0.0775, -0.2238],\n",
       "                        [-0.0571, -0.2417, -0.2259]],\n",
       "              \n",
       "                       [[-0.1108,  0.0578,  0.2150],\n",
       "                        [-0.1000,  0.1666,  0.2679],\n",
       "                        [-0.0473,  0.1298,  0.1487]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.2559, -0.1803,  0.0141],\n",
       "                        [-0.2492, -0.0955,  0.0846],\n",
       "                        [-0.1614, -0.0608, -0.0487]],\n",
       "              \n",
       "                       [[-0.1505,  0.0650,  0.0168],\n",
       "                        [-0.1898, -0.0146,  0.1504],\n",
       "                        [-0.0555,  0.1159,  0.1735]],\n",
       "              \n",
       "                       [[ 0.1641,  0.2030,  0.0422],\n",
       "                        [ 0.1504, -0.1502, -0.0497],\n",
       "                        [ 0.1450, -0.1265, -0.2116]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2430, -0.1891, -0.1834],\n",
       "                        [-0.0622, -0.1214, -0.2139],\n",
       "                        [ 0.5142,  0.2361,  0.1361]],\n",
       "              \n",
       "                       [[-0.1484, -0.1370, -0.1995],\n",
       "                        [-0.1627, -0.1367, -0.2503],\n",
       "                        [-0.0077,  0.0106, -0.0370]],\n",
       "              \n",
       "                       [[ 0.0856,  0.0673,  0.0646],\n",
       "                        [-0.0218, -0.0816, -0.1267],\n",
       "                        [ 0.0535, -0.0434, -0.1558]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0121, -0.0453, -0.0152],\n",
       "                        [-0.0701, -0.0980, -0.0884],\n",
       "                        [-0.1358,  0.0586,  0.1427]],\n",
       "              \n",
       "                       [[ 0.0969,  0.0974,  0.0183],\n",
       "                        [ 0.0044,  0.0136, -0.0318],\n",
       "                        [-0.0781, -0.0261, -0.0914]],\n",
       "              \n",
       "                       [[-0.0347, -0.0168,  0.0208],\n",
       "                        [-0.0017,  0.0636,  0.0129],\n",
       "                        [-0.0184,  0.0770,  0.1377]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0739, -0.2572, -0.0998],\n",
       "                        [ 0.0658, -0.1494, -0.1830],\n",
       "                        [-0.0858, -0.2126, -0.2987]],\n",
       "              \n",
       "                       [[ 0.0018,  0.1105, -0.1209],\n",
       "                        [ 0.1622,  0.3122,  0.0259],\n",
       "                        [ 0.0030,  0.1779,  0.0544]],\n",
       "              \n",
       "                       [[-0.1063, -0.1610, -0.1484],\n",
       "                        [-0.0393, -0.1491, -0.1667],\n",
       "                        [ 0.1375,  0.0430,  0.0804]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1852, -0.1992, -0.2095],\n",
       "                        [-0.1139, -0.1705, -0.1691],\n",
       "                        [-0.0067, -0.0662, -0.1142]],\n",
       "              \n",
       "                       [[ 0.0605, -0.0094,  0.0999],\n",
       "                        [ 0.0206, -0.0034,  0.0122],\n",
       "                        [-0.0587, -0.0027, -0.0717]],\n",
       "              \n",
       "                       [[ 0.0370,  0.1835,  0.1163],\n",
       "                        [ 0.0177,  0.2201,  0.2675],\n",
       "                        [ 0.0089,  0.1282,  0.1708]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2747,  0.0768, -0.0370],\n",
       "                        [ 0.0990, -0.0089, -0.0917],\n",
       "                        [-0.1302, -0.0299, -0.0374]],\n",
       "              \n",
       "                       [[ 0.0270,  0.0469,  0.1108],\n",
       "                        [-0.0093,  0.1143,  0.1700],\n",
       "                        [-0.1847, -0.1940, -0.1680]],\n",
       "              \n",
       "                       [[-0.0548, -0.1692, -0.2138],\n",
       "                        [-0.0183, -0.2422, -0.1510],\n",
       "                        [-0.0006, -0.2289, -0.1065]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.2382,  0.0461, -0.1419],\n",
       "                        [ 0.1135,  0.0208,  0.0111],\n",
       "                        [-0.1012, -0.0770,  0.0903]],\n",
       "              \n",
       "                       [[-0.1574, -0.0317, -0.1266],\n",
       "                        [ 0.0207,  0.1278,  0.0411],\n",
       "                        [-0.0558,  0.1040,  0.0760]],\n",
       "              \n",
       "                       [[-0.1348,  0.0779,  0.1519],\n",
       "                        [ 0.0086,  0.1342,  0.2490],\n",
       "                        [ 0.1279,  0.2465,  0.1962]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0512,  0.0185,  0.1555],\n",
       "                        [-0.1807, -0.0139,  0.2570],\n",
       "                        [-0.1918, -0.0514,  0.0820]],\n",
       "              \n",
       "                       [[ 0.1704,  0.0982, -0.0390],\n",
       "                        [ 0.0485,  0.0422, -0.0185],\n",
       "                        [-0.0411, -0.1185, -0.2182]],\n",
       "              \n",
       "                       [[-0.2099, -0.1831, -0.0087],\n",
       "                        [-0.1119,  0.0168,  0.1992],\n",
       "                        [-0.0338,  0.0972,  0.1409]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.1506, -0.0294,  0.0917],\n",
       "                        [ 0.1225,  0.0391,  0.0012],\n",
       "                        [-0.0934, -0.2671, -0.1637]],\n",
       "              \n",
       "                       [[-0.0131, -0.0187, -0.0307],\n",
       "                        [ 0.0254, -0.0020, -0.0125],\n",
       "                        [ 0.0163, -0.0057, -0.0451]],\n",
       "              \n",
       "                       [[ 0.0298, -0.0842, -0.2562],\n",
       "                        [-0.0779, -0.1239, -0.2586],\n",
       "                        [ 0.0265, -0.0361, -0.2389]]]], device='mps:0')),\n",
       "             ('model.4.1.weight',\n",
       "              tensor([1.2667, 1.4633, 1.4652, 1.2473, 1.2678, 1.3275, 1.2704, 1.3103, 1.3267,\n",
       "                      1.3089, 1.1075, 1.1774, 1.2050, 1.2696, 1.1714, 1.5410, 1.3020, 1.1395,\n",
       "                      1.1654, 1.1934, 1.3384, 1.2071, 1.4387, 1.2159, 1.2397, 1.2435, 1.4120,\n",
       "                      1.3159, 1.2508, 1.4048, 1.6459, 1.2926, 1.4018, 1.0484, 1.1737, 1.2419,\n",
       "                      1.3507, 1.0782, 1.3316, 1.2210, 1.3727, 1.2936, 1.3843, 1.1571, 1.3361,\n",
       "                      1.4103, 1.6268, 1.4152, 1.2670, 1.2594, 1.3058, 1.2780, 1.2616, 1.3644,\n",
       "                      1.2317, 1.5056, 1.6880, 1.2385, 1.4461, 1.5951, 1.2388, 1.2257, 1.4276,\n",
       "                      1.2208], device='mps:0')),\n",
       "             ('model.4.1.bias',\n",
       "              tensor([-0.6408, -0.6198, -0.7592, -0.4416, -0.4156, -0.2914, -0.3514, -0.3897,\n",
       "                      -0.4619, -0.5087, -0.6733, -0.3757, -0.6500, -0.3709, -0.2966, -0.4767,\n",
       "                      -0.6845, -0.3249, -0.6998, -0.6008, -0.7427, -0.5350, -0.5317, -0.0403,\n",
       "                      -0.4801, -0.3535, -0.4311, -0.2167, -0.5179, -0.7103, -1.2621, -0.2590,\n",
       "                      -0.7217, -0.4630, -0.3937, -0.1279, -0.7425, -0.3795, -0.5655, -0.3992,\n",
       "                      -0.6073, -0.5747, -0.5160, -0.2287, -0.4712, -0.4138, -0.4513, -0.4475,\n",
       "                      -0.1692, -0.8053, -0.4273, -0.3812, -0.3055, -0.4501, -0.6182, -0.6035,\n",
       "                      -0.6540, -1.2893, -0.2947, -1.0344, -0.5830, -0.1799, -0.5099, -0.4666],\n",
       "                     device='mps:0')),\n",
       "             ('model.4.1.running_mean',\n",
       "              tensor([-0.1712, -2.9924, -3.9894, -3.7864, -0.4492, -0.5111, -1.9707,  0.2430,\n",
       "                      -3.3982, -2.9897, -2.7136, -3.6269, -1.5591, -0.5647,  1.7384, -5.5602,\n",
       "                      -0.7546, -1.8628, -0.7234, -2.7783, -4.3481, -0.9485, -5.6084, -5.9963,\n",
       "                      -4.2262, -2.9135, -0.9096, -2.3539, -2.8570, -1.7209, -5.6677, -4.7934,\n",
       "                      -0.3744, -2.1989, -3.3858, -1.3962, -4.3127, -3.4048,  0.4998, -1.2620,\n",
       "                      -6.0606,  0.2153, -6.9155, -0.7795, -3.5615, -1.5350, -4.2344,  0.6406,\n",
       "                      -1.8597, -3.0793, -0.6737, -7.0803, -5.9711, -3.1867, -1.6543, -2.5406,\n",
       "                      -2.3667,  2.4432, -2.5341, -1.2085, -5.0150, -3.8083, -4.3958, -3.2634],\n",
       "                     device='mps:0')),\n",
       "             ('model.4.1.running_var',\n",
       "              tensor([ 9.2968, 14.5978, 12.4169, 15.6569, 10.9786, 11.1443, 12.0076, 11.0371,\n",
       "                      13.0240, 16.7714, 15.1350, 14.0162, 13.4145, 15.1080,  7.8095, 12.5433,\n",
       "                      14.3805, 11.1915,  8.3504,  9.7478,  8.9307, 11.0929, 12.6689, 17.2849,\n",
       "                      13.1578,  9.0983, 11.1299, 11.2598,  8.2720, 11.4439, 15.6425, 18.8047,\n",
       "                      16.7939, 12.8289,  9.0320, 14.2778,  9.3551,  7.4770,  8.5497, 18.9538,\n",
       "                      12.2480, 12.8766, 13.4617, 13.5801, 20.2872, 13.4006, 34.3165, 14.4026,\n",
       "                      19.9387,  8.7656, 13.6607, 14.7772, 24.2815, 11.3039, 12.5187, 17.9699,\n",
       "                      23.4652,  7.1584, 17.3399,  9.9262, 12.7152, 14.1361, 14.4367, 10.9589],\n",
       "                     device='mps:0')),\n",
       "             ('model.4.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.5.0.weight',\n",
       "              tensor([[[[-0.0205,  0.0247,  0.0467],\n",
       "                        [ 0.0772,  0.1259,  0.0626],\n",
       "                        [ 0.1496,  0.1431,  0.1448]],\n",
       "              \n",
       "                       [[ 0.1279,  0.0364,  0.0453],\n",
       "                        [ 0.0520, -0.0388, -0.0640],\n",
       "                        [-0.0325, -0.0919, -0.0671]],\n",
       "              \n",
       "                       [[-0.0360, -0.0230, -0.0184],\n",
       "                        [ 0.0748,  0.1162,  0.0832],\n",
       "                        [ 0.0859,  0.0921,  0.0654]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0460, -0.0099, -0.0158],\n",
       "                        [ 0.0531,  0.1059,  0.0305],\n",
       "                        [ 0.1434,  0.2077,  0.1595]],\n",
       "              \n",
       "                       [[-0.0768,  0.0232,  0.0048],\n",
       "                        [-0.0434,  0.0096, -0.0145],\n",
       "                        [-0.0662, -0.0897, -0.0662]],\n",
       "              \n",
       "                       [[ 0.0391,  0.0036, -0.0158],\n",
       "                        [-0.0323, -0.0143,  0.0606],\n",
       "                        [-0.0804, -0.0757, -0.0080]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0415, -0.0392, -0.1055],\n",
       "                        [ 0.0267,  0.0222,  0.1402],\n",
       "                        [ 0.1352,  0.1365,  0.1584]],\n",
       "              \n",
       "                       [[-0.0452, -0.0955, -0.0099],\n",
       "                        [ 0.0216, -0.0048, -0.0030],\n",
       "                        [ 0.1167,  0.0180,  0.0296]],\n",
       "              \n",
       "                       [[ 0.0756,  0.0569,  0.0418],\n",
       "                        [ 0.0625,  0.0471,  0.0101],\n",
       "                        [-0.0270, -0.0600, -0.0563]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0248, -0.0065, -0.1094],\n",
       "                        [ 0.1249,  0.0277, -0.0280],\n",
       "                        [ 0.0777,  0.0496,  0.0189]],\n",
       "              \n",
       "                       [[ 0.0467,  0.0615, -0.0585],\n",
       "                        [-0.0526, -0.0643, -0.0714],\n",
       "                        [-0.0739, -0.0950, -0.1224]],\n",
       "              \n",
       "                       [[-0.0545, -0.0589, -0.0288],\n",
       "                        [-0.0756, -0.0372, -0.0241],\n",
       "                        [-0.0644, -0.0760, -0.0891]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0256, -0.0905,  0.0598],\n",
       "                        [-0.0650, -0.0473,  0.0266],\n",
       "                        [-0.1142, -0.1023, -0.0545]],\n",
       "              \n",
       "                       [[-0.0794, -0.1641, -0.2534],\n",
       "                        [-0.1043, -0.1951, -0.2406],\n",
       "                        [-0.0295, -0.0298, -0.1966]],\n",
       "              \n",
       "                       [[-0.1387,  0.0119, -0.0139],\n",
       "                        [-0.0163,  0.0771,  0.0741],\n",
       "                        [ 0.0339, -0.0287, -0.0170]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0377, -0.0585, -0.0464],\n",
       "                        [ 0.0158, -0.0892, -0.0256],\n",
       "                        [-0.0705, -0.0818, -0.0254]],\n",
       "              \n",
       "                       [[ 0.0407, -0.0483, -0.0504],\n",
       "                        [ 0.0868,  0.0413, -0.0414],\n",
       "                        [ 0.0294,  0.0045,  0.0348]],\n",
       "              \n",
       "                       [[-0.0209, -0.0362, -0.0505],\n",
       "                        [-0.1339, -0.1714, -0.1521],\n",
       "                        [-0.1785, -0.1308, -0.0697]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0389, -0.0631, -0.0263],\n",
       "                        [-0.0772, -0.1617, -0.0986],\n",
       "                        [ 0.0463, -0.0223, -0.0764]],\n",
       "              \n",
       "                       [[ 0.0483, -0.0113,  0.0040],\n",
       "                        [ 0.0447, -0.0682,  0.0025],\n",
       "                        [-0.0331,  0.0082,  0.0393]],\n",
       "              \n",
       "                       [[-0.0326, -0.0277, -0.0194],\n",
       "                        [-0.0448, -0.0107, -0.0672],\n",
       "                        [ 0.0269,  0.0913,  0.1017]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1041,  0.1090,  0.1676],\n",
       "                        [ 0.0436, -0.0584,  0.0537],\n",
       "                        [ 0.0176, -0.1275, -0.1277]],\n",
       "              \n",
       "                       [[-0.1125, -0.0690, -0.1392],\n",
       "                        [-0.0446, -0.0037, -0.0108],\n",
       "                        [-0.0512,  0.0039,  0.1312]],\n",
       "              \n",
       "                       [[-0.0469, -0.0614, -0.1127],\n",
       "                        [-0.0237, -0.0179, -0.0512],\n",
       "                        [-0.0174,  0.0101,  0.0169]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0629,  0.0406,  0.0430],\n",
       "                        [ 0.0305,  0.0280,  0.0607],\n",
       "                        [-0.0104,  0.0841,  0.0612]],\n",
       "              \n",
       "                       [[ 0.0005, -0.0212, -0.0243],\n",
       "                        [-0.0570, -0.1607, -0.0841],\n",
       "                        [-0.0319, -0.1210, -0.1309]],\n",
       "              \n",
       "                       [[-0.0665, -0.0370, -0.0666],\n",
       "                        [-0.0038, -0.0154,  0.0040],\n",
       "                        [-0.0663, -0.0978,  0.0248]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.1144,  0.1104,  0.0134],\n",
       "                        [ 0.0984,  0.0386, -0.0228],\n",
       "                        [-0.0412, -0.0298,  0.0089]],\n",
       "              \n",
       "                       [[-0.0660, -0.1089, -0.1419],\n",
       "                        [ 0.0575,  0.0316,  0.0074],\n",
       "                        [ 0.0342, -0.0590, -0.0515]],\n",
       "              \n",
       "                       [[-0.0121,  0.0391,  0.0767],\n",
       "                        [-0.0518, -0.0084,  0.0083],\n",
       "                        [-0.0430, -0.0959, -0.0616]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1468,  0.2227,  0.1510],\n",
       "                        [ 0.0730,  0.1294,  0.1378],\n",
       "                        [-0.0169, -0.0746, -0.0816]],\n",
       "              \n",
       "                       [[ 0.0389,  0.0496, -0.0165],\n",
       "                        [ 0.1249,  0.0191,  0.0284],\n",
       "                        [ 0.1653,  0.0833,  0.0912]],\n",
       "              \n",
       "                       [[-0.0023,  0.1798,  0.1027],\n",
       "                        [-0.0187,  0.0257,  0.0559],\n",
       "                        [ 0.0389,  0.0220, -0.0217]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0811,  0.0605,  0.0709],\n",
       "                        [-0.0063, -0.0554, -0.0178],\n",
       "                        [-0.0323, -0.0548, -0.0778]],\n",
       "              \n",
       "                       [[ 0.0766,  0.0812,  0.1239],\n",
       "                        [ 0.0490,  0.0838,  0.0445],\n",
       "                        [-0.0417,  0.0430, -0.0313]],\n",
       "              \n",
       "                       [[ 0.0398,  0.0680,  0.1484],\n",
       "                        [ 0.0247,  0.0051,  0.0182],\n",
       "                        [-0.0787, -0.0992, -0.0988]]]], device='mps:0')),\n",
       "             ('model.5.1.weight',\n",
       "              tensor([1.0140, 0.9736, 1.2556, 1.2318, 0.9264, 1.0344, 1.0676, 1.1272, 1.1088,\n",
       "                      1.0506, 1.2019, 1.1125, 1.0218, 1.0692, 1.1038, 1.3065, 0.9893, 1.0874,\n",
       "                      1.0050, 1.0805, 0.9795, 0.9976, 1.1720, 1.1852, 0.9186, 1.0760, 1.0798,\n",
       "                      1.0449, 0.9614, 0.9175, 1.2760, 1.0854, 1.2161, 1.1676, 1.0621, 1.0518,\n",
       "                      0.9884, 0.8626, 1.1106, 1.1950, 1.1592, 1.0935, 1.1499, 1.0638, 0.9965,\n",
       "                      0.9562, 1.0989, 1.3429, 0.9100, 1.0978, 1.0770, 0.9810, 1.0555, 1.1279,\n",
       "                      1.2253, 0.9577, 1.0505, 0.9827, 1.0881, 0.9525, 0.7627, 1.1386, 1.1727,\n",
       "                      0.9317, 1.0694, 1.1305, 1.1657, 1.1649, 1.0793, 0.9340, 1.0519, 1.2099,\n",
       "                      1.0100, 0.8113, 1.1334, 0.8933, 1.2030, 1.1499, 1.0374, 1.0672, 0.8542,\n",
       "                      1.1928, 1.1136, 1.2677, 1.2306, 1.0328, 1.0910, 1.1113, 1.1547, 1.1631,\n",
       "                      1.0613, 1.0297, 0.9618, 0.9999, 1.2014, 1.0486, 1.1726, 1.0459, 1.0579,\n",
       "                      1.1641, 1.1641, 1.0820, 1.1023, 1.1828, 1.0458, 1.2258, 0.9245, 0.9279,\n",
       "                      1.0948, 1.1835, 1.1201, 0.8823, 0.9543, 1.0699, 1.1234, 1.1590, 0.7684,\n",
       "                      1.1993, 1.0217, 1.1087, 1.0649, 1.2777, 1.1501, 1.1071, 0.9283, 1.1411,\n",
       "                      0.8879, 0.9850], device='mps:0')),\n",
       "             ('model.5.1.bias',\n",
       "              tensor([-0.5263, -0.5478, -0.4994, -0.5335, -0.5097, -0.5218, -0.4032, -0.6312,\n",
       "                      -0.5050, -0.5273, -0.6893, -0.5927, -0.6077, -0.4639, -0.5039, -0.6330,\n",
       "                      -0.7610, -0.4761, -0.3610, -0.3871, -0.4580, -0.5331, -0.5201, -0.6251,\n",
       "                      -0.6084, -0.5871, -0.5085, -0.4806, -0.5408, -0.4773, -0.3139, -0.5220,\n",
       "                      -0.2583, -0.5592, -0.5345, -0.4714, -0.5243, -0.4496, -0.5270, -0.4659,\n",
       "                      -0.2209, -0.2945, -0.5870, -0.4366, -0.4439, -0.4272, -0.7581, -0.2973,\n",
       "                      -0.3479, -0.3981, -0.6374, -0.1865, -0.4967, -0.4857, -0.5466, -0.3689,\n",
       "                      -0.7173, -0.6589, -0.3944, -0.4567, -0.3389, -0.4487, -0.3903, -0.5621,\n",
       "                      -0.4034, -0.4803, -0.3478, -1.0259, -0.4243, -0.5426, -0.5957, -0.3485,\n",
       "                      -0.4469, -0.3792, -0.5411, -0.5224, -0.6441, -0.4373, -0.4811, -0.4358,\n",
       "                      -0.5821, -0.4319, -0.5532, -0.4229, -0.4360, -0.4681, -0.3269, -0.3979,\n",
       "                      -0.4030, -0.6962, -0.2599, -0.7554, -0.4021, -0.6306, -0.5172, -0.6080,\n",
       "                      -0.4151, -0.4502, -0.8285, -0.3271, -0.6280, -0.4987, -0.6136, -0.4464,\n",
       "                      -0.6666, -0.4909, -0.5058, -0.5042, -0.6150, -0.2558, -0.1543, -0.5681,\n",
       "                      -0.6638, -0.6351, -0.4955, -0.1856, -0.5583, -0.4768, -0.4187, -0.4695,\n",
       "                      -0.4704, -0.2672, -0.3975, -0.3925, -0.6803, -0.3118, -0.5160, -0.5535],\n",
       "                     device='mps:0')),\n",
       "             ('model.5.1.running_mean',\n",
       "              tensor([-0.9107, -1.7900, -2.0575, -2.7286, -0.1095, -4.3137, -0.7282, -0.0096,\n",
       "                      -2.2766, -3.7708, -3.2161,  0.1168, -1.8256,  0.5896, -1.2735, -2.5248,\n",
       "                       1.2874, -2.0714, -2.6182, -2.3522, -3.3696, -2.6149, -0.5353, -3.3668,\n",
       "                      -2.8591, -3.8399, -1.3838, -2.3339, -1.2429, -1.3038, -3.5862, -1.1368,\n",
       "                      -4.6798, -2.1754, -4.7398, -1.4704, -2.6670, -2.0895, -2.3351, -1.3321,\n",
       "                      -4.8913, -0.7241, -3.9685, -0.8153, -2.1489, -2.9554,  2.5546,  0.2889,\n",
       "                      -2.7581, -1.6367, -0.7957, -0.5689, -0.1830, -3.8407, -2.0927, -2.3386,\n",
       "                      -0.4910,  0.0958, -0.3657, -3.8049, -2.1003, -1.2631, -1.6398, -0.2197,\n",
       "                      -2.9286, -2.4470, -1.7715, -3.3434, -2.2361, -2.5269, -2.1939, -0.5478,\n",
       "                       0.4924, -2.8499, -2.9747, -2.8869, -2.6982,  1.1808,  1.0057, -1.9344,\n",
       "                       1.7154, -2.2927, -1.4055, -0.2220,  1.8079, -0.3228, -4.7191, -1.6164,\n",
       "                      -2.1280, -1.7292, -3.2136, -0.7913, -1.8100,  0.6174,  2.6839,  0.7837,\n",
       "                      -1.4608, -3.4953, -2.9808, -2.2763,  2.4667, -0.5746, -1.2432, -3.7246,\n",
       "                      -0.3427, -1.8846, -2.3365, -1.9829, -2.5499, -3.8720, -2.2094,  2.4115,\n",
       "                      -3.4715,  0.6986, -3.7998, -3.9402,  0.0683, -2.3245,  0.7748, -1.5087,\n",
       "                      -4.3211,  2.0422, -1.6110, -3.2429, -0.1015, -3.0634, -0.6176, -0.7073],\n",
       "                     device='mps:0')),\n",
       "             ('model.5.1.running_var',\n",
       "              tensor([ 7.3390,  5.8133, 10.4117,  6.9549,  5.5715,  9.2830,  7.1265,  8.9383,\n",
       "                       5.9799,  6.3383,  6.8513,  5.4686,  5.5909,  6.5746,  7.4847,  9.3103,\n",
       "                      10.4039,  6.8292,  9.1206,  8.1145,  5.5759,  8.7575,  8.0304,  7.1336,\n",
       "                       7.1955,  5.9291,  9.2876,  5.9773,  5.9917,  4.2327,  8.1502,  6.2169,\n",
       "                       9.4943,  8.1296,  7.5176,  6.9142,  7.2648,  8.0803,  5.7002,  7.5392,\n",
       "                       8.6073,  7.4584,  9.5716,  7.0691,  5.0327,  7.2253,  8.3937,  6.9662,\n",
       "                       7.4576,  6.9791,  5.7069,  6.6470,  8.0972,  7.9759,  7.3617,  4.6515,\n",
       "                       9.9394,  6.9997,  6.1799,  9.2857,  4.8129,  5.0629,  7.7315,  5.4794,\n",
       "                       7.8451,  6.2684,  6.5667,  8.8231, 12.3260,  5.6659,  7.8807,  7.3687,\n",
       "                       5.3254,  7.4585, 10.1799,  6.1135,  6.1652,  7.4181,  5.9595,  7.1828,\n",
       "                      10.6328,  9.4817,  6.8109,  7.7314,  8.8330,  7.3143,  7.2334,  5.0406,\n",
       "                       9.6885,  9.8645,  8.6359,  8.4416,  6.2059, 10.1041,  7.0868,  9.3789,\n",
       "                       8.1381, 10.0535,  7.0274,  5.5471,  5.2143,  6.0760,  8.7418,  9.1028,\n",
       "                       8.5876,  7.3243, 10.3937,  5.7336,  7.0564,  5.9117,  7.3540,  6.6826,\n",
       "                       8.1686,  6.7226,  6.6651,  7.4824,  5.7141,  8.1723,  5.5397,  7.0979,\n",
       "                       7.3226,  9.0407,  8.2133,  6.5011,  6.7883,  6.3399,  8.1782,  5.9610],\n",
       "                     device='mps:0')),\n",
       "             ('model.5.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.6.0.weight',\n",
       "              tensor([[[[-1.5354e-02,  1.6217e-02,  1.7414e-02],\n",
       "                        [ 1.6167e-02,  1.3051e-02,  1.5055e-02],\n",
       "                        [ 2.0788e-01,  1.7980e-01,  7.1811e-02]],\n",
       "              \n",
       "                       [[-1.3643e-01, -1.5158e-01, -1.0574e-01],\n",
       "                        [-1.3999e-01, -1.2414e-01, -8.7790e-02],\n",
       "                        [ 2.7520e-02,  4.3926e-02, -3.7491e-02]],\n",
       "              \n",
       "                       [[ 1.2896e-02,  3.6678e-02,  6.1837e-02],\n",
       "                        [-4.8332e-02, -1.4126e-02,  2.9848e-02],\n",
       "                        [-1.3820e-01, -1.3733e-01, -9.9609e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.9680e-02,  4.9325e-02,  3.0791e-02],\n",
       "                        [-1.2682e-02, -1.9302e-02, -1.4824e-03],\n",
       "                        [ 1.1458e-02,  4.3487e-02, -8.9041e-03]],\n",
       "              \n",
       "                       [[-1.6942e-01, -3.8658e-02, -3.2368e-02],\n",
       "                        [-5.1043e-02, -1.5225e-02, -7.8275e-02],\n",
       "                        [ 3.4903e-02, -2.3936e-02, -7.7602e-02]],\n",
       "              \n",
       "                       [[-2.6266e-02, -1.7725e-02,  7.9433e-02],\n",
       "                        [ 3.0650e-02, -7.7182e-05,  3.9189e-02],\n",
       "                        [ 8.2557e-02, -1.2076e-02,  2.5773e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 5.9670e-02,  1.3784e-01,  1.2194e-01],\n",
       "                        [ 2.8056e-03, -4.7721e-02, -2.3364e-02],\n",
       "                        [-1.3856e-01, -8.9754e-02, -1.1520e-01]],\n",
       "              \n",
       "                       [[-6.2918e-02, -1.9526e-02, -3.8300e-02],\n",
       "                        [-7.4681e-02, -7.5941e-02, -1.0325e-02],\n",
       "                        [-1.0021e-01,  8.1888e-03,  5.9245e-03]],\n",
       "              \n",
       "                       [[ 2.4499e-01,  1.9405e-01, -4.6967e-02],\n",
       "                        [ 5.3671e-02, -1.4612e-02, -5.6238e-02],\n",
       "                        [ 1.7039e-02, -6.5469e-02,  2.0335e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3782e-01, -5.5361e-02, -5.8022e-02],\n",
       "                        [-2.7038e-02, -1.2664e-01, -9.8652e-02],\n",
       "                        [-6.0174e-02, -2.7194e-02,  8.4918e-03]],\n",
       "              \n",
       "                       [[ 4.5778e-02,  3.2863e-02, -8.2634e-03],\n",
       "                        [-1.1836e-01, -1.0031e-02,  1.4164e-01],\n",
       "                        [-1.2959e-01, -5.9157e-02,  5.2683e-02]],\n",
       "              \n",
       "                       [[-1.0234e-01, -7.7967e-02, -8.1150e-02],\n",
       "                        [-7.8564e-02, -9.0317e-02, -7.2680e-02],\n",
       "                        [ 3.7609e-02,  3.1480e-02, -2.8134e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.7578e-03, -3.5649e-02, -4.9075e-02],\n",
       "                        [-5.6350e-02, -6.6085e-02, -7.1923e-02],\n",
       "                        [-5.3546e-02,  1.1262e-01, -2.7459e-02]],\n",
       "              \n",
       "                       [[-4.5255e-03, -1.8942e-02, -8.3203e-02],\n",
       "                        [ 2.4911e-02, -1.0033e-01, -1.1002e-01],\n",
       "                        [-3.1624e-03,  5.0052e-02,  5.3223e-02]],\n",
       "              \n",
       "                       [[-3.2017e-02,  1.0766e-01, -3.9899e-03],\n",
       "                        [-7.0967e-02,  8.7982e-02, -3.3040e-02],\n",
       "                        [-1.0673e-01, -5.3850e-02, -7.4965e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.9319e-02, -6.7305e-02, -3.0483e-02],\n",
       "                        [ 1.2687e-02, -1.5986e-01, -2.8070e-02],\n",
       "                        [ 1.0238e-01,  5.7692e-02,  5.8612e-02]],\n",
       "              \n",
       "                       [[-3.9243e-02, -5.6864e-02, -3.9086e-02],\n",
       "                        [ 5.4323e-03, -5.7328e-02, -8.1355e-02],\n",
       "                        [ 1.2421e-01, -7.4687e-02, -7.1855e-02]],\n",
       "              \n",
       "                       [[ 5.9919e-02,  1.8137e-02,  1.9677e-02],\n",
       "                        [ 8.8455e-02,  1.2633e-01,  1.1621e-01],\n",
       "                        [ 7.9000e-02,  8.2516e-02,  5.1222e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.0566e-02, -3.5078e-02, -2.1313e-02],\n",
       "                        [ 4.2362e-02,  7.4342e-02, -1.2727e-02],\n",
       "                        [ 6.6565e-02,  7.7948e-03, -2.2316e-02]],\n",
       "              \n",
       "                       [[ 2.3791e-02, -2.3853e-02,  3.1778e-02],\n",
       "                        [ 8.4099e-02, -1.8164e-02,  6.9623e-02],\n",
       "                        [ 5.9658e-02, -1.9268e-02,  3.2313e-02]],\n",
       "              \n",
       "                       [[-7.3192e-02, -1.8820e-02,  4.4859e-02],\n",
       "                        [-1.1540e-01, -1.8180e-01, -4.8628e-02],\n",
       "                        [-2.4636e-01, -1.1763e-01,  5.9367e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.5585e-03,  1.8020e-03,  1.1613e-01],\n",
       "                        [-4.1104e-02, -3.0720e-02, -1.4741e-01],\n",
       "                        [ 4.5172e-02, -2.4631e-02, -6.5192e-02]],\n",
       "              \n",
       "                       [[-3.2372e-02, -7.7876e-02,  6.7020e-02],\n",
       "                        [-3.5187e-02, -1.1203e-01,  2.6171e-03],\n",
       "                        [-1.4505e-01, -2.3333e-01, -7.9461e-02]],\n",
       "              \n",
       "                       [[-8.0288e-02, -9.9464e-02, -7.6488e-02],\n",
       "                        [-7.1549e-02, -1.6537e-01, -1.3707e-01],\n",
       "                        [-7.4038e-02, -1.8226e-01, -1.5557e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.0025e-02, -3.4925e-02, -4.1067e-03],\n",
       "                        [ 6.1060e-02, -2.3774e-02,  5.7831e-03],\n",
       "                        [ 1.0447e-02, -3.6296e-02, -5.6996e-02]],\n",
       "              \n",
       "                       [[-5.4087e-02, -6.1758e-02, -7.7486e-02],\n",
       "                        [-1.3123e-01, -7.0396e-02, -1.5892e-01],\n",
       "                        [ 6.0707e-02,  5.0854e-02,  1.7449e-02]],\n",
       "              \n",
       "                       [[ 6.3034e-03, -1.1662e-01, -1.0970e-01],\n",
       "                        [ 1.1141e-02, -1.1890e-01, -4.3812e-02],\n",
       "                        [-9.6343e-02,  5.3895e-03, -2.7611e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.3115e-02,  4.9802e-02,  2.5395e-02],\n",
       "                        [-1.0413e-01, -6.8861e-02, -8.5125e-02],\n",
       "                        [ 8.5972e-02,  2.2125e-02, -6.6227e-02]],\n",
       "              \n",
       "                       [[-1.3475e-01, -7.7926e-02, -4.3808e-02],\n",
       "                        [-1.0636e-01, -1.7218e-01, -1.7948e-01],\n",
       "                        [ 2.7263e-05,  4.1266e-02,  1.5567e-03]],\n",
       "              \n",
       "                       [[ 5.4045e-02,  8.2080e-02,  4.9001e-02],\n",
       "                        [ 3.8261e-02,  4.8881e-02,  6.1113e-02],\n",
       "                        [-1.2305e-02, -4.0608e-02,  3.3018e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.3554e-02,  9.0679e-02, -5.5785e-02],\n",
       "                        [-1.0929e-02,  9.2945e-02, -2.4075e-02],\n",
       "                        [-1.9748e-02,  1.9609e-02, -2.2558e-02]],\n",
       "              \n",
       "                       [[-3.1773e-02, -4.9519e-02, -3.5621e-02],\n",
       "                        [ 1.4005e-02,  1.8999e-02,  5.4526e-02],\n",
       "                        [-3.9120e-02, -7.5056e-03,  6.6775e-03]],\n",
       "              \n",
       "                       [[-9.3685e-02,  5.9152e-02,  1.0313e-01],\n",
       "                        [-1.4960e-01, -1.9510e-02,  2.0944e-01],\n",
       "                        [-2.1063e-01, -8.7310e-02,  1.8700e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.8574e-02,  9.2517e-03,  1.8408e-02],\n",
       "                        [ 6.4905e-02, -2.0691e-02,  2.5479e-02],\n",
       "                        [-1.2619e-01, -1.2009e-01, -1.3951e-01]],\n",
       "              \n",
       "                       [[ 3.8722e-02,  8.0839e-02,  4.4349e-02],\n",
       "                        [ 5.8926e-02,  1.0239e-01,  1.5708e-01],\n",
       "                        [-6.2716e-02, -6.4815e-02,  1.6227e-02]],\n",
       "              \n",
       "                       [[-7.1990e-02, -1.5712e-02, -4.2307e-02],\n",
       "                        [-4.4866e-02, -7.8299e-03, -9.6548e-03],\n",
       "                        [-2.9060e-02, -4.8894e-02, -1.8387e-02]]]], device='mps:0')),\n",
       "             ('model.6.1.weight',\n",
       "              tensor([1.1526, 1.1482, 0.9900, 1.1343, 1.1138, 1.2443, 1.1020, 1.0665, 0.8731,\n",
       "                      0.8043, 0.7415, 0.7914, 0.5157, 1.1658, 1.1919, 1.0789, 1.1050, 1.0320,\n",
       "                      1.0102, 0.9659, 1.1503, 1.0771, 1.1378, 0.9645, 1.2206, 0.8819, 1.2276,\n",
       "                      1.0232, 0.9599, 1.0652, 1.0426, 0.8801, 0.9001, 1.0520, 1.2384, 1.0371,\n",
       "                      1.1459, 0.7239, 1.0081, 1.0407, 0.9410, 1.0131, 1.0289, 1.2335, 1.1129,\n",
       "                      1.1447, 0.8910, 0.9432, 1.1408, 1.4637, 1.1949, 0.9326, 1.2022, 0.9732,\n",
       "                      1.0157, 1.0846, 0.8173, 1.0330, 1.1555, 0.9685, 1.0584, 0.9141, 1.1383,\n",
       "                      0.8645, 1.0814, 1.0079, 1.1124, 1.1677, 0.9491, 1.0846, 1.1392, 1.0338,\n",
       "                      1.2344, 1.1792, 0.9903, 0.9972, 0.9688, 0.9192, 0.7430, 1.1353, 0.8268,\n",
       "                      0.9240, 1.1942, 0.9969, 0.8375, 1.0839, 1.2274, 0.9947, 1.0217, 1.0462,\n",
       "                      1.0816, 1.1179, 0.8991, 1.0624, 0.8386, 0.9563, 1.2655, 1.0359, 1.1616,\n",
       "                      0.9273, 1.1063, 1.2023, 1.1115, 0.8698, 0.9787, 1.3083, 0.9990, 1.0867,\n",
       "                      0.9825, 1.0406, 1.1095, 0.8490, 1.0567, 1.1508, 1.1538, 0.9430, 1.0294,\n",
       "                      0.9534, 1.1051, 0.9429, 1.1147, 1.2227, 1.2274, 1.2263, 1.0179, 1.2231,\n",
       "                      1.0968, 1.1759], device='mps:0')),\n",
       "             ('model.6.1.bias',\n",
       "              tensor([-0.9318, -0.8034, -0.7789, -0.7513, -0.5571, -0.6597, -0.5878, -0.4918,\n",
       "                      -0.6506, -0.6969, -0.6652, -0.7630, -0.5041, -0.5478, -0.9574, -0.9306,\n",
       "                      -0.6893, -0.6976, -0.7303, -0.4917, -0.6462, -0.8852, -0.5242, -0.6753,\n",
       "                      -0.7111, -0.5506, -0.6987, -0.6386, -0.6250, -0.7620, -0.8163, -0.5814,\n",
       "                      -0.7545, -0.8194, -0.8086, -0.9323, -0.7641, -0.8011, -0.6057, -0.7011,\n",
       "                      -0.5599, -0.3462, -0.8906, -0.9003, -0.9468, -0.9139, -0.6954, -0.8006,\n",
       "                      -0.6122, -0.5867, -0.7812, -0.7346, -0.8918, -0.9232, -1.0643, -0.7710,\n",
       "                      -0.7530, -0.3898, -1.1535, -0.7662, -0.9408, -0.5669, -0.7147, -0.7523,\n",
       "                      -0.7713, -0.6036, -0.9161, -0.8026, -0.6351, -0.5427, -0.6794, -0.8186,\n",
       "                      -0.6333, -0.6090, -0.8912, -0.7917, -0.7036, -1.0113, -0.6024, -0.7377,\n",
       "                      -0.4211, -0.9564, -0.6686, -0.4839, -0.8220, -0.8057, -0.6200, -0.6759,\n",
       "                      -0.7390, -0.6504, -0.6110, -0.7558, -0.8080, -0.8608, -0.6657, -0.5955,\n",
       "                      -0.7639, -0.7052, -0.8412, -0.8861, -0.7786, -0.8541, -0.7427, -0.9499,\n",
       "                      -0.7204, -0.4734, -0.8762, -0.6549, -0.3505, -0.6037, -0.7420, -0.5641,\n",
       "                      -0.7070, -0.4766, -0.5968, -0.8190, -0.6498, -0.8889, -0.7401, -0.7742,\n",
       "                      -0.5113, -1.0216, -0.6875, -0.8230, -0.7808, -0.5557, -0.7319, -0.4652],\n",
       "                     device='mps:0')),\n",
       "             ('model.6.1.running_mean',\n",
       "              tensor([-1.1957, -1.7162, -0.8805, -2.1526, -2.9462, -2.3159, -0.8520, -4.8048,\n",
       "                      -2.0099, -0.0535, -3.0584, -1.4745, -0.1293, -2.8202, -1.1799,  0.0596,\n",
       "                      -1.3392, -1.3800, -1.9197, -3.5598, -0.6140, -1.9829,  0.0196, -2.6455,\n",
       "                      -3.1544, -3.4206, -2.1984, -3.1482, -0.8784, -4.3819, -3.8112, -2.6930,\n",
       "                      -2.9225, -1.1028, -2.7071, -1.7872, -2.1325,  0.3216, -1.0077, -1.4445,\n",
       "                      -2.3025, -2.0434, -1.6410, -2.1695, -2.3755, -1.3966, -3.0577, -2.2409,\n",
       "                      -3.1820, -3.1014, -1.6404, -3.3476, -2.2201, -2.6425,  1.0515, -2.0435,\n",
       "                       0.0334, -1.6489, -2.9894, -1.1960, -1.9713, -2.0550, -2.0630, -0.3144,\n",
       "                      -1.0672, -2.1179, -2.4707, -2.9483, -1.2084, -2.1797, -2.5332, -1.1906,\n",
       "                      -3.4954, -1.5278, -1.6549, -2.7654, -1.4122, -0.8728, -1.4721, -3.4363,\n",
       "                      -1.8930, -1.9702, -2.5747, -2.6061, -2.1635, -2.2426, -1.8597, -2.9178,\n",
       "                      -0.4660, -2.7665, -2.6516, -0.7597, -0.1164, -1.0808,  0.3845, -2.0383,\n",
       "                      -2.1645, -1.3188, -1.8660, -0.5465, -2.2045, -0.8688, -1.1150, -0.5432,\n",
       "                      -0.4113, -1.6053, -1.5969, -2.0374, -1.9859,  1.2539, -1.9291, -0.4869,\n",
       "                      -1.6793, -4.4315, -1.9309, -1.2451, -3.1039, -2.3003, -1.8690, -2.2327,\n",
       "                      -2.8768, -1.5186, -1.6882, -2.8017, -2.7879, -2.2827, -1.1081, -3.1748],\n",
       "                     device='mps:0')),\n",
       "             ('model.6.1.running_var',\n",
       "              tensor([ 6.6900,  5.4153,  6.0425,  5.4622,  8.0007,  7.7510,  6.1009,  7.2614,\n",
       "                       6.6004,  4.3530,  4.5596,  6.4508,  4.9979,  6.1042,  6.9753,  5.7561,\n",
       "                       5.7045,  7.0969,  5.4256,  7.1702,  6.3720,  5.3390,  7.6964,  5.7568,\n",
       "                       6.3745,  5.8920,  6.8791,  6.0229,  4.6042,  7.5006,  4.8315,  4.6364,\n",
       "                       5.0875,  5.8688,  7.4879,  5.7091,  6.5328,  6.9680,  5.1269,  5.3722,\n",
       "                       4.9809,  5.9907,  6.4453,  7.4565,  7.1081,  6.4177,  5.9539,  5.9835,\n",
       "                       6.6002, 13.4296,  6.9831,  6.0318,  6.9048,  4.7802,  5.3641,  5.6029,\n",
       "                       5.6991,  7.9826,  7.0423,  4.4167,  5.2923,  5.5239,  4.8785,  4.5030,\n",
       "                       5.9117,  4.6106,  5.5053,  7.4638,  5.5717,  5.2481,  5.4679,  5.8507,\n",
       "                       9.4832,  8.5251,  6.4527,  5.7425,  5.9205,  4.2573,  3.8259,  5.9157,\n",
       "                       5.6662,  4.6673,  6.3395,  4.7957,  5.6240,  5.6875,  7.4769,  5.2389,\n",
       "                       5.7160,  4.2078,  5.1445,  4.5642,  4.6558,  5.3440,  3.0982,  4.9475,\n",
       "                       6.9748,  6.1372,  5.4026,  5.8553,  6.7545,  6.3827,  6.2735,  5.0943,\n",
       "                       6.2072,  8.3780,  5.7689,  5.5307,  6.3222,  5.5570,  4.8683,  4.8156,\n",
       "                       4.9667,  6.9944,  5.3638,  5.2753,  6.8807,  5.2042,  5.1856,  3.9278,\n",
       "                       6.9632,  6.9907,  8.6224,  7.0119,  6.3654,  8.0286,  5.5482,  7.5286],\n",
       "                     device='mps:0')),\n",
       "             ('model.6.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.7.0.weight',\n",
       "              tensor([[[[ 7.5910e-02,  5.4683e-02,  3.0322e-02],\n",
       "                        [-1.7837e-03,  8.4180e-03, -5.0583e-02],\n",
       "                        [-1.0774e-01, -1.1413e-01, -1.1907e-01]],\n",
       "              \n",
       "                       [[-9.7641e-02, -7.5787e-02, -8.8121e-02],\n",
       "                        [-8.6561e-02, -6.3873e-02, -4.0782e-02],\n",
       "                        [-2.1441e-02,  2.4564e-02,  3.9275e-02]],\n",
       "              \n",
       "                       [[-3.1951e-02, -6.4867e-02, -7.3506e-04],\n",
       "                        [-2.8872e-03, -5.3425e-02,  1.2965e-02],\n",
       "                        [-1.4857e-02, -3.1940e-02, -1.0621e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.3396e-03, -2.2119e-02,  3.3186e-02],\n",
       "                        [-1.3899e-02,  2.1387e-02,  5.2410e-02],\n",
       "                        [-4.3119e-02, -4.9171e-02, -3.2710e-02]],\n",
       "              \n",
       "                       [[-3.6841e-02, -2.6411e-02, -5.1245e-02],\n",
       "                        [ 2.5460e-02,  7.3194e-02,  5.7084e-02],\n",
       "                        [-1.8237e-02, -8.0592e-03, -1.5945e-02]],\n",
       "              \n",
       "                       [[ 6.8811e-02,  4.7476e-02,  5.9196e-02],\n",
       "                        [-1.5873e-02, -4.8520e-02, -6.2704e-02],\n",
       "                        [-7.4144e-02, -6.6195e-02, -8.0565e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.3267e-03,  2.6044e-02,  1.8906e-02],\n",
       "                        [ 2.1960e-02,  3.1476e-02,  2.7443e-02],\n",
       "                        [-6.0798e-02, -6.3141e-02, -4.1343e-02]],\n",
       "              \n",
       "                       [[-1.6539e-02,  2.1315e-02, -2.2696e-03],\n",
       "                        [ 1.2876e-02, -4.2154e-02, -3.8450e-02],\n",
       "                        [-4.3300e-02,  1.2805e-02,  7.3587e-03]],\n",
       "              \n",
       "                       [[-6.3485e-02, -5.3899e-02, -3.0412e-02],\n",
       "                        [ 1.5611e-02, -1.7121e-02,  8.8125e-04],\n",
       "                        [-2.2552e-02, -3.4309e-02, -2.8597e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.6796e-02,  3.2238e-02,  4.4980e-02],\n",
       "                        [ 1.9905e-02, -2.8269e-02,  8.0557e-03],\n",
       "                        [-8.8761e-02, -7.1015e-02, -1.0170e-01]],\n",
       "              \n",
       "                       [[-1.1105e-02, -5.7393e-02, -6.4641e-03],\n",
       "                        [-4.0826e-02, -6.1926e-02, -3.6526e-02],\n",
       "                        [-5.1630e-02, -8.0944e-02, -4.5080e-02]],\n",
       "              \n",
       "                       [[-6.4609e-02, -8.5225e-03, -5.9021e-02],\n",
       "                        [ 4.4836e-02,  7.5375e-02,  4.8544e-02],\n",
       "                        [ 4.1624e-02,  9.3464e-02,  6.7039e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.9389e-02,  4.8539e-02,  8.5840e-02],\n",
       "                        [ 2.9569e-02,  4.1249e-02,  1.7625e-02],\n",
       "                        [-4.5169e-02, -1.0071e-01, -6.0172e-02]],\n",
       "              \n",
       "                       [[-2.9260e-02, -5.4970e-02, -4.9870e-02],\n",
       "                        [-4.9780e-02, -6.6647e-03,  6.6983e-03],\n",
       "                        [-5.5037e-02, -6.4092e-03, -3.4609e-02]],\n",
       "              \n",
       "                       [[ 8.7412e-02,  5.5105e-02,  5.9010e-02],\n",
       "                        [-1.0163e-02, -1.3180e-02, -2.2225e-02],\n",
       "                        [-2.5894e-02, -6.8413e-02, -5.6265e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.0336e-02, -5.8663e-03,  2.5627e-02],\n",
       "                        [ 3.6439e-02,  1.4832e-02,  2.3641e-02],\n",
       "                        [ 7.8313e-03, -1.9043e-02, -1.9285e-02]],\n",
       "              \n",
       "                       [[ 1.5435e-01,  1.0475e-01,  1.3456e-01],\n",
       "                        [ 1.0886e-01,  4.9376e-02,  4.2451e-02],\n",
       "                        [-9.2743e-03, -7.0502e-02, -7.6000e-02]],\n",
       "              \n",
       "                       [[-7.6290e-03,  6.3104e-02, -3.5033e-03],\n",
       "                        [-3.8742e-02,  3.5503e-02, -4.9684e-02],\n",
       "                        [-1.0177e-01, -6.4619e-02, -1.5883e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 4.3049e-02,  6.1279e-02,  5.0074e-02],\n",
       "                        [-3.8527e-02, -2.3193e-02, -2.8086e-02],\n",
       "                        [-5.4679e-02, -2.0224e-02, -1.4098e-02]],\n",
       "              \n",
       "                       [[-3.7530e-02, -6.9741e-02, -2.3459e-02],\n",
       "                        [-1.9276e-02, -5.6814e-02, -1.2904e-02],\n",
       "                        [ 5.2592e-02,  3.3004e-02,  3.5216e-02]],\n",
       "              \n",
       "                       [[-5.8410e-02, -2.2886e-02, -3.2053e-02],\n",
       "                        [-7.3035e-03, -3.8817e-02, -8.1954e-03],\n",
       "                        [ 1.6932e-03,  1.3742e-02, -7.6392e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-4.0695e-03, -1.4148e-02, -1.2213e-04],\n",
       "                        [-5.4539e-02, -6.0491e-02, -2.1562e-02],\n",
       "                        [-5.4261e-02, -6.0061e-02, -6.2087e-02]],\n",
       "              \n",
       "                       [[-7.2051e-02, -3.3394e-02, -2.6836e-02],\n",
       "                        [-5.4487e-02, -1.2019e-02, -1.0087e-02],\n",
       "                        [ 7.0735e-04,  7.0171e-03,  3.7843e-02]],\n",
       "              \n",
       "                       [[ 1.0297e-01,  9.1523e-02,  6.1391e-02],\n",
       "                        [-3.2094e-02,  2.9106e-03, -8.3848e-03],\n",
       "                        [-5.2674e-02, -8.6778e-02, -6.8391e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.9346e-02,  6.1875e-02,  4.6642e-02],\n",
       "                        [-1.7912e-03,  1.3161e-03, -4.8518e-03],\n",
       "                        [-4.5249e-02,  2.4303e-03,  1.2281e-02]],\n",
       "              \n",
       "                       [[-2.4536e-02, -4.9064e-02, -3.0009e-02],\n",
       "                        [ 4.7805e-02,  3.2606e-02, -7.6464e-03],\n",
       "                        [ 4.1986e-02,  3.3052e-02,  2.1605e-02]],\n",
       "              \n",
       "                       [[-6.8668e-02, -8.0491e-02, -8.2970e-02],\n",
       "                        [-3.8623e-02, -6.5300e-02, -7.1152e-02],\n",
       "                        [-3.0703e-02, -8.3503e-04, -3.8969e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1149e-03,  3.9709e-02,  2.9492e-02],\n",
       "                        [-6.8064e-02, -7.1409e-02, -2.5037e-02],\n",
       "                        [-7.0903e-02, -8.9650e-02, -7.3290e-02]],\n",
       "              \n",
       "                       [[ 4.8375e-02, -1.3903e-02,  1.7467e-02],\n",
       "                        [ 3.8539e-02,  2.4640e-02,  3.2465e-02],\n",
       "                        [-3.8135e-02, -6.6060e-02, -3.0767e-02]],\n",
       "              \n",
       "                       [[-1.6285e-02,  2.4335e-02, -2.4263e-02],\n",
       "                        [-2.4966e-02,  2.6745e-02, -2.3130e-02],\n",
       "                        [-8.0837e-02, -7.0704e-02, -7.7369e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.3759e-02, -4.1179e-02, -4.3488e-02],\n",
       "                        [-3.0179e-02, -7.1476e-03, -2.5191e-02],\n",
       "                        [ 1.7340e-02, -3.1599e-02, -6.2961e-04]],\n",
       "              \n",
       "                       [[ 2.7178e-02,  2.3510e-02,  3.8324e-02],\n",
       "                        [ 2.5877e-02,  5.8817e-02,  9.2815e-02],\n",
       "                        [ 7.8795e-02,  1.0072e-01,  1.3112e-01]],\n",
       "              \n",
       "                       [[-2.8865e-02, -3.7765e-02,  2.8971e-02],\n",
       "                        [ 1.3437e-02,  3.8527e-02,  4.4278e-02],\n",
       "                        [-1.6616e-02, -4.3777e-03,  3.8411e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.4029e-03,  7.6663e-03,  1.8545e-02],\n",
       "                        [ 6.8405e-02,  1.0965e-01,  7.6022e-02],\n",
       "                        [ 7.9486e-02,  1.2414e-01,  7.4817e-02]],\n",
       "              \n",
       "                       [[-3.5782e-03,  1.2330e-02, -2.8389e-03],\n",
       "                        [ 3.3706e-03,  1.5497e-02,  1.7250e-02],\n",
       "                        [-2.3481e-02, -2.1112e-02, -8.3011e-03]],\n",
       "              \n",
       "                       [[ 6.6304e-02,  1.8096e-02,  6.3936e-02],\n",
       "                        [ 6.8328e-02,  5.8837e-02,  7.6811e-02],\n",
       "                        [-1.9997e-02, -1.7378e-02, -1.3286e-02]]]], device='mps:0')),\n",
       "             ('model.7.1.weight',\n",
       "              tensor([0.9675, 1.1227, 0.9505, 0.9792, 0.8670, 1.0075, 1.0217, 0.9969, 0.9976,\n",
       "                      0.8107, 0.9353, 0.9790, 1.0015, 0.9258, 1.0368, 0.8956, 0.9118, 0.9010,\n",
       "                      1.0990, 0.9334, 0.8525, 0.8062, 0.8805, 0.9170, 1.0043, 0.8684, 1.0201,\n",
       "                      1.0005, 1.0232, 0.9896, 0.8027, 0.8861, 0.9150, 0.9570, 0.8736, 0.7612,\n",
       "                      0.5939, 0.6253, 0.9191, 1.0288, 0.7436, 0.9745, 0.9506, 1.0051, 0.8694,\n",
       "                      0.9926, 0.8788, 0.9170, 0.8358, 0.9738, 0.8432, 1.0165, 0.9400, 0.7918,\n",
       "                      0.9296, 0.7039, 0.9315, 0.9786, 0.5991, 0.6464, 0.7739, 0.8393, 0.8832,\n",
       "                      0.9610, 0.8336, 1.0620, 0.9754, 1.1158, 0.9440, 0.8312, 0.8923, 0.7150,\n",
       "                      1.0553, 0.7510, 1.0099, 1.0093, 0.7670, 1.1920, 0.7898, 1.0881, 0.8562,\n",
       "                      0.9329, 1.0688, 0.9703, 0.6822, 0.8515, 0.9583, 0.6701, 0.7661, 0.7491,\n",
       "                      0.6676, 0.7705, 0.9715, 0.8901, 0.9341, 0.8181, 0.9634, 0.9325, 0.8489,\n",
       "                      0.9321, 0.9530, 0.9211, 1.0159, 0.6856, 0.9134, 0.6189, 1.1494, 0.9075,\n",
       "                      0.9497, 0.8040, 0.9941, 1.0023, 0.6952, 0.6142, 0.7752, 0.8037, 1.0146,\n",
       "                      0.6795, 0.8229, 0.8837, 0.7624, 1.0425, 1.1014, 0.9375, 1.0194, 1.0619,\n",
       "                      0.8820, 1.0318, 0.9902, 0.8797, 1.0018, 0.8912, 0.7288, 0.7511, 1.0302,\n",
       "                      1.0637, 0.8714, 0.9398, 0.7332, 0.7498, 0.7444, 0.9964, 1.0121, 1.0214,\n",
       "                      1.0661, 1.1010, 0.9395, 0.8545, 0.6963, 0.9288, 0.8703, 0.7366, 0.8033,\n",
       "                      1.0432, 0.9577, 1.0301, 1.0357, 1.1212, 0.9146, 0.8835, 0.7933, 0.7355,\n",
       "                      0.6121, 0.8581, 0.9033, 0.6410, 0.9294, 0.7579, 0.9763, 0.6196, 0.8305,\n",
       "                      0.9989, 0.7367, 0.7182, 0.8106, 1.0546, 0.9253, 0.8100, 0.9875, 0.7990,\n",
       "                      0.9309, 0.7698, 0.9340, 1.1791, 0.9321, 1.0301, 0.8304, 0.8540, 1.0163,\n",
       "                      0.8360, 0.7153, 1.0644, 0.7483, 0.8535, 1.0328, 0.8178, 0.6414, 0.8936,\n",
       "                      0.9257, 1.0292, 1.1175, 0.9245, 0.9438, 1.1169, 1.0007, 1.0611, 0.8198,\n",
       "                      0.6578, 1.1177, 0.9248, 0.8831, 0.9267, 0.8492, 0.9321, 0.9184, 0.6631,\n",
       "                      1.0676, 0.8431, 0.9155, 0.9293, 1.0957, 0.8003, 0.9349, 0.9939, 0.8517,\n",
       "                      0.7088, 1.0490, 0.8172, 0.8282, 0.9732, 0.9923, 1.0389, 0.9256, 1.0332,\n",
       "                      1.0361, 0.9681, 0.9311, 1.0209, 0.8738, 0.9698, 0.9123, 0.9389, 0.8953,\n",
       "                      0.9298, 1.1734, 0.9290, 0.9590, 0.6831, 0.8074, 0.5237, 0.8675, 0.8996,\n",
       "                      1.0215, 1.1326, 0.9885, 0.9673], device='mps:0')),\n",
       "             ('model.7.1.bias',\n",
       "              tensor([-0.3631, -0.2535, -0.4933, -0.2911, -0.5154, -0.3594, -0.3964, -0.5251,\n",
       "                      -0.5125, -0.4565, -0.4301, -0.4195, -0.3274, -0.4769, -0.4231, -0.2692,\n",
       "                      -0.6417, -0.5386, -0.7778, -0.4097, -0.5882, -0.3741, -0.4079, -0.4366,\n",
       "                      -0.3672, -0.6616, -0.4605, -0.5837, -0.4053, -0.5818, -0.6013, -0.4677,\n",
       "                      -0.6780, -0.4626, -0.3418, -0.4844, -0.3988, -0.2979, -0.7535, -0.4268,\n",
       "                      -0.4917, -0.3264, -0.4143, -0.4697, -0.5850, -0.3086, -0.3711, -0.6802,\n",
       "                      -0.4121, -0.6085, -0.2482, -0.4463, -0.8062, -0.4348, -0.5698, -0.5209,\n",
       "                      -0.3616, -0.4922, -0.2932, -0.3203, -0.6081, -0.7090, -0.5163, -0.5407,\n",
       "                      -0.5896, -0.5831, -0.6676, -0.5157, -0.7168, -0.3995, -0.6961, -0.4851,\n",
       "                      -0.4681, -0.3496, -0.2774, -0.2923, -0.4847, -0.2635, -0.4703, -0.4654,\n",
       "                      -0.5551, -0.5421, -0.5531, -0.2778, -0.4615, -0.4270, -0.5681, -0.4005,\n",
       "                      -0.5452, -0.5952, -0.5463, -0.3357, -0.6981, -0.6524, -0.4547, -0.5307,\n",
       "                      -0.3002, -0.8021, -0.4798, -0.6531, -0.3885, -0.4805, -0.3084, -0.3684,\n",
       "                      -0.6540, -0.4146, -0.4140, -0.4019, -0.7932, -0.5788, -0.3240, -0.7795,\n",
       "                      -0.5478, -0.3913, -0.6161, -0.4759, -0.1447, -0.5430, -0.5702, -0.4835,\n",
       "                      -0.5187, -0.4749, -0.2456, -0.3872, -0.6612, -0.4473, -0.5705, -0.5146,\n",
       "                      -0.6018, -0.4247, -0.6101, -0.6984, -0.5798, -0.4584, -0.4559, -0.5827,\n",
       "                      -0.4955, -0.5527, -0.6057, -0.5652, -0.4539, -0.6581, -0.4583, -0.6554,\n",
       "                      -0.3348, -0.3056, -0.6021, -0.5355, -0.4436, -0.4183, -0.4224, -0.4843,\n",
       "                      -0.4990, -0.4588, -0.5205, -0.3869, -0.8551, -0.5169, -0.5031, -0.4082,\n",
       "                      -0.5333, -0.4860, -0.5533, -0.4498, -0.5693, -0.4342, -0.4153, -0.3920,\n",
       "                      -0.4110, -0.3813, -0.6381, -0.3633, -0.4350, -0.5351, -0.6140, -0.5995,\n",
       "                      -0.6324, -0.5977, -0.6879, -0.4875, -0.6665, -0.4264, -0.6529, -0.7651,\n",
       "                      -0.5679, -0.4144, -0.6439, -0.5457, -0.3782, -0.4351, -0.5073, -0.5471,\n",
       "                      -0.3623, -0.3947, -0.2191, -0.6344, -0.4285, -0.5190, -0.6481, -0.4989,\n",
       "                      -0.7070, -0.5802, -0.4934, -0.2108, -0.6921, -0.5497, -0.3981, -0.4961,\n",
       "                      -0.2869, -0.4953, -0.3909, -0.7479, -0.7506, -0.5584, -0.7428, -0.5841,\n",
       "                      -0.1845, -0.6970, -0.5349, -0.5148, -0.5365, -0.6355, -0.5170, -0.5247,\n",
       "                      -0.5758, -0.4382, -0.7726, -0.6694, -0.5079, -0.5408, -0.6548, -0.6612,\n",
       "                      -0.3504, -0.2096, -0.3456, -0.2435, -0.5484, -0.5911, -0.5124, -0.4016,\n",
       "                      -0.5403, -0.2215, -0.4316, -0.4712, -0.5997, -0.5339, -0.5466, -0.5285,\n",
       "                      -0.4417, -0.3767, -0.4852, -0.3781, -0.5914, -0.3974, -0.3611, -0.4404],\n",
       "                     device='mps:0')),\n",
       "             ('model.7.1.running_mean',\n",
       "              tensor([-0.6555, -1.6841, -0.8074, -1.8573, -1.3348, -1.2559, -1.0963, -0.7839,\n",
       "                      -0.7733, -0.3453, -1.9863, -1.3936, -2.0823, -0.3456, -1.2113, -1.0551,\n",
       "                      -1.2143, -1.3734, -0.4885, -1.4554, -1.1317, -0.8874, -2.0634, -0.9569,\n",
       "                      -2.2711, -1.9078, -0.8468, -0.6898, -2.6416, -0.7960, -0.6104, -1.2734,\n",
       "                      -0.6714, -1.8492, -1.5798, -1.1345, -0.3184, -0.9569, -0.3854, -0.5022,\n",
       "                      -0.4822, -1.2595, -0.3874, -1.8064, -0.9220, -1.4273, -1.9071, -1.5201,\n",
       "                      -1.4767, -1.8050, -1.8924, -0.5506, -1.1172, -1.4393,  0.0544, -0.9915,\n",
       "                      -1.0812, -1.6903, -1.3242, -1.1250, -0.9423, -0.7232, -0.8183, -0.4369,\n",
       "                      -0.4872, -1.5214, -0.8619, -1.0103, -0.5936, -1.5708, -1.5443, -1.3813,\n",
       "                      -1.2713, -1.5322, -1.2358, -2.0794, -1.1028, -0.6411, -0.6882, -0.7965,\n",
       "                      -0.8932, -1.0523, -1.8078, -1.5494, -0.2682, -1.0592, -2.3417, -1.0728,\n",
       "                      -1.2573, -0.6237, -0.8770, -1.2095, -1.5869, -0.5884, -1.2690, -0.8940,\n",
       "                      -1.0300, -1.4574,  0.9158, -1.8267, -0.4450, -1.2022, -2.2810, -0.1987,\n",
       "                      -0.5187, -0.5227, -0.3335, -1.5320, -0.4255,  0.0416, -0.1532, -1.2937,\n",
       "                      -0.9074, -0.7648, -1.1885, -0.4124, -1.3946, -1.1673, -0.5521, -1.0891,\n",
       "                      -0.6600, -1.2950, -1.2913, -2.2490, -0.4895, -1.1671, -1.6298, -0.8405,\n",
       "                      -0.5882, -2.2902, -0.7467, -1.2419, -0.7930, -1.0105, -1.5396, -1.3559,\n",
       "                       0.2114, -0.8757, -0.5745, -0.8246, -0.7537, -1.7519, -0.8622, -1.8593,\n",
       "                      -0.6234, -1.4580, -1.2905, -1.5350, -1.2270, -1.4816, -1.1138, -1.2412,\n",
       "                      -1.4718, -1.5491, -1.7276, -1.2614, -0.8606,  0.2111, -0.8306, -0.2578,\n",
       "                      -1.1774, -0.1454,  0.0453, -0.8885, -1.2955, -1.4122, -1.5175, -1.3313,\n",
       "                      -0.4546, -0.7996, -1.3150, -1.2654, -1.2870, -0.6461, -1.0462, -0.8878,\n",
       "                      -0.8947, -0.8530, -1.0887, -0.6492, -0.3391, -0.4926, -1.0511, -0.8236,\n",
       "                      -0.6541, -1.5487, -1.4966, -0.1163,  0.2459, -1.2538, -1.1806, -0.6739,\n",
       "                      -2.0794, -1.0592, -1.8548, -0.7669, -1.0098, -0.9189, -1.9149, -0.5218,\n",
       "                      -1.2524,  0.3932, -0.0250, -1.2860, -1.1513, -1.0084, -0.7118, -0.8175,\n",
       "                      -1.7773, -0.7762, -1.2448, -1.2721, -0.7212, -1.0295,  0.2190, -0.5115,\n",
       "                      -1.6553, -0.2365, -1.6499, -1.6393, -1.5411, -0.4533, -1.7174, -1.1422,\n",
       "                      -1.2922, -0.0932, -0.8124, -0.8142, -0.1430, -1.6784, -1.5711, -3.0365,\n",
       "                      -2.3976, -2.0246, -0.4032, -0.7458, -1.4214, -0.6815, -0.7927, -0.8775,\n",
       "                      -1.5740, -1.6397, -1.3682, -0.4840, -1.1867, -1.9936, -1.9949, -1.4031,\n",
       "                      -1.0557, -0.6072, -1.0122, -1.2556, -1.7868, -1.3698, -0.8597,  0.1704],\n",
       "                     device='mps:0')),\n",
       "             ('model.7.1.running_var',\n",
       "              tensor([0.9829, 1.4027, 1.0510, 1.7831, 1.1523, 1.5572, 1.0334, 1.2622, 1.6725,\n",
       "                      0.8937, 1.4009, 1.2957, 1.1115, 1.3498, 1.1917, 1.1282, 1.3961, 1.3674,\n",
       "                      1.9431, 1.0785, 1.2682, 1.1357, 1.3984, 1.6072, 1.3146, 1.5860, 1.3781,\n",
       "                      1.1044, 2.1857, 1.2687, 1.4049, 1.2430, 1.1573, 1.2344, 1.0413, 1.0435,\n",
       "                      1.2424, 1.3175, 1.3974, 1.1450, 1.2048, 1.1545, 1.1807, 1.5916, 1.2088,\n",
       "                      1.3017, 1.2250, 1.4039, 1.0721, 1.2330, 1.6546, 1.1391, 1.5601, 1.0400,\n",
       "                      1.4962, 0.9934, 0.8599, 1.1104, 1.1557, 1.1937, 0.8430, 1.4038, 1.0378,\n",
       "                      1.2170, 1.3056, 1.6450, 1.2347, 1.4771, 1.4288, 1.2105, 1.7176, 1.2423,\n",
       "                      1.7317, 1.1736, 1.0647, 1.4809, 1.0097, 1.3838, 1.6264, 1.2863, 1.1015,\n",
       "                      0.9013, 1.8438, 1.2961, 1.0801, 1.3217, 1.4647, 1.3246, 1.2514, 0.9844,\n",
       "                      1.1939, 1.2877, 1.2311, 1.5338, 1.1197, 1.4086, 1.1582, 1.6622, 1.0676,\n",
       "                      1.3275, 0.9435, 1.0982, 1.4205, 0.8226, 1.5027, 1.7053, 2.0806, 1.2883,\n",
       "                      1.4541, 0.9824, 0.9909, 1.6238, 1.8280, 1.6758, 1.5779, 1.0293, 1.2719,\n",
       "                      1.0734, 1.1838, 0.8859, 1.6214, 1.4890, 1.0071, 1.4748, 1.8360, 1.1294,\n",
       "                      1.6289, 1.4353, 1.3130, 1.7049, 1.5616, 1.2752, 1.2518, 0.8671, 1.3156,\n",
       "                      1.5499, 1.0464, 1.4211, 1.7135, 1.9433, 0.9475, 1.4147, 1.0183, 1.5593,\n",
       "                      1.3289, 1.3351, 1.1505, 1.6158, 1.4333, 1.3074, 1.3580, 1.4599, 1.2217,\n",
       "                      1.3432, 1.5867, 1.3102, 1.4224, 1.4816, 1.7894, 1.0682, 1.6374, 0.8694,\n",
       "                      1.5599, 0.8973, 1.6104, 1.1903, 0.9622, 0.9628, 1.0339, 1.2180, 1.4436,\n",
       "                      1.5732, 1.3914, 1.2750, 1.2889, 1.5073, 1.5404, 1.2277, 1.8020, 1.1398,\n",
       "                      1.4426, 0.9158, 1.8035, 2.1949, 1.1733, 1.3180, 1.5100, 1.2248, 1.2223,\n",
       "                      1.0844, 0.8742, 1.5064, 1.5284, 1.8062, 1.6403, 1.2048, 1.2073, 1.3405,\n",
       "                      1.2085, 1.1214, 1.9673, 1.7692, 1.0907, 1.2806, 1.1829, 1.4972, 0.9866,\n",
       "                      1.2293, 1.7453, 1.0302, 1.0799, 1.2489, 1.2640, 1.3757, 1.3127, 1.2886,\n",
       "                      1.6026, 1.7580, 1.1661, 1.2047, 1.4451, 1.2550, 1.4218, 1.6187, 1.5248,\n",
       "                      1.1563, 1.9787, 1.4328, 0.9431, 1.2193, 1.4249, 2.3322, 1.3269, 1.5257,\n",
       "                      1.3408, 0.8798, 1.1422, 1.1257, 1.1731, 1.2861, 1.4049, 0.8277, 1.3222,\n",
       "                      1.3853, 2.1064, 1.3436, 1.5919, 0.9468, 1.4868, 1.2504, 0.9047, 1.0434,\n",
       "                      1.3671, 1.2401, 1.1654, 1.9629], device='mps:0')),\n",
       "             ('model.7.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('model.8.0.weight',\n",
       "              tensor([[[[-3.0168e-02, -7.6708e-03, -1.5573e-02],\n",
       "                        [-9.8868e-03, -3.2023e-02, -1.9806e-02],\n",
       "                        [ 5.8903e-03, -1.3656e-03, -1.0718e-02]],\n",
       "              \n",
       "                       [[ 3.0271e-02,  6.4691e-03,  1.2573e-02],\n",
       "                        [-2.1954e-02, -2.4908e-02, -2.5616e-03],\n",
       "                        [-4.2953e-02, -3.9630e-02, -2.5609e-02]],\n",
       "              \n",
       "                       [[-2.7080e-02,  1.0444e-02,  1.6147e-02],\n",
       "                        [-4.1309e-02, -2.4608e-02, -3.2125e-02],\n",
       "                        [-4.9239e-02, -4.2255e-02, -5.4569e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.1926e-02, -2.0021e-02, -4.7760e-02],\n",
       "                        [-4.8920e-03, -2.8714e-02, -2.6230e-02],\n",
       "                        [-2.8033e-02,  7.1174e-04, -2.9601e-02]],\n",
       "              \n",
       "                       [[-3.2270e-02, -1.6481e-02, -1.1581e-02],\n",
       "                        [ 1.0719e-02, -9.6610e-03, -3.9159e-02],\n",
       "                        [ 2.8188e-02, -4.7967e-03, -7.2376e-03]],\n",
       "              \n",
       "                       [[ 6.9307e-03,  1.4343e-02,  1.5391e-03],\n",
       "                        [ 1.8817e-02,  7.5693e-03,  2.3330e-02],\n",
       "                        [ 4.7858e-03,  1.6419e-02,  7.0070e-06]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 7.9061e-03, -1.2735e-02, -1.6869e-03],\n",
       "                        [ 5.6892e-03, -2.4985e-02, -6.2826e-03],\n",
       "                        [ 6.2254e-03, -3.5305e-02, -1.7403e-02]],\n",
       "              \n",
       "                       [[ 4.8222e-03,  3.6315e-04,  4.4588e-03],\n",
       "                        [-3.1164e-02, -4.4497e-02, -1.5847e-02],\n",
       "                        [-2.0965e-02, -3.9476e-03, -1.7445e-02]],\n",
       "              \n",
       "                       [[ 5.3467e-03,  2.4315e-02,  7.4126e-03],\n",
       "                        [-8.2150e-03, -1.5851e-02, -6.2708e-03],\n",
       "                        [-3.6171e-02, -1.2939e-03, -1.4976e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-7.3617e-03,  7.1514e-04,  1.7155e-02],\n",
       "                        [-1.0853e-02, -1.2416e-02, -1.8132e-02],\n",
       "                        [-1.4768e-02, -1.0473e-02, -2.8021e-02]],\n",
       "              \n",
       "                       [[-3.8187e-03,  2.2462e-02,  2.8636e-02],\n",
       "                        [-6.9663e-03,  3.5659e-03,  3.9363e-02],\n",
       "                        [ 1.3404e-02, -1.6896e-02,  1.1430e-02]],\n",
       "              \n",
       "                       [[-1.0430e-03,  9.1268e-03,  1.0148e-02],\n",
       "                        [-2.6466e-02, -2.3425e-02, -1.0852e-02],\n",
       "                        [-5.3916e-02, -3.9715e-02, -3.4506e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7843e-02, -5.0821e-02, -4.5108e-02],\n",
       "                        [-6.2374e-02, -5.5430e-02, -4.0633e-02],\n",
       "                        [-2.5661e-02, -4.3405e-02, -1.7859e-02]],\n",
       "              \n",
       "                       [[ 1.0539e-02, -3.7343e-02, -2.6692e-02],\n",
       "                        [-1.8067e-02, -7.1004e-02, -3.6903e-02],\n",
       "                        [-4.1934e-02, -6.0858e-02, -4.8291e-02]],\n",
       "              \n",
       "                       [[ 1.8932e-03, -7.6767e-03, -2.2510e-03],\n",
       "                        [-4.7701e-02, -3.8065e-02, -6.0919e-03],\n",
       "                        [-3.3916e-03,  6.6357e-04, -3.6400e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.7771e-02, -6.7366e-02, -4.5924e-02],\n",
       "                        [-6.4883e-02, -7.1817e-02, -8.3660e-02],\n",
       "                        [-6.4472e-02, -7.8665e-02, -4.9041e-02]],\n",
       "              \n",
       "                       [[ 2.2448e-02, -7.9445e-03, -2.5902e-02],\n",
       "                        [-3.0015e-02, -5.3157e-02, -4.2718e-02],\n",
       "                        [-8.5998e-03, -2.3035e-02, -1.2705e-02]],\n",
       "              \n",
       "                       [[-1.9064e-02, -3.4704e-02, -1.7819e-02],\n",
       "                        [-3.0182e-02, -4.5836e-02, -5.9080e-02],\n",
       "                        [-3.8873e-02, -3.4770e-02, -1.6886e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 5.8702e-03,  6.8515e-03, -2.0388e-03],\n",
       "                        [-1.0833e-02, -1.8401e-02, -2.4071e-02],\n",
       "                        [-4.0374e-03, -8.0181e-03, -2.3214e-02]],\n",
       "              \n",
       "                       [[-2.1485e-02, -1.1634e-02, -6.1990e-03],\n",
       "                        [-1.2291e-02, -6.3019e-02, -1.5111e-02],\n",
       "                        [-1.7218e-02, -3.4021e-02,  4.8477e-03]],\n",
       "              \n",
       "                       [[-8.4766e-03, -3.5826e-02, -3.5586e-03],\n",
       "                        [-5.2973e-02, -3.0279e-02, -9.8959e-03],\n",
       "                        [-2.1247e-02, -1.1753e-02, -9.0205e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.2522e-02,  3.6628e-03,  3.7974e-02],\n",
       "                        [ 2.3555e-02, -2.8655e-02,  1.1324e-02],\n",
       "                        [ 3.2601e-02,  2.9777e-04,  3.1403e-03]],\n",
       "              \n",
       "                       [[ 1.8980e-02,  3.3130e-02,  2.6358e-02],\n",
       "                        [ 1.5671e-02, -1.7249e-02,  2.9767e-02],\n",
       "                        [-1.8166e-02, -1.2555e-02, -1.0856e-02]],\n",
       "              \n",
       "                       [[-7.3598e-03,  1.9659e-02,  1.4315e-02],\n",
       "                        [ 2.0768e-03, -3.3350e-02, -9.2786e-03],\n",
       "                        [-5.8694e-02, -3.0899e-02, -4.6824e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.4034e-02, -3.7698e-02, -1.3985e-03],\n",
       "                        [-2.5831e-02, -2.5119e-02, -1.2089e-02],\n",
       "                        [-2.5191e-02,  4.1643e-03, -2.2906e-02]],\n",
       "              \n",
       "                       [[ 3.6644e-02,  3.1997e-04,  1.7653e-02],\n",
       "                        [ 2.1408e-02, -1.2411e-02, -2.5772e-02],\n",
       "                        [ 2.5596e-02, -2.2725e-02,  1.8071e-02]],\n",
       "              \n",
       "                       [[-2.2227e-02,  4.5408e-03, -1.2287e-02],\n",
       "                        [-1.3711e-02, -3.8088e-02, -2.4859e-02],\n",
       "                        [-2.9436e-02, -2.8553e-02, -3.6893e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.7718e-02, -2.7182e-02, -3.0649e-02],\n",
       "                        [-5.9171e-02, -5.2970e-02, -4.7795e-02],\n",
       "                        [-1.3224e-02, -2.2334e-02, -4.1423e-02]],\n",
       "              \n",
       "                       [[-1.6268e-02, -1.4023e-02, -1.3823e-02],\n",
       "                        [-2.4447e-02, -4.9765e-03, -4.2601e-02],\n",
       "                        [-2.1630e-02, -1.7375e-02, -4.9212e-03]],\n",
       "              \n",
       "                       [[-5.0970e-02, -4.8046e-02, -5.9222e-02],\n",
       "                        [-8.3811e-02, -7.6556e-02, -7.2725e-02],\n",
       "                        [-1.5504e-02, -3.9333e-02, -3.6030e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.8928e-02, -4.8646e-03,  7.5118e-03],\n",
       "                        [-4.7823e-02, -4.2544e-02, -3.0703e-02],\n",
       "                        [-7.9526e-04, -1.1317e-02, -1.5849e-02]],\n",
       "              \n",
       "                       [[-4.1966e-03, -1.8554e-02, -1.9337e-02],\n",
       "                        [-4.7693e-02, -3.2356e-02, -2.8931e-02],\n",
       "                        [-1.7698e-02, -1.1199e-02, -1.9506e-05]],\n",
       "              \n",
       "                       [[-5.5008e-02, -5.6039e-02, -5.7755e-03],\n",
       "                        [-5.0614e-02, -5.8016e-02, -2.5962e-02],\n",
       "                        [ 4.0553e-06, -4.3726e-02, -7.4586e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.9035e-02,  3.1074e-02,  1.5538e-02],\n",
       "                        [-1.2119e-02,  1.9834e-02,  1.1956e-02],\n",
       "                        [ 7.8825e-03, -1.5554e-02,  2.5736e-02]],\n",
       "              \n",
       "                       [[ 7.0239e-03, -1.1370e-02, -2.9957e-02],\n",
       "                        [ 6.5035e-03,  5.1479e-03, -4.3408e-03],\n",
       "                        [ 8.9171e-03,  1.4371e-03, -1.5489e-02]],\n",
       "              \n",
       "                       [[ 2.2683e-02,  1.8384e-02,  1.2628e-02],\n",
       "                        [ 4.6655e-03, -2.0947e-03,  8.1952e-03],\n",
       "                        [-4.0643e-03, -1.3595e-02, -1.0407e-02]]]], device='mps:0')),\n",
       "             ('model.8.1.weight',\n",
       "              tensor([1.0308, 0.8874, 1.1690, 1.0939, 0.9597, 1.0182, 1.0422, 1.0519, 1.0254,\n",
       "                      0.9102, 0.8797, 1.0252, 0.9971, 0.9420, 1.0300, 0.8791, 1.0207, 0.8250,\n",
       "                      1.0679, 1.0767, 1.0078, 0.9794, 1.0096, 0.9176, 0.8638, 1.0283, 0.9737,\n",
       "                      1.2252, 1.0279, 0.9214, 0.9660, 0.8590, 0.9516, 1.0178, 1.0257, 1.0721,\n",
       "                      0.9995, 1.1100, 0.9444, 1.0553, 1.0520, 1.0258, 1.1414, 0.9887, 0.9818,\n",
       "                      0.9086, 1.1466, 1.0036, 1.1037, 1.0161, 0.8205, 1.2591, 0.8926, 0.8514,\n",
       "                      1.0578, 0.8890, 0.9357, 1.3010, 0.8723, 0.8431, 0.8837, 1.2148, 0.8862,\n",
       "                      0.9405, 0.9404, 1.1990, 1.0773, 1.0639, 0.8848, 0.9433, 1.0363, 1.0046,\n",
       "                      1.0270, 0.9579, 1.1859, 0.9207, 1.1511, 1.3795, 1.2800, 1.0925, 1.0532,\n",
       "                      0.9058, 1.1037, 1.1546, 1.4678, 0.8637, 1.0895, 0.9946, 0.8525, 1.0097,\n",
       "                      1.0130, 1.0805, 0.9000, 0.9938, 0.9160, 0.9015, 0.9371, 0.8111, 1.3362,\n",
       "                      0.9495, 1.1069, 0.9501, 0.9961, 0.9890, 1.1570, 0.9527, 1.0031, 1.0758,\n",
       "                      1.1379, 0.9681, 1.0346, 1.2044, 1.0579, 0.8749, 0.9119, 0.9735, 0.8443,\n",
       "                      1.0533, 1.0377, 1.1058, 1.0459, 1.1998, 1.0271, 1.1396, 1.0182, 0.9188,\n",
       "                      0.9236, 0.8104, 1.0594, 1.0661, 1.0839, 0.9170, 1.0707, 0.8988, 1.1952,\n",
       "                      0.9974, 1.0785, 0.8823, 0.9564, 1.0571, 0.9317, 1.0122, 0.8481, 0.9531,\n",
       "                      0.8810, 1.2565, 1.1384, 1.1721, 1.0015, 0.8507, 1.0699, 1.0163, 1.2615,\n",
       "                      0.8918, 1.0795, 0.9757, 0.8327, 0.9575, 0.8884, 0.8958, 1.2742, 1.0658,\n",
       "                      1.2866, 1.0630, 1.1079, 1.2583, 0.9767, 0.9718, 0.9714, 1.0285, 0.8815,\n",
       "                      0.9229, 0.9334, 1.0596, 0.8340, 0.8687, 1.1986, 0.8790, 1.0460, 1.0660,\n",
       "                      1.1495, 1.0142, 1.0247, 0.9169, 1.1923, 1.1508, 0.9166, 1.0844, 1.0570,\n",
       "                      1.0437, 0.9492, 1.0880, 1.1078, 1.1288, 1.1991, 1.0240, 0.9840, 1.1613,\n",
       "                      1.1755, 0.9921, 1.2538, 0.8211, 1.0968, 1.0313, 0.9014, 0.8620, 1.0041,\n",
       "                      0.8604, 0.8378, 0.9413, 0.9620, 1.0918, 0.9267, 0.8885, 1.0364, 0.9624,\n",
       "                      0.8983, 1.0512, 1.4554, 0.9081, 0.9179, 0.8989, 1.1124, 0.7234, 0.8016,\n",
       "                      0.9833, 1.1875, 0.9908, 0.9968, 0.9409, 1.0454, 1.1355, 0.9838, 1.0515,\n",
       "                      1.0068, 1.2345, 0.9344, 1.0950, 0.9760, 0.8508, 0.8350, 1.2071, 1.0936,\n",
       "                      1.1375, 0.9939, 0.9701, 1.0758, 0.9567, 1.1311, 0.8718, 1.2685, 1.0941,\n",
       "                      0.7996, 1.0430, 0.9925, 1.0784], device='mps:0')),\n",
       "             ('model.8.1.bias',\n",
       "              tensor([-0.4297, -0.5306, -0.6522, -0.9460, -0.5909, -0.6346, -0.3648, -0.4678,\n",
       "                      -0.6528, -0.5129, -0.4458, -0.5373, -0.6273, -0.6467, -0.6617, -0.4957,\n",
       "                      -0.5592, -0.4778, -0.4642, -0.5432, -0.5031, -0.5746, -0.5194, -0.4136,\n",
       "                      -0.4350, -0.5166, -0.4099, -0.8586, -0.5386, -0.4165, -0.3852, -0.6161,\n",
       "                      -0.4731, -0.4703, -0.5774, -0.6953, -0.5967, -0.5666, -0.4505, -0.5691,\n",
       "                      -0.6166, -0.4949, -0.5010, -0.4893, -0.2954, -0.5817, -0.7623, -0.5835,\n",
       "                      -0.6048, -0.4831, -0.4075, -0.8801, -0.5465, -0.5924, -0.5974, -0.4542,\n",
       "                      -0.5365, -0.7509, -0.5065, -0.5472, -0.5536, -0.9623, -0.4126, -0.6254,\n",
       "                      -0.5301, -0.6219, -0.6780, -0.5267, -0.5273, -0.3749, -0.4525, -0.5350,\n",
       "                      -0.7062, -0.5382, -0.5854, -0.4124, -0.7003, -0.7501, -0.6699, -0.5676,\n",
       "                      -0.4915, -0.5154, -0.5079, -0.6253, -1.0708, -0.3795, -0.5496, -0.4179,\n",
       "                      -0.4856, -0.5466, -0.3663, -0.7055, -0.6357, -0.6861, -0.5295, -0.6710,\n",
       "                      -0.5037, -0.5149, -0.6724, -0.4119, -0.7595, -0.4505, -0.6070, -0.4635,\n",
       "                      -0.6386, -0.5070, -0.4414, -0.4363, -0.8381, -0.5448, -0.4918, -0.3434,\n",
       "                      -0.5887, -0.4095, -0.7194, -0.5968, -0.4249, -0.5259, -0.3846, -0.7521,\n",
       "                      -0.5495, -0.7547, -0.4687, -0.4553, -0.5425, -0.4854, -0.4703, -0.4409,\n",
       "                      -0.4465, -0.6088, -0.4162, -0.5630, -0.5933, -0.5668, -0.7198, -0.3739,\n",
       "                      -0.4954, -0.4415, -0.5833, -0.5773, -0.4115, -0.5750, -0.6064, -0.4999,\n",
       "                      -0.5046, -0.7538, -0.5424, -0.2867, -0.4727, -0.6023, -0.4670, -0.5892,\n",
       "                      -0.8211, -0.4967, -0.6999, -0.5581, -0.4762, -0.4887, -0.3881, -0.6685,\n",
       "                      -0.5463, -0.4625, -0.6814, -0.6130, -0.5760, -0.5286, -0.5229, -0.5419,\n",
       "                      -0.4914, -0.5513, -0.5305, -0.6109, -0.3773, -0.6094, -0.4505, -0.4551,\n",
       "                      -0.6951, -0.5618, -0.5203, -0.4814, -0.6877, -0.6259, -0.4978, -0.6259,\n",
       "                      -0.5890, -0.6354, -0.4739, -0.4595, -0.6090, -0.5328, -0.4235, -0.6749,\n",
       "                      -0.7384, -0.7435, -0.6458, -0.7893, -0.4314, -0.4818, -0.3246, -0.6784,\n",
       "                      -0.7593, -0.4083, -0.5982, -0.5051, -0.5719, -0.5169, -0.5252, -0.4735,\n",
       "                      -0.4891, -0.5337, -0.4350, -0.6003, -0.6640, -0.4913, -0.5520, -0.6375,\n",
       "                      -0.5353, -0.6208, -1.2581, -0.5395, -0.4945, -0.5444, -0.4623, -0.4866,\n",
       "                      -0.4639, -0.3255, -0.6739, -0.5035, -0.6120, -0.6378, -0.4831, -0.4154,\n",
       "                      -0.5791, -0.4852, -0.5985, -0.9302, -0.5191, -0.6362, -0.5885, -0.4418,\n",
       "                      -0.4797, -0.6341, -0.6760, -0.6201, -0.7298, -0.5507, -0.3566, -0.6133,\n",
       "                      -0.6768, -0.4960, -0.5479, -0.6586, -0.4764, -0.5067, -0.6405, -0.6827],\n",
       "                     device='mps:0')),\n",
       "             ('model.8.1.running_mean',\n",
       "              tensor([-0.4941, -1.0094, -1.7038, -1.0933, -0.2052, -0.6391, -0.4792, -0.4224,\n",
       "                      -1.0133, -0.7561, -0.9313, -0.7992, -1.1958, -0.2733, -0.6834, -1.1708,\n",
       "                      -1.4565, -0.9840, -0.5442, -1.2844, -0.8634, -0.7614, -0.8149, -0.7377,\n",
       "                      -0.8901, -1.0905, -0.1318, -1.6965, -1.2550, -0.1603, -0.4412, -0.4027,\n",
       "                      -1.1215, -0.1332, -1.5407, -0.7312, -0.6987, -1.3687, -1.6190, -1.6145,\n",
       "                      -0.6750, -1.2861, -0.0686, -0.8464, -0.4286, -0.3853, -0.8645, -0.3802,\n",
       "                      -0.6891, -0.1906, -0.8926, -0.7729, -1.2678, -0.7436, -1.3636, -0.7532,\n",
       "                      -0.6793, -0.8838, -0.7856, -1.1342,  0.3065, -0.7740, -0.8689, -0.7690,\n",
       "                      -1.4846, -1.8971, -1.2731, -1.5975, -1.0179, -1.6392, -0.7223, -0.6726,\n",
       "                      -1.6052, -1.1385, -1.1608, -1.5979, -1.0783, -0.0740, -1.6524, -1.3062,\n",
       "                      -0.9039, -1.0959, -1.3106, -1.6887, -0.1421, -0.8588, -1.4282, -1.2323,\n",
       "                      -0.6870, -0.9969, -0.3731, -0.4749, -1.0172, -0.2783, -0.7414, -0.6548,\n",
       "                      -1.0883, -0.9434, -0.9272, -0.8400, -1.3631, -1.2235,  0.0030, -0.6526,\n",
       "                      -1.2325, -0.9278, -1.6742, -0.7637, -0.6251, -1.1302, -0.8258, -1.3936,\n",
       "                      -1.0333, -1.0198, -0.6443, -1.2059, -1.0648, -0.6850, -0.2598, -1.3440,\n",
       "                      -1.1077, -1.2384, -1.3569, -1.2979, -0.8110, -1.4729, -1.2413, -0.5510,\n",
       "                      -0.4350, -0.5714, -0.4959, -0.3828, -1.4676, -1.1333, -1.9143, -0.8810,\n",
       "                      -1.0257, -1.2960, -0.6164, -0.6843, -0.7134, -1.1263, -0.8478, -0.6832,\n",
       "                      -1.1148, -1.0303, -1.0954, -0.5106, -0.7351, -0.6655, -0.7223, -0.8696,\n",
       "                      -1.2527, -0.9287, -1.1434, -1.2369, -1.0862, -0.7628, -1.1860, -0.3517,\n",
       "                      -0.4351, -0.4609, -2.0867, -1.2452, -0.9814, -1.5421, -0.4826, -0.6479,\n",
       "                      -1.3353, -0.6166, -1.2609, -1.1498, -0.3941, -0.9518, -1.3849, -0.8849,\n",
       "                      -1.5015, -0.3610, -0.2098, -0.2693, -0.9152, -0.2270, -1.0735, -1.4492,\n",
       "                      -0.1947, -0.6456, -0.4786, -0.1632, -1.2389, -1.1187, -0.5968, -0.2930,\n",
       "                      -1.0498, -1.4973, -1.6538, -1.1066, -0.8791, -0.8750, -0.8764, -0.9775,\n",
       "                      -1.2535, -1.1121, -1.3016, -0.8005, -0.7101, -0.9187, -0.2567, -1.1557,\n",
       "                      -0.8811, -0.6118, -1.4393, -1.8863, -0.4922, -0.8364, -1.1875, -0.3090,\n",
       "                      -1.5371, -0.9290, -0.8305, -0.9168, -0.9670, -0.9073, -0.3790, -0.5844,\n",
       "                      -1.5206, -0.9008, -1.2376, -0.1514, -1.7173, -1.0512, -1.9179, -0.4761,\n",
       "                      -1.2766, -0.4609, -0.8908, -1.1010, -1.2556, -0.4665, -0.5917, -0.7208,\n",
       "                      -0.7762, -1.4240, -1.3414, -1.0601, -1.0455, -0.5723, -0.2770, -0.9137,\n",
       "                      -1.0671, -1.4327, -0.7263, -1.3254, -1.1180, -2.0386, -1.1148, -0.1366],\n",
       "                     device='mps:0')),\n",
       "             ('model.8.1.running_var',\n",
       "              tensor([1.4486, 0.9410, 1.9336, 1.8464, 0.7545, 1.1844, 2.0913, 1.9786, 1.4301,\n",
       "                      0.7836, 0.7025, 1.4848, 1.2630, 0.7371, 1.1402, 0.7764, 1.3917, 0.9301,\n",
       "                      2.0012, 1.0894, 1.8083, 1.1755, 0.6608, 1.4172, 0.8511, 1.1616, 1.9248,\n",
       "                      1.7294, 1.2275, 1.0325, 1.3673, 0.6833, 1.3174, 1.0913, 1.1091, 1.5753,\n",
       "                      1.5667, 2.0198, 1.2408, 1.3926, 1.6969, 1.0224, 2.6325, 1.6065, 1.3534,\n",
       "                      0.7884, 1.4172, 1.1191, 1.8695, 1.9037, 0.7170, 2.5935, 0.8115, 0.6250,\n",
       "                      1.3450, 0.9900, 1.0555, 1.9741, 0.4291, 1.0998, 1.3793, 2.3187, 1.0527,\n",
       "                      0.9273, 1.0971, 2.0150, 1.4723, 1.3347, 0.5419, 1.2895, 1.2344, 0.9124,\n",
       "                      1.3229, 1.4740, 1.7842, 1.0743, 1.8532, 4.2239, 3.1490, 1.5755, 1.2016,\n",
       "                      1.1128, 2.0294, 2.3130, 2.3699, 0.7994, 1.9352, 1.3770, 0.6520, 1.1812,\n",
       "                      2.0382, 1.5768, 0.9825, 0.9533, 0.8271, 1.1481, 0.8781, 0.7549, 1.9215,\n",
       "                      1.1129, 1.6512, 1.4645, 2.0936, 1.7363, 2.0574, 1.4335, 1.6929, 2.4693,\n",
       "                      1.6423, 1.5502, 1.9099, 2.5104, 1.4948, 0.8968, 0.9023, 1.0435, 0.6639,\n",
       "                      2.0681, 1.8700, 2.4401, 1.0850, 2.3445, 1.3905, 2.7239, 1.3116, 1.0093,\n",
       "                      1.1597, 0.4427, 1.9121, 1.2763, 1.8084, 0.8658, 1.2189, 0.7377, 1.8883,\n",
       "                      1.4138, 1.6949, 0.5872, 0.6841, 0.9259, 1.2526, 0.8595, 0.9855, 0.8502,\n",
       "                      0.5589, 2.9702, 1.8585, 2.5343, 1.5037, 0.6117, 2.3511, 1.0088, 1.8796,\n",
       "                      0.7482, 1.6115, 1.5352, 0.8302, 1.2797, 0.9398, 0.9961, 2.4591, 1.8624,\n",
       "                      2.2100, 1.7028, 2.4314, 2.3245, 1.3403, 0.9439, 0.8248, 1.3867, 1.1654,\n",
       "                      0.8723, 1.3498, 2.1462, 0.7504, 0.6136, 2.6036, 0.7964, 1.4090, 1.6043,\n",
       "                      1.3164, 1.2661, 2.0636, 0.8865, 2.5826, 1.9045, 1.5928, 1.2491, 1.3145,\n",
       "                      0.9583, 1.3678, 1.4880, 1.8180, 1.4515, 1.9290, 1.2365, 0.9649, 2.0984,\n",
       "                      2.5091, 0.8113, 2.4730, 0.4708, 1.8647, 1.6986, 0.8772, 0.8933, 1.3446,\n",
       "                      0.7136, 0.8453, 0.9925, 0.8827, 1.6811, 0.9046, 0.7479, 1.3585, 0.8767,\n",
       "                      0.9858, 1.6682, 2.7497, 1.0419, 0.9196, 0.9028, 2.4798, 0.6961, 0.7325,\n",
       "                      1.2366, 2.3166, 1.2347, 1.5441, 0.7337, 1.6086, 2.1972, 0.9934, 2.5463,\n",
       "                      0.9574, 1.9913, 1.3071, 1.9962, 1.0894, 0.7424, 0.8064, 2.2150, 1.6131,\n",
       "                      2.0072, 1.4788, 1.4969, 2.3147, 0.8446, 1.9977, 0.8550, 3.4108, 1.3492,\n",
       "                      0.5936, 1.6755, 1.4523, 1.3157], device='mps:0')),\n",
       "             ('model.8.1.num_batches_tracked', tensor(22484., device='mps:0')),\n",
       "             ('classifier.weight',\n",
       "              tensor([[ 0.2539, -0.4129, -0.4710,  ..., -0.4428,  0.2303,  0.4016],\n",
       "                      [ 0.1029,  0.1596, -0.3664,  ..., -0.2843, -0.4157,  0.2217],\n",
       "                      [-0.5143, -0.1243,  0.5136,  ..., -0.0231,  0.6600, -0.4438],\n",
       "                      ...,\n",
       "                      [-0.2359, -0.4257, -0.5459,  ...,  0.6352, -0.4225, -0.0575],\n",
       "                      [ 0.6336, -0.2844, -0.2890,  ..., -0.3333, -0.1888, -0.2694],\n",
       "                      [ 0.2781,  0.1640,  0.0090,  ...,  0.6026, -0.0594, -0.3741]],\n",
       "                     device='mps:0')),\n",
       "             ('classifier.bias',\n",
       "              tensor([ 0.3287, -0.6605,  0.4113,  0.5110,  0.0181, -0.2609,  0.0303,  0.1200,\n",
       "                      -0.0530, -0.3119], device='mps:0'))])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "switch_classes_no_defense.avg_weight_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "(Device 43/Epoch 0) Train Loss: 0.386 | Train Acc: 87.900Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.508 | Test Acc: 67.450\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 19.802228927612305 \n",
      "\n",
      "Round:  101\n",
      "(Device 7/Epoch 0) Train Loss: 0.438 | Train Acc: 85.440Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.583 | Test Acc: 79.280\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 31.828557014465332 \n",
      "\n",
      "Round:  102\n",
      "(Device 35/Epoch 0) Train Loss: 0.374 | Train Acc: 86.840Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.835 | Test Acc: 69.240\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 40.55018210411072 \n",
      "\n",
      "Round:  103\n",
      "(Device 19/Epoch 0) Train Loss: 0.373 | Train Acc: 86.660Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.657 | Test Acc: 77.470\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 49.13584089279175 \n",
      "\n",
      "Round:  104\n",
      "(Device 48/Epoch 0) Train Loss: 0.359 | Train Acc: 87.640Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.715 | Test Acc: 75.320\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 57.36453104019165 \n",
      "\n",
      "Round:  105\n",
      "(Device 39/Epoch 0) Train Loss: 0.357 | Train Acc: 87.160Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.696 | Test Acc: 76.980\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 65.82795310020447 \n",
      "\n",
      "Round:  106\n",
      "(Device 47/Epoch 0) Train Loss: 0.344 | Train Acc: 88.440Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.773 | Test Acc: 75.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 80.35896110534668 \n",
      "\n",
      "Round:  107\n",
      "(Device 30/Epoch 0) Train Loss: 0.352 | Train Acc: 87.580Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.681 | Test Acc: 75.670\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 92.11691689491272 \n",
      "\n",
      "Round:  108\n",
      "(Device 34/Epoch 0) Train Loss: 0.350 | Train Acc: 87.780Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.727 | Test Acc: 76.160\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 106.1803789138794 \n",
      "\n",
      "Round:  109\n",
      "(Device 49/Epoch 0) Train Loss: 0.351 | Train Acc: 87.620Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.777 | Test Acc: 74.910\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 116.06819200515747 \n",
      "\n",
      "Round:  110\n",
      "(Device 37/Epoch 0) Train Loss: 0.363 | Train Acc: 86.800Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.723 | Test Acc: 75.990\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 125.91239404678345 \n",
      "\n",
      "Round:  111\n",
      "(Device 30/Epoch 0) Train Loss: 0.400 | Train Acc: 86.420Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.736 | Test Acc: 73.820\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 135.5897409915924 \n",
      "\n",
      "Round:  112\n",
      "(Device 0/Epoch 0) Train Loss: 0.380 | Train Acc: 86.900Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.789 | Test Acc: 73.970\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 144.1921329498291 \n",
      "\n",
      "Round:  113\n",
      "(Device 29/Epoch 0) Train Loss: 0.344 | Train Acc: 88.060Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.865 | Test Acc: 74.200\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 153.08284711837769 \n",
      "\n",
      "Round:  114\n",
      "(Device 46/Epoch 0) Train Loss: 0.386 | Train Acc: 86.140Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.753 | Test Acc: 75.300\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 162.08767819404602 \n",
      "\n",
      "Round:  115\n",
      "(Device 16/Epoch 0) Train Loss: 0.348 | Train Acc: 87.500Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.866 | Test Acc: 73.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 172.04300713539124 \n",
      "\n",
      "Round:  116\n",
      "(Device 26/Epoch 0) Train Loss: 0.398 | Train Acc: 86.680Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.718 | Test Acc: 75.810\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 181.01415586471558 \n",
      "\n",
      "Round:  117\n",
      "(Device 6/Epoch 0) Train Loss: 0.337 | Train Acc: 88.180Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.835 | Test Acc: 74.940\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 193.92838311195374 \n",
      "\n",
      "Round:  118\n",
      "(Device 34/Epoch 0) Train Loss: 0.375 | Train Acc: 86.500Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.746 | Test Acc: 75.890\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 206.98106503486633 \n",
      "\n",
      "Round:  119\n",
      "(Device 3/Epoch 0) Train Loss: 0.430 | Train Acc: 87.220Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.843 | Test Acc: 75.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 220.10046815872192 \n",
      "\n",
      "Round:  120\n",
      "(Device 44/Epoch 0) Train Loss: 0.393 | Train Acc: 87.380Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.752 | Test Acc: 76.510\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 241.0419750213623 \n",
      "\n",
      "Round:  121\n",
      "(Device 10/Epoch 0) Train Loss: 0.345 | Train Acc: 87.780Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.796 | Test Acc: 76.320\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 255.8760280609131 \n",
      "\n",
      "Round:  122\n",
      "(Device 38/Epoch 0) Train Loss: 0.347 | Train Acc: 87.420Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.747 | Test Acc: 76.680\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 265.22406697273254 \n",
      "\n",
      "Round:  123\n",
      "(Device 43/Epoch 0) Train Loss: 0.378 | Train Acc: 87.140Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.734 | Test Acc: 75.140\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 276.5279037952423 \n",
      "\n",
      "Round:  124\n",
      "(Device 3/Epoch 0) Train Loss: 0.436 | Train Acc: 85.580Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 0.800 | Test Acc: 74.630\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 290.3334550857544 \n",
      "\n",
      "Round:  125\n",
      "(Device 6/Epoch 0) Train Loss: 0.406 | Train Acc: 86.400Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.084 | Test Acc: 69.090\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 299.6922001838684 \n",
      "\n",
      "Round:  126\n",
      "(Device 45/Epoch 0) Train Loss: 0.485 | Train Acc: 82.720Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.001 | Test Acc: 69.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 310.5632998943329 \n",
      "\n",
      "Round:  127\n",
      "(Device 21/Epoch 0) Train Loss: 0.474 | Train Acc: 82.600Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.011 | Test Acc: 69.810\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 322.5382959842682 \n",
      "\n",
      "Round:  128\n",
      "(Device 5/Epoch 0) Train Loss: 0.510 | Train Acc: 82.100Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.012 | Test Acc: 70.070\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 332.42797088623047 \n",
      "\n",
      "Round:  129\n",
      "(Device 32/Epoch 0) Train Loss: 0.513 | Train Acc: 81.520Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 1.020 | Test Acc: 69.590\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 341.52513813972473 \n",
      "\n",
      "Round:  130\n",
      "(Device 1/Epoch 0) Train Loss: 0.509 | Train Acc: 81.540 | Test Loss: 0.514 | Test Acc: 83.110\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 355.09473490715027 \n",
      "\n",
      "Round:  131\n",
      "(Device 3/Epoch 0) Train Loss: 0.382 | Train Acc: 87.240 | Test Loss: 0.463 | Test Acc: 84.640\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 369.60166001319885 \n",
      "\n",
      "Round:  132\n",
      "(Device 41/Epoch 0) Train Loss: 0.337 | Train Acc: 88.780 | Test Loss: 0.453 | Test Acc: 85.220\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 394.36123299598694 \n",
      "\n",
      "Round:  133\n",
      "(Device 30/Epoch 0) Train Loss: 0.328 | Train Acc: 89.080 | Test Loss: 0.438 | Test Acc: 85.700\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 408.3563930988312 \n",
      "\n",
      "Round:  134\n",
      "(Device 36/Epoch 0) Train Loss: 0.315 | Train Acc: 89.040 | Test Loss: 0.430 | Test Acc: 85.830\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 421.1418950557709 \n",
      "\n",
      "Round:  135\n",
      "(Device 5/Epoch 0) Train Loss: 0.337 | Train Acc: 89.600 | Test Loss: 0.438 | Test Acc: 85.610\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 440.06766510009766 \n",
      "\n",
      "Round:  136\n",
      "(Device 14/Epoch 0) Train Loss: 0.303 | Train Acc: 89.860 | Test Loss: 0.412 | Test Acc: 86.180\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 457.3614628314972 \n",
      "\n",
      "Round:  137\n",
      "(Device 34/Epoch 0) Train Loss: 0.315 | Train Acc: 89.980 | Test Loss: 0.425 | Test Acc: 86.080\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 472.93342208862305 \n",
      "\n",
      "Round:  138\n",
      "(Device 27/Epoch 0) Train Loss: 0.276 | Train Acc: 90.340 | Test Loss: 0.404 | Test Acc: 86.540\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 492.3361701965332 \n",
      "\n",
      "Round:  139\n",
      "(Device 28/Epoch 0) Train Loss: 0.265 | Train Acc: 91.020 | Test Loss: 0.404 | Test Acc: 86.830\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 511.4685800075531 \n",
      "\n",
      "Round:  140\n",
      "(Device 43/Epoch 0) Train Loss: 0.302 | Train Acc: 89.920 | Test Loss: 0.401 | Test Acc: 86.720\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 528.4219539165497 \n",
      "\n",
      "Round:  141\n",
      "(Device 26/Epoch 0) Train Loss: 0.273 | Train Acc: 90.200 | Test Loss: 0.399 | Test Acc: 86.760\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 544.2916059494019 \n",
      "\n",
      "Round:  142\n",
      "(Device 33/Epoch 0) Train Loss: 0.270 | Train Acc: 90.880 | Test Loss: 0.403 | Test Acc: 86.880\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 576.4343011379242 \n",
      "\n",
      "Round:  143\n",
      "(Device 12/Epoch 0) Train Loss: 0.253 | Train Acc: 91.580 | Test Loss: 0.398 | Test Acc: 86.980\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 592.7115209102631 \n",
      "\n",
      "Round:  144\n",
      "(Device 28/Epoch 0) Train Loss: 0.256 | Train Acc: 91.760 | Test Loss: 0.399 | Test Acc: 86.990\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 608.1541459560394 \n",
      "\n",
      "Round:  145\n",
      "(Device 25/Epoch 0) Train Loss: 0.279 | Train Acc: 91.000 | Test Loss: 0.399 | Test Acc: 86.950\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 650.8982601165771 \n",
      "\n",
      "Round:  146\n",
      "(Device 18/Epoch 0) Train Loss: 0.251 | Train Acc: 91.780 | Test Loss: 0.403 | Test Acc: 86.820\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 668.5553770065308 \n",
      "\n",
      "Round:  147\n",
      "(Device 19/Epoch 0) Train Loss: 0.270 | Train Acc: 91.140 | Test Loss: 0.395 | Test Acc: 86.960\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 686.0818939208984 \n",
      "\n",
      "Round:  148\n",
      "(Device 35/Epoch 0) Train Loss: 0.259 | Train Acc: 91.080 | Test Loss: 0.389 | Test Acc: 87.230\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 699.5693900585175 \n",
      "\n",
      "Round:  149\n",
      "(Device 20/Epoch 0) Train Loss: 0.262 | Train Acc: 91.780 | Test Loss: 0.396 | Test Acc: 87.020\n",
      "\n",
      "Diff: 0.0\n",
      "\n",
      "Round time: 720.389307975769 \n",
      "\n",
      "Total training time: 720.3893489837646 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sca_zero_one = switch_classes_attack(0,1,2)\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_no_defense_fixed_2 = run_federated_test(                    \n",
    "                                         rounds = 150, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 1,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.02,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_no_defense_multiple_final_fixed_2.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = False, \n",
    "                                         multiple_attack_rounds = (list(range(100,130)))) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_classes_no_defense_fixed = load_result(\"switch_classes_no_defense_multiple_final_fixed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      " | Test Loss: 0.583 | Test Acc: 81.537\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from snapshot!\n",
      "\n",
      "Restoring devices\n",
      "\n",
      "Finished restoring\n",
      "\n",
      "Round:  100\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 3.217 | Test Acc: 10.000\n",
      "\n",
      "Diff: 61.552120208740234\n",
      "\n",
      "Round time: 10.591506958007812 \n",
      "\n",
      "Round:  101\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.378 | Test Acc: 10.000\n",
      "\n",
      "Diff: 51.3704948425293\n",
      "\n",
      "Round time: 17.99462580680847 \n",
      "\n",
      "Round:  102\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.349 | Test Acc: 10.000\n",
      "\n",
      "Diff: 42.18416213989258\n",
      "\n",
      "Round time: 25.071220874786377 \n",
      "\n",
      "Round:  103\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.341 | Test Acc: 10.000\n",
      "\n",
      "Diff: 46.517311096191406\n",
      "\n",
      "Round time: 31.770357847213745 \n",
      "\n",
      "Round:  104\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 28.81142234802246\n",
      "\n",
      "Round time: 38.38723683357239 \n",
      "\n",
      "Round:  105\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 37.083778381347656\n",
      "\n",
      "Round time: 45.42861986160278 \n",
      "\n",
      "Round:  106\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 21.506816864013672\n",
      "\n",
      "Round time: 51.44661498069763 \n",
      "\n",
      "Round:  107\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 31.953285217285156\n",
      "\n",
      "Round time: 60.55612087249756 \n",
      "\n",
      "Round:  108\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 17.579307556152344\n",
      "\n",
      "Round time: 93.60609102249146 \n",
      "\n",
      "Round:  109\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 16.440658569335938\n",
      "\n",
      "Round time: 101.16266179084778 \n",
      "\n",
      "Round:  110\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 28.39679527282715\n",
      "\n",
      "Round time: 162.01305866241455 \n",
      "\n",
      "Round:  111\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 27.719554901123047\n",
      "\n",
      "Round time: 168.36285996437073 \n",
      "\n",
      "Round:  112\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 14.355100631713867\n",
      "\n",
      "Round time: 174.33972191810608 \n",
      "\n",
      "Round:  113\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.92732048034668\n",
      "\n",
      "Round time: 179.88981676101685 \n",
      "\n",
      "Round:  114\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.670799255371094\n",
      "\n",
      "Round time: 231.22068881988525 \n",
      "\n",
      "Round:  115\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.55969524383545\n",
      "\n",
      "Round time: 236.37237977981567 \n",
      "\n",
      "Round:  116\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.427392959594727\n",
      "\n",
      "Round time: 241.97601890563965 \n",
      "\n",
      "Round:  117\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.328110694885254\n",
      "\n",
      "Round time: 247.23519492149353 \n",
      "\n",
      "Round:  118\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.25359058380127\n",
      "\n",
      "Round time: 252.7442228794098 \n",
      "\n",
      "Round:  119\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.197632789611816\n",
      "\n",
      "Round time: 420.3441159725189 \n",
      "\n",
      "Round:  120\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.155601501464844\n",
      "\n",
      "Round time: 428.65854001045227 \n",
      "\n",
      "Round:  121\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.1240234375\n",
      "\n",
      "Round time: 443.08194875717163 \n",
      "\n",
      "Round:  122\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.338 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.10029411315918\n",
      "\n",
      "Round time: 452.06119084358215 \n",
      "\n",
      "Round:  123\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.082463264465332\n",
      "\n",
      "Round time: 458.6382179260254 \n",
      "\n",
      "Round:  124\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.034759521484375\n",
      "\n",
      "Round time: 465.1850926876068 \n",
      "\n",
      "Round:  125\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.06956958770752\n",
      "\n",
      "Round time: 471.02753591537476 \n",
      "\n",
      "Round:  126\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 13.059511184692383\n",
      "\n",
      "Round time: 476.9182369709015 \n",
      "\n",
      "Round:  127\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.017478942871094\n",
      "\n",
      "Round time: 482.5736289024353 \n",
      "\n",
      "Round:  128\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.339 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.020511627197266\n",
      "\n",
      "Round time: 489.25805377960205 \n",
      "\n",
      "Round:  129\n",
      "Attacking!\n",
      "\n",
      "Using memoized attack\n",
      "\n",
      "Confirm the attack worked. (This should be high)\n",
      " | Test Loss: 0.272 | Test Acc: 91.250\n",
      "Finished attacking\n",
      "\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 26.02301597595215\n",
      "\n",
      "Round time: 495.76498889923096 \n",
      "\n",
      "Round:  130\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.4103201627731323\n",
      "\n",
      "Round time: 501.856055021286 \n",
      "\n",
      "Round:  131\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.32292768359184265\n",
      "\n",
      "Round time: 507.3080909252167 \n",
      "\n",
      "Round:  132\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.2541482746601105\n",
      "\n",
      "Round time: 513.599399805069 \n",
      "\n",
      "Round:  133\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.20001816749572754\n",
      "\n",
      "Round time: 519.2582058906555 \n",
      "\n",
      "Round:  134\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.15741708874702454\n",
      "\n",
      "Round time: 524.468759059906 \n",
      "\n",
      "Round:  135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.12388928979635239\n",
      "\n",
      "Round time: 529.5668148994446 \n",
      "\n",
      "Round:  136\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.09750255942344666\n",
      "\n",
      "Round time: 535.0100967884064 \n",
      "\n",
      "Round:  137\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.07673581689596176\n",
      "\n",
      "Round time: 541.0797357559204 \n",
      "\n",
      "Round:  138\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.060392115265131\n",
      "\n",
      "Round time: 548.1955320835114 \n",
      "\n",
      "Round:  139\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.04752938821911812\n",
      "\n",
      "Round time: 554.9258449077606 \n",
      "\n",
      "Round:  140\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.03740628808736801\n",
      "\n",
      "Round time: 560.588574886322 \n",
      "\n",
      "Round:  141\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.02943924255669117\n",
      "\n",
      "Round time: 566.3499519824982 \n",
      "\n",
      "Round:  142\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.02316909097135067\n",
      "\n",
      "Round time: 572.1516077518463 \n",
      "\n",
      "Round:  143\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.018234383314847946\n",
      "\n",
      "Round time: 577.8510336875916 \n",
      "\n",
      "Round:  144\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.014350706711411476\n",
      "\n",
      "Round time: 583.4424538612366 \n",
      "\n",
      "Round:  145\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.01129419356584549\n",
      "\n",
      "Round time: 589.2257039546967 \n",
      "\n",
      "Round:  146\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.008888684213161469\n",
      "\n",
      "Round time: 594.6060657501221 \n",
      "\n",
      "Round:  147\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.006995517294853926\n",
      "\n",
      "Round time: 599.3306179046631 \n",
      "\n",
      "Round:  148\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.005505563225597143\n",
      "\n",
      "Round time: 603.9891109466553 \n",
      "\n",
      "Round:  149\n",
      " | Test Loss: 2.340 | Test Acc: 10.000\n",
      "\n",
      "Diff: 0.004332953132688999\n",
      "\n",
      "Round time: 609.0267109870911 \n",
      "\n",
      "Total training time: 609.0267210006714 seconds\n",
      "Writing file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empirical_cutoffs = {\n",
    "    'model.0.0.weight' : (-0.01,0.01),\n",
    "    'model.1.0.weight' : (-0.01,0.01),\n",
    "    'model.2.0.weight' : (-0.01,0.01),\n",
    "    'model.3.0.weight' : (-0.01,0.01),\n",
    "    'model.4.0.weight' : (-0.01,0.01),\n",
    "    'model.5.0.weight' : (-0.01,0.01),\n",
    "    'model.6.0.weight' : (-0.01,0.01),\n",
    "    'model.7.0.weight' : (-0.01,0.01),\n",
    "    'model.8.0.weight' : (-0.01,0.01),\n",
    "}\n",
    "all_keys   = [x for x in baseline.avg_weight_history[-1].keys() if \".weight\" in x]\n",
    "test_cutoffs = {\n",
    "    x : (-0.01,0.01)\n",
    "    for x in all_keys\n",
    "}\n",
    "\n",
    "sigmoid_aggregation = make_sigmoid_defense(test_cutoffs, stickiness=0)\n",
    "\n",
    "# sca_zero_one = switch_classes_attack(0,1,2)\n",
    "baseline = load_result(\"baseline.pickle\")\n",
    "switch_classes_sigmoid_defense_fixed = run_federated_test(  agg_fn=sigmoid_aggregation,                \n",
    "                                         rounds = 150, # go 30 pounds past where the baseline left off              \n",
    "                                         local_epochs = 0,        # all else the same                     \n",
    "                                         num_devices = 50,         \n",
    "                                         device_pct = 0.4,          \n",
    "                                         data_pct = 0.1,           \n",
    "                                         net = ConvNet().to(mps),  \n",
    "                                         evil_round = 110, # attack after 10 rounds more training with the new acc fn        \n",
    "                                         attacker_strategy = sca_zero_one, # device 2 will carry out big noise attack  \n",
    "                                         evil_device_id = 2,     \n",
    "                                         evaluate_attack = None, # we will evaluate manually afterwards \n",
    "                                         output_filename = \"switch_classes_sigmoid_defense_multiple_final_fixed.pickle\",\n",
    "                                         resume_from_snap = baseline, #pick up where baseline left off  \n",
    "                                         snapshot = False, \n",
    "                                         multiple_attack_rounds = (list(range(100,130)))) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_evaluate_switch_classes_attack_weights(a, b):\n",
    "    checker_test_set_nontarget = make_testloader_subset([x for x in list(range(10)) if x not in [a,b]], 0, 0)\n",
    "    checker_test_set_target = make_testloader_subset([a,b], a, b)\n",
    "    \n",
    "    def f(w):\n",
    "        test_device = make_test_device(trainset)\n",
    "        test_device['net'].load_state_dict(w)\n",
    "\n",
    "        test(0, test_device, nn.CrossEntropyLoss(), checker_test_set_nontarget)\n",
    "        nontarget_accuracy = test_device['test_acc_tracker'][-1]\n",
    "        test(0, test_device, nn.CrossEntropyLoss(), checker_test_set_target)\n",
    "        target_accuracy = test_device['test_acc_tracker'][-1]\n",
    "        return nontarget_accuracy, target_accuracy\n",
    "    return f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      " | Test Loss: 2.495 | Test Acc: 12.750\n",
      " | Test Loss: 4.315 | Test Acc: 0.000\n",
      " | Test Loss: 1.529 | Test Acc: 43.112\n",
      " | Test Loss: 3.896 | Test Acc: 1.750\n",
      " | Test Loss: 1.215 | Test Acc: 56.038\n",
      " | Test Loss: 4.935 | Test Acc: 1.600\n",
      " | Test Loss: 1.095 | Test Acc: 60.550\n",
      " | Test Loss: 5.553 | Test Acc: 2.300\n",
      " | Test Loss: 1.050 | Test Acc: 62.237\n",
      " | Test Loss: 6.340 | Test Acc: 2.600\n",
      " | Test Loss: 0.918 | Test Acc: 66.938\n",
      " | Test Loss: 6.901 | Test Acc: 1.850\n",
      " | Test Loss: 0.874 | Test Acc: 69.225\n",
      " | Test Loss: 7.538 | Test Acc: 1.900\n",
      " | Test Loss: 0.765 | Test Acc: 72.787\n",
      " | Test Loss: 8.150 | Test Acc: 1.400\n",
      " | Test Loss: 0.732 | Test Acc: 74.875\n",
      " | Test Loss: 8.904 | Test Acc: 1.350\n",
      " | Test Loss: 0.704 | Test Acc: 75.537\n",
      " | Test Loss: 9.064 | Test Acc: 1.100\n",
      " | Test Loss: 0.641 | Test Acc: 78.325\n",
      " | Test Loss: 9.676 | Test Acc: 0.850\n",
      " | Test Loss: 0.633 | Test Acc: 78.588\n",
      " | Test Loss: 9.873 | Test Acc: 1.000\n",
      " | Test Loss: 0.591 | Test Acc: 79.938\n",
      " | Test Loss: 10.739 | Test Acc: 0.950\n",
      " | Test Loss: 0.575 | Test Acc: 80.700\n",
      " | Test Loss: 10.654 | Test Acc: 0.850\n",
      " | Test Loss: 0.553 | Test Acc: 81.550\n",
      " | Test Loss: 11.122 | Test Acc: 0.800\n",
      " | Test Loss: 0.563 | Test Acc: 81.125\n",
      " | Test Loss: 11.305 | Test Acc: 0.850\n",
      " | Test Loss: 0.511 | Test Acc: 82.838\n",
      " | Test Loss: 10.693 | Test Acc: 0.800\n",
      " | Test Loss: 0.521 | Test Acc: 83.162\n",
      " | Test Loss: 12.151 | Test Acc: 0.750\n",
      " | Test Loss: 0.492 | Test Acc: 83.963\n",
      " | Test Loss: 12.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.491 | Test Acc: 83.987\n",
      " | Test Loss: 12.486 | Test Acc: 0.750\n",
      " | Test Loss: 0.487 | Test Acc: 84.525\n",
      " | Test Loss: 13.002 | Test Acc: 0.700\n",
      " | Test Loss: 0.465 | Test Acc: 84.987\n",
      " | Test Loss: 12.353 | Test Acc: 0.750\n",
      " | Test Loss: 0.478 | Test Acc: 84.775\n",
      " | Test Loss: 12.557 | Test Acc: 0.750\n",
      " | Test Loss: 0.448 | Test Acc: 85.388\n",
      " | Test Loss: 13.923 | Test Acc: 0.650\n",
      " | Test Loss: 0.447 | Test Acc: 85.487\n",
      " | Test Loss: 13.003 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.075\n",
      " | Test Loss: 13.419 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.250\n",
      " | Test Loss: 13.364 | Test Acc: 0.600\n",
      " | Test Loss: 0.424 | Test Acc: 86.287\n",
      " | Test Loss: 13.373 | Test Acc: 0.600\n",
      " | Test Loss: 0.423 | Test Acc: 86.350\n",
      " | Test Loss: 13.393 | Test Acc: 0.550\n",
      " | Test Loss: 0.420 | Test Acc: 86.275\n",
      " | Test Loss: 13.580 | Test Acc: 0.600\n",
      " | Test Loss: 0.418 | Test Acc: 86.450\n",
      " | Test Loss: 13.570 | Test Acc: 0.600\n",
      " | Test Loss: 0.418 | Test Acc: 86.638\n",
      " | Test Loss: 13.781 | Test Acc: 0.650\n",
      " | Test Loss: 0.421 | Test Acc: 86.612\n",
      " | Test Loss: 13.731 | Test Acc: 0.700\n",
      " | Test Loss: 0.418 | Test Acc: 86.450\n",
      " | Test Loss: 13.786 | Test Acc: 0.600\n",
      " | Test Loss: 0.422 | Test Acc: 86.463\n",
      " | Test Loss: 14.140 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.650\n",
      " | Test Loss: 14.069 | Test Acc: 0.700\n",
      " | Test Loss: 0.423 | Test Acc: 86.537\n",
      " | Test Loss: 14.123 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.675\n",
      " | Test Loss: 14.300 | Test Acc: 0.750\n",
      " | Test Loss: 0.425 | Test Acc: 86.500\n",
      " | Test Loss: 14.319 | Test Acc: 0.600\n",
      " | Test Loss: 0.420 | Test Acc: 86.825\n",
      " | Test Loss: 14.229 | Test Acc: 0.700\n",
      " | Test Loss: 0.425 | Test Acc: 86.775\n",
      " | Test Loss: 14.526 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.537\n",
      " | Test Loss: 14.549 | Test Acc: 0.700\n",
      " | Test Loss: 0.423 | Test Acc: 86.812\n",
      " | Test Loss: 14.497 | Test Acc: 0.650\n",
      " | Test Loss: 0.423 | Test Acc: 86.850\n",
      " | Test Loss: 14.632 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.812\n",
      " | Test Loss: 14.778 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.737\n",
      " | Test Loss: 14.787 | Test Acc: 0.750\n",
      " | Test Loss: 0.426 | Test Acc: 86.862\n",
      " | Test Loss: 14.844 | Test Acc: 0.750\n",
      " | Test Loss: 0.427 | Test Acc: 86.750\n",
      " | Test Loss: 14.902 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.800\n",
      " | Test Loss: 15.216 | Test Acc: 0.750\n",
      " | Test Loss: 0.426 | Test Acc: 87.025\n",
      " | Test Loss: 15.040 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 87.088\n",
      " | Test Loss: 15.092 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.950\n",
      " | Test Loss: 15.084 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.925\n",
      " | Test Loss: 15.199 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.812\n",
      " | Test Loss: 15.111 | Test Acc: 0.850\n",
      " | Test Loss: 0.424 | Test Acc: 87.000\n",
      " | Test Loss: 15.261 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.173 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.825\n",
      " | Test Loss: 15.223 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.325 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.912\n",
      " | Test Loss: 15.265 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.900\n",
      " | Test Loss: 15.115 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.125\n",
      " | Test Loss: 15.235 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 86.975\n",
      " | Test Loss: 15.302 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.050\n",
      " | Test Loss: 15.345 | Test Acc: 0.750\n",
      " | Test Loss: 0.424 | Test Acc: 86.987\n",
      " | Test Loss: 15.295 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.276 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 86.875\n",
      " | Test Loss: 15.285 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.950\n",
      " | Test Loss: 15.314 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.025\n",
      " | Test Loss: 15.228 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.975\n",
      " | Test Loss: 15.255 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.975\n",
      " | Test Loss: 15.257 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.975\n",
      " | Test Loss: 15.156 | Test Acc: 0.800\n",
      " | Test Loss: 0.420 | Test Acc: 87.062\n",
      " | Test Loss: 15.183 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.000\n",
      " | Test Loss: 15.287 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.297 | Test Acc: 0.800\n",
      " | Test Loss: 0.422 | Test Acc: 87.025\n",
      " | Test Loss: 15.249 | Test Acc: 0.750\n",
      " | Test Loss: 0.427 | Test Acc: 86.963\n",
      " | Test Loss: 15.349 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.888\n",
      " | Test Loss: 15.307 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.963\n",
      " | Test Loss: 15.313 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.950\n",
      " | Test Loss: 15.302 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 87.013\n",
      " | Test Loss: 15.304 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.950\n",
      " | Test Loss: 15.200 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.912\n",
      " | Test Loss: 15.330 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.050\n",
      " | Test Loss: 15.409 | Test Acc: 0.750\n",
      " | Test Loss: 0.422 | Test Acc: 87.025\n",
      " | Test Loss: 15.248 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 87.000\n",
      " | Test Loss: 15.236 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.013\n",
      " | Test Loss: 15.235 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.037\n",
      " | Test Loss: 15.341 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 87.000\n",
      " | Test Loss: 15.322 | Test Acc: 0.800\n",
      " | Test Loss: 0.424 | Test Acc: 87.025\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.427 | Test Acc: 86.900\n",
      " | Test Loss: 15.300 | Test Acc: 0.750\n",
      " | Test Loss: 0.424 | Test Acc: 86.900\n",
      " | Test Loss: 15.219 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.212 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.987\n",
      " | Test Loss: 15.283 | Test Acc: 0.800\n",
      " | Test Loss: 0.425 | Test Acc: 86.888\n",
      " | Test Loss: 15.293 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 87.075\n",
      " | Test Loss: 15.247 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 87.062\n",
      " | Test Loss: 15.341 | Test Acc: 0.800\n",
      " | Test Loss: 0.426 | Test Acc: 86.987\n",
      " | Test Loss: 15.313 | Test Acc: 0.800\n",
      " | Test Loss: 0.423 | Test Acc: 86.938\n",
      " | Test Loss: 15.135 | Test Acc: 0.800\n",
      " | Test Loss: 0.428 | Test Acc: 86.850\n",
      " | Test Loss: 15.289 | Test Acc: 0.750\n",
      " | Test Loss: 0.423 | Test Acc: 86.925\n",
      " | Test Loss: 15.191 | Test Acc: 0.800\n",
      " | Test Loss: 0.527 | Test Acc: 83.275\n",
      " | Test Loss: 0.888 | Test Acc: 73.150\n",
      " | Test Loss: 0.471 | Test Acc: 85.000\n",
      " | Test Loss: 2.341 | Test Acc: 25.100\n",
      " | Test Loss: 0.480 | Test Acc: 84.350\n",
      " | Test Loss: 0.837 | Test Acc: 71.750\n",
      " | Test Loss: 0.452 | Test Acc: 85.312\n",
      " | Test Loss: 2.059 | Test Acc: 27.650\n",
      " | Test Loss: 0.491 | Test Acc: 83.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | Test Loss: 1.284 | Test Acc: 42.000\n",
      " | Test Loss: 0.442 | Test Acc: 85.088\n",
      " | Test Loss: 1.955 | Test Acc: 35.800\n",
      " | Test Loss: 0.541 | Test Acc: 83.075\n",
      " | Test Loss: 1.814 | Test Acc: 35.650\n",
      " | Test Loss: 0.439 | Test Acc: 85.575\n",
      " | Test Loss: 1.428 | Test Acc: 39.700\n",
      " | Test Loss: 0.478 | Test Acc: 84.213\n",
      " | Test Loss: 2.273 | Test Acc: 31.450\n",
      " | Test Loss: 0.519 | Test Acc: 83.037\n",
      " | Test Loss: 1.661 | Test Acc: 35.650\n",
      " | Test Loss: 0.506 | Test Acc: 83.975\n",
      " | Test Loss: 2.242 | Test Acc: 34.350\n",
      " | Test Loss: 0.460 | Test Acc: 84.912\n",
      " | Test Loss: 1.122 | Test Acc: 50.850\n",
      " | Test Loss: 0.484 | Test Acc: 84.037\n",
      " | Test Loss: 2.383 | Test Acc: 37.000\n",
      " | Test Loss: 0.488 | Test Acc: 84.250\n",
      " | Test Loss: 1.684 | Test Acc: 39.400\n",
      " | Test Loss: 0.466 | Test Acc: 84.500\n",
      " | Test Loss: 2.231 | Test Acc: 29.450\n",
      " | Test Loss: 0.491 | Test Acc: 84.325\n",
      " | Test Loss: 1.515 | Test Acc: 40.850\n",
      " | Test Loss: 0.471 | Test Acc: 84.612\n",
      " | Test Loss: 2.201 | Test Acc: 35.700\n",
      " | Test Loss: 0.469 | Test Acc: 84.625\n",
      " | Test Loss: 1.793 | Test Acc: 36.700\n",
      " | Test Loss: 0.463 | Test Acc: 84.900\n",
      " | Test Loss: 2.241 | Test Acc: 35.500\n",
      " | Test Loss: 0.449 | Test Acc: 85.237\n",
      " | Test Loss: 1.778 | Test Acc: 33.650\n",
      " | Test Loss: 0.476 | Test Acc: 84.562\n",
      " | Test Loss: 1.917 | Test Acc: 37.400\n",
      " | Test Loss: 0.465 | Test Acc: 84.787\n",
      " | Test Loss: 2.094 | Test Acc: 35.950\n",
      " | Test Loss: 0.458 | Test Acc: 84.912\n",
      " | Test Loss: 1.960 | Test Acc: 40.350\n",
      " | Test Loss: 0.535 | Test Acc: 82.263\n",
      " | Test Loss: 1.855 | Test Acc: 42.150\n",
      " | Test Loss: 0.480 | Test Acc: 84.037\n",
      " | Test Loss: 1.676 | Test Acc: 38.000\n",
      " | Test Loss: 0.467 | Test Acc: 84.600\n",
      " | Test Loss: 0.537 | Test Acc: 81.550\n",
      " | Test Loss: 0.480 | Test Acc: 84.200\n",
      " | Test Loss: 0.704 | Test Acc: 73.350\n",
      " | Test Loss: 0.484 | Test Acc: 84.188\n",
      " | Test Loss: 0.679 | Test Acc: 74.750\n",
      " | Test Loss: 0.476 | Test Acc: 84.513\n",
      " | Test Loss: 0.671 | Test Acc: 75.400\n",
      " | Test Loss: 0.479 | Test Acc: 84.263\n",
      " | Test Loss: 0.643 | Test Acc: 76.400\n",
      " | Test Loss: 0.495 | Test Acc: 83.938\n",
      " | Test Loss: 3.701 | Test Acc: 9.000\n",
      " | Test Loss: 0.469 | Test Acc: 84.662\n",
      " | Test Loss: 4.208 | Test Acc: 6.100\n",
      " | Test Loss: 0.446 | Test Acc: 85.487\n",
      " | Test Loss: 4.282 | Test Acc: 4.750\n",
      " | Test Loss: 0.444 | Test Acc: 85.612\n",
      " | Test Loss: 4.712 | Test Acc: 4.150\n",
      " | Test Loss: 0.428 | Test Acc: 85.912\n",
      " | Test Loss: 4.699 | Test Acc: 3.150\n",
      " | Test Loss: 0.443 | Test Acc: 85.300\n",
      " | Test Loss: 5.184 | Test Acc: 2.850\n",
      " | Test Loss: 0.431 | Test Acc: 85.625\n",
      " | Test Loss: 5.308 | Test Acc: 2.950\n",
      " | Test Loss: 0.425 | Test Acc: 86.125\n",
      " | Test Loss: 5.503 | Test Acc: 2.850\n",
      " | Test Loss: 0.422 | Test Acc: 85.987\n",
      " | Test Loss: 5.633 | Test Acc: 2.350\n",
      " | Test Loss: 0.419 | Test Acc: 86.375\n",
      " | Test Loss: 5.774 | Test Acc: 2.000\n",
      " | Test Loss: 0.429 | Test Acc: 85.925\n",
      " | Test Loss: 6.125 | Test Acc: 2.100\n",
      " | Test Loss: 0.414 | Test Acc: 86.388\n",
      " | Test Loss: 5.880 | Test Acc: 1.700\n",
      " | Test Loss: 0.426 | Test Acc: 86.200\n",
      " | Test Loss: 6.211 | Test Acc: 1.700\n",
      " | Test Loss: 0.419 | Test Acc: 86.388\n",
      " | Test Loss: 6.341 | Test Acc: 1.700\n",
      " | Test Loss: 0.422 | Test Acc: 86.425\n",
      " | Test Loss: 6.522 | Test Acc: 1.700\n",
      " | Test Loss: 0.417 | Test Acc: 86.513\n",
      " | Test Loss: 6.337 | Test Acc: 1.850\n",
      " | Test Loss: 0.425 | Test Acc: 86.275\n",
      " | Test Loss: 6.629 | Test Acc: 1.650\n",
      " | Test Loss: 0.422 | Test Acc: 86.188\n",
      " | Test Loss: 6.719 | Test Acc: 1.750\n",
      " | Test Loss: 0.409 | Test Acc: 86.662\n",
      " | Test Loss: 6.723 | Test Acc: 1.600\n",
      " | Test Loss: 0.416 | Test Acc: 86.525\n",
      " | Test Loss: 6.847 | Test Acc: 1.450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(12.75, 0.0),\n",
       " (43.1125, 1.75),\n",
       " (56.0375, 1.6),\n",
       " (60.55, 2.3),\n",
       " (62.2375, 2.6),\n",
       " (66.9375, 1.85),\n",
       " (69.225, 1.9),\n",
       " (72.7875, 1.4),\n",
       " (74.875, 1.35),\n",
       " (75.5375, 1.1),\n",
       " (78.325, 0.85),\n",
       " (78.5875, 1.0),\n",
       " (79.9375, 0.95),\n",
       " (80.7, 0.85),\n",
       " (81.55, 0.8),\n",
       " (81.125, 0.85),\n",
       " (82.8375, 0.8),\n",
       " (83.1625, 0.75),\n",
       " (83.9625, 0.8),\n",
       " (83.9875, 0.75),\n",
       " (84.525, 0.7),\n",
       " (84.9875, 0.75),\n",
       " (84.775, 0.75),\n",
       " (85.3875, 0.65),\n",
       " (85.4875, 0.6),\n",
       " (86.075, 0.6),\n",
       " (86.25, 0.6),\n",
       " (86.2875, 0.6),\n",
       " (86.35, 0.55),\n",
       " (86.275, 0.6),\n",
       " (86.45, 0.6),\n",
       " (86.6375, 0.65),\n",
       " (86.6125, 0.7),\n",
       " (86.45, 0.6),\n",
       " (86.4625, 0.75),\n",
       " (86.65, 0.7),\n",
       " (86.5375, 0.75),\n",
       " (86.675, 0.75),\n",
       " (86.5, 0.6),\n",
       " (86.825, 0.7),\n",
       " (86.775, 0.75),\n",
       " (86.5375, 0.7),\n",
       " (86.8125, 0.65),\n",
       " (86.85, 0.8),\n",
       " (86.8125, 0.8),\n",
       " (86.7375, 0.75),\n",
       " (86.8625, 0.75),\n",
       " (86.75, 0.8),\n",
       " (86.8, 0.75),\n",
       " (87.025, 0.8),\n",
       " (87.0875, 0.8),\n",
       " (86.95, 0.8),\n",
       " (86.925, 0.8),\n",
       " (86.8125, 0.85),\n",
       " (87.0, 0.8),\n",
       " (86.925, 0.8),\n",
       " (86.825, 0.8),\n",
       " (86.9875, 0.8),\n",
       " (86.9125, 0.75),\n",
       " (86.9, 0.8),\n",
       " (87.125, 0.75),\n",
       " (86.975, 0.8),\n",
       " (87.05, 0.75),\n",
       " (86.9875, 0.8),\n",
       " (87.0125, 0.8),\n",
       " (86.875, 0.8),\n",
       " (86.95, 0.75),\n",
       " (87.025, 0.8),\n",
       " (86.975, 0.8),\n",
       " (86.975, 0.8),\n",
       " (86.975, 0.8),\n",
       " (87.0625, 0.8),\n",
       " (87.0, 0.8),\n",
       " (87.0125, 0.8),\n",
       " (87.025, 0.75),\n",
       " (86.9625, 0.8),\n",
       " (86.8875, 0.8),\n",
       " (86.9625, 0.75),\n",
       " (86.95, 0.8),\n",
       " (87.0125, 0.8),\n",
       " (86.95, 0.8),\n",
       " (86.9125, 0.8),\n",
       " (87.05, 0.75),\n",
       " (87.025, 0.8),\n",
       " (87.0, 0.75),\n",
       " (87.0125, 0.8),\n",
       " (87.0375, 0.75),\n",
       " (87.0, 0.8),\n",
       " (87.025, 0.8),\n",
       " (86.9, 0.75),\n",
       " (86.9, 0.75),\n",
       " (86.9375, 0.8),\n",
       " (86.9875, 0.8),\n",
       " (86.8875, 0.8),\n",
       " (87.075, 0.8),\n",
       " (87.0625, 0.8),\n",
       " (86.9875, 0.8),\n",
       " (86.9375, 0.8),\n",
       " (86.85, 0.75),\n",
       " (86.925, 0.8),\n",
       " (83.275, 73.15),\n",
       " (85.0, 25.1),\n",
       " (84.35, 71.75),\n",
       " (85.3125, 27.65),\n",
       " (83.75, 42.0),\n",
       " (85.0875, 35.8),\n",
       " (83.075, 35.65),\n",
       " (85.575, 39.7),\n",
       " (84.2125, 31.45),\n",
       " (83.0375, 35.65),\n",
       " (83.975, 34.35),\n",
       " (84.9125, 50.85),\n",
       " (84.0375, 37.0),\n",
       " (84.25, 39.4),\n",
       " (84.5, 29.45),\n",
       " (84.325, 40.85),\n",
       " (84.6125, 35.7),\n",
       " (84.625, 36.7),\n",
       " (84.9, 35.5),\n",
       " (85.2375, 33.65),\n",
       " (84.5625, 37.4),\n",
       " (84.7875, 35.95),\n",
       " (84.9125, 40.35),\n",
       " (82.2625, 42.15),\n",
       " (84.0375, 38.0),\n",
       " (84.6, 81.55),\n",
       " (84.2, 73.35),\n",
       " (84.1875, 74.75),\n",
       " (84.5125, 75.4),\n",
       " (84.2625, 76.4),\n",
       " (83.9375, 9.0),\n",
       " (84.6625, 6.1),\n",
       " (85.4875, 4.75),\n",
       " (85.6125, 4.15),\n",
       " (85.9125, 3.15),\n",
       " (85.3, 2.85),\n",
       " (85.625, 2.95),\n",
       " (86.125, 2.85),\n",
       " (85.9875, 2.35),\n",
       " (86.375, 2.0),\n",
       " (85.925, 2.1),\n",
       " (86.3875, 1.7),\n",
       " (86.2, 1.7),\n",
       " (86.3875, 1.7),\n",
       " (86.425, 1.7),\n",
       " (86.5125, 1.85),\n",
       " (86.275, 1.65),\n",
       " (86.1875, 1.75),\n",
       " (86.6625, 1.6),\n",
       " (86.525, 1.45)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_defense_switch_classes = load_result(\"switch_classes_no_defense_multiple_final_fixed_2.pickle\")\n",
    "evaluate_switch_classes_attack_weights = make_evaluate_switch_classes_attack_weights(0,1)\n",
    "d = [evaluate_switch_classes_attack_weights(no_defense_switch_classes.avg_weight_history[r]) for r in range(len(no_defense_switch_classes.avg_weight_history))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_better(trackers, num_epochs, title, y_axis_lab, should_average=False, n = 100, add_attack_region = None, alpha = .2):\n",
    "  avg_fn = (lambda x : moving_average(x, n)) if should_average else (lambda x : x) \n",
    "  x = np.arange(1, len(avg_fn(list(trackers.values())[0])) + 1)\n",
    "  x = x / (len(x)/num_epochs)\n",
    "  ax = plt.subplot(1,1,1)\n",
    "  plt.title(title)\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(y_axis_lab)\n",
    "  # plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
    "  ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%1.0f'))\n",
    "\n",
    "  for lab, t in trackers.items(): \n",
    "    l1, = ax.plot(x, avg_fn(t), label = lab)\n",
    "  _ = plt.legend()\n",
    "  sample_tracker = list(trackers.items())[0][1]\n",
    "  if add_attack_region is not None: \n",
    "      ax.fill_between(range(add_attack_region[0], add_attack_region[1]), 0, 100, alpha=alpha,  color='red')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_acc = [x[0] for x in d]\n",
    "other_acc = [x[1] for x in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = {\n",
    "    \"Misclassifications (Attacked Cases)\" : other_acc,   \n",
    "    \"Accuracy on Remaining Classes\" : regular_acc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plot_better(tracks, len(other_acc), title=\"Misclassification Attack No Defense\", y_axis_lab=\"%\", should_average=True,  add_attack_region =(100,130), n=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_against_noise = load_result(\"sigmoid_against_noise_attack_final.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_defense_noise = load_result(\"noise_attack_1_no_defense.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = sigmoid_against_noise.test_accuracy[100:]\n",
    "nd = no_defense_noise.test_accuracy[100:]\n",
    "tracks2 = {\n",
    "    \"Sigmoid Defense\" : sd,   \n",
    "    \"No Defense\" : nd\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA090lEQVR4nO3deXwcdf348dd7d3NvjjZJD3rQQEsLpbSl5VAUUAQElEugIGoBtX4RkUsFj58cFgWtghzit8ilVG5EQL5SxFoQy9G0KVcLbaGV9EzSK3f2eP/+mNntpuTu7s4meT8fj33M7GdmZz6zm8x7Pp+ZeY+oKsYYYwyAz+sKGGOMyRwWFIwxxsRZUDDGGBNnQcEYY0ycBQVjjDFxFhSMMcbEWVAwA5aIjBMRFZGA13XpiIj8SET+4HU9ksn9vsd7XQ/TdxYUTNKJyAUi8paINInIZhG5S0RKUrSu0SLyhIjUishOEXlbRC5IwnKPFZHqbua5390JHp5QNl5EenTzj6r+XFW/sbd17aRuF7h1m7VH+XUi8uAeZf8SkZTUw/Q/FhRMUonIVcDNwPeBYuBIYF/gBRHJTsEq/wR85K6jFPgqsCUF6+nMNmBuGtfXU7Nx6vY1ryti+hlVtZe9kvICioAG4Jw9yoNADXCR+/464FHgj0A98A4wM2H+fYAn3M98CHy3i3U2ANM6mTYOUJwd5H+BWuDHCdNzgFuBje7rVresAGgGou7yG4B9Olj+/cBvgM3AMW7ZeOffqt22PI2zg14DfDNh2nXAg+54LvAgUAfsAN4AhrvTioF7gE3ABpwg5O/iO9nXrfuXgDAwwi3/PNAGhNxtWgHcCESAFrfsDnfe3+IE211AJfDphOX7gR8Ba93frxIY405TYLw7/il3Gcd6/bdpr56/rKVgkumTODu3JxMLVbUBeA44PqH4VOBhoARnp3kHgIj4gGdwdlijgOOAy0XkxE7W+Spwp4icKyJjO5nnU8BEd1k/FZED3fIf47RkpgFTgcOBn6hqI3ASsFFVg+5rYyfLbgJ+jrNz7cjDQDVOcDgL+LmIfLaD+Wbj7PzH4LR4/gcnMIETfMI4AWc6cALQVXfP14ClqvoEsBI4H0BV/+7W9RF3m6aq6o+Bl4HvuGXfcZfxhvu9DAX+DDwmIrnutCuB84CTcQ4ELnK/hzgR+TzwEPAlVf1XF3U1GcaCgkmmMqBWVcMdTNvkTo/5t6o+p6oRnC6gqW75YUC5qt6gqm2q+gFwN3BuJ+s8G2en9v+AD0WkSkQO22Oe61W1WVVX4ASb2LrOB25Q1a2qWgNcj9P91Fv/C4wVkZMSC0VkDHAUcLWqtqhqFfAHOu7SCeEEg/GqGlHVSlXdJSLDcXa+l6tqo6puBW6h8+8Dd/l/dsf/3Mn6uqSqD6pqnaqGVfXXOC2oie7kb+AEz/fUsUJV6xI+fjbOd3KSqr7e23Ubb1lQMMlUC5R1crXPSHd6zOaE8SYg1/3cvsA+IrIj9sLpqhje0QpVdbuqXqOqk915qoCnRES6WFfQHd8HWJ8wbb1b1iuq2gr8zH0l2gfYpqr1e6xjVAeL+RPwPPCwiGwUkV+KSBbO95EFbEr4Pv4XGNZRXUTkKKACp4UCTlCYIiLTerNNIvI9EVnpnrzfgdOKiQX1MThdR525HHhUVd/uzTpNZrCgYJJpCdAKnJlYKCJBnO6YF3uwjI+AD1W1JOFVqKond/dBVa0F5uHsjIf2YF0bcXa6MWPdMnD6xnvjPpyusMRt3wgMFZHCPdaxYc8Pq2pIVa9X1YNwuuG+gHOE/xHOd1qW8H0UuUGwI7MBAapEZDPwWkJ5Z9vVrkxEPg38ADgHGKKqJcBOd7m4ddq/k/WD01I4XUQu62Iek6EsKJikUdWdOF0wt4vI50UkS0TG4ZxUrsY5Gu7O60C9iFwtInki4heRgzvoEgJARG52pwfcne/FwJo9ujM68xDwExEpF5Ey4Kc4J3vBuYKpVESKe7Ac3C6za4GrE8o+Av4D/EJEckXkEODrCetI3I7PiMgUEfHjnNwNAVFV3QQsBH4tIkUi4hOR/UXkmA6WkYuzI5+Dcz4g9roU+LLbEtsCjHPP3cRsAfZLeF+Icw6jBgiIyE9xzh3E/AH4mYhMEMchIlKaMH0jzvmby0Tk4i6/OJNxLCiYpFLVX+J098zD2bm9hnNkeZzbzdLd5yM4R8nTcK48qsXZCXW2c84H/oJzxc4HOEf+p/awunOBpcCbwFvAMrcMVV2FEzQ+cLttetKt9BDOuZNE5+FcBbXRree1qvqPDj47Angc5ztbCSxmdxD9GpANvAtsd+cb2cEyTsc5Of1HVd0cewH3AgGcq48ec+etE5Fl7vhvgbNEZLuI3IbTjfV34H2c7q4WnN8w5jc4gX6hW997gLzEiqjqf3ECwzV2D0T/Iqr2kB1jjDEOaykYY4yJs6BgjDEmzoKCMcaYOAsKxhhj4jIypXBPlZWV6bhx47yuhjHG9CuVlZW1qlre0bR+HRTGjRvH0qVLva6GMcb0KyKyvrNp1n1kjDEmzoKCMcaYOAsKxhhj4iwoGGOMibOgYIwxJi5lQUFE7hWRrSLydkLZUBF5QURWu8MhbrmIyG0iskZE3hSRQ1NVL2OMMZ1LZUvhfpysjImuAV5U1Qk4ufWvcctPAia4rznAXSmslzHGmE6kLCio6ks4DytPdBrwgDv+AE6q31j5H91H+70KlIhIR6mBjTHGpFC6b14b7j40BJxHJMYesTiK9vnaq92yPXPTIyJzcFoTjB3b2XPaTUZ79FGo68EzcEpL4ZxzUl8fY0ycZ3c0q6qKSK8f5qCq84H5ADNnzrSHQfRHdXUwenT381VXp74uxph20n310ZZYt5A73OqWb8B5GHjMaDp4jq0xxpjUSndQeJrdDxCfDfw1ofxr7lVIRwI7E7qZjDHGpEnKuo9E5CHgWKBMRKpxHmp+E/CoiHwd59mvsQ7j54CTgTVAE3BhqupljDGmcykLCqp6XieTjutgXgUuSVVdjDHG9Izd0WyMMSbOgoIxxpg4CwrGGGPiLCgYY4yJs6BgjDEmzoKCMcaYOAsKxhhj4iwoGGOMibOgYIwxJs6CgjHGmDgLCsYYY+IsKBhjjInz7CE7xhgzUESjSlMoQmNrmIbWME2tEWfYFqaxLYIAfp/g9wmB+NCHzwcBn699ud8Z94kzj9+f+Jndn80OOJ9LNgsKxphBoTUcYVtjG3UNbWxrdF51jW1sa2ylrsEZ397YRiiqoEpUQVGiUVBAVdFYmUI4EqWhNUJTW5imtkjat2fu6QfzlSP3TfpyLSgYsxdUldZwlLZIlNaQM2wLO6/WcGT3eCRKKBwlmBugtCCH0mA2Q/Kz9+pIT1UJRZQsvyCS/CPG3mpqC7NlVytbdrWwZVcLW2Pj9a3U1LdQFsxh/LAg+5cHGT8sSEVZAblZ/h4te2dTiFWbd/HelnpWbqrnvc27WL21gdZwFJ+AIM5QBBEQnPFYWSgcpb413OGy/T5hSH42Ze5vkp/jwyfgE4kvR4Td6/E5w4BfKMgJUJDtd4cBZ5jjpyA7QH6On2BOgPxsZxvDUSXivmLj4YgS1dj7KKGIEk2cHk18H21XPn1sSXJ+uD1YUDBmD63hCB/WNrJ1Vyvbm5yjx21NIXfovN+e8L4tHO3TekRgSH42pQXZlAaz48GitCCHvGwfDS1hdrWEqW8JU98Scoat7tAtC0WUwtwAFWUF7FtaQEVpPuNi42UFDMnPSmrA2N7Yxntb6nlvcz3vbalnXW1jPAB0tNPNy/IzojiX0oJs3qzeyd/e2oTq7u0fMySf/csL2gWL/OwA72+pZ+XmXc56NtezaWdLfJnFeVlMGlHI6dNGkZ/jB4WoexQfO7pXdY/scaYFfD73e85hqPt9Dy1wvvui3Cx8KeiG6a8sKKSKKmh095CE9ygEcsHXs6Oknq3OOWLd2Rza/WoKtX/fHGJXc4iG1jChiHNU0haJEnbHJdzMqNA6KsIfsF/4Qyqi68mTNsTnd18+fD6/8/L78PkC+Px+/D4f0exCaosOYkvhZDbmT6SZHFpDUVoTjphbw1GiUWV2azZTk7Td2xvbuHhBJbuaw1SUF7BfmbMzrCgrYL+yIMX5WV1+Zxt2NPPe5npWua/3Nu/ig5pGwlFtN68IlORlMSQ/myEF2YwqyWPKqCKG5GdTlJdFTsDp440Ns/3+3ePxMh/1LWHqGlvZ1thGbUMbdQ2t8e6MlZt3UdfQxs7mUHy9hTkBCnMDBHMDFOZmURbMoaIsSGGuU16QHaCmvpV1dY1UfbSdv725kcSqxwLGuNICRg3Jozgvi6LcLIryAu4wi6LcgDvMIjvgXHvS1BZm9ZaG+M7//S3O91NT3xpfdnFeFvuXFzBpRBFHH5DD8KJchhflMLwwl2HueDAn0C4otYScgLtmawNraxrcYSP/WVtH6x7BNcsv7F8e5Mj9Spk4opBJIwqZNKKI4UU5GdEyGqgsKHShqS3Mmq0NNLZG8An4Qw3kNW0gu3EjOQ0byWmsJrthI9nuMNBci2jP+xbbfLmEAgVEs4JoTiG+nCICeYVkFxTjyymEnEII5EC4hXBrM81NDbQ0NdDa0ki4tZlIaxOEm5FwC/5IK7s0l606hK1aQg3FbNUh1GgxNVrCVkpoyS4jJ6+Awhw/w307mBj9kP2jThDYN/QBI8Ib8OH8Y7b48tmcvz/NvlKikQjRaASNRoiGIkS1DaIR0Cg+ovhQhko90+Q5AMLq430dQ1V0f95mPCv9B7AhMIZAIIv61jCL28p5ZjTsk7N3v080qlz+SBXL1u/gE/uX8s6Gnfz97c1EEvaKpQXZ8SBRUV5AMCcQP/p8b3N9u6PbUSV5HDiykOMPGs7EEUXsU5xLSb5zRFmcl5WSk3odCUWiNIciBLMDvT6CbQ1HqN7ezLraRj6sbWR9XRPr6hpZ9t/t/O2tTe2+m47kZvkoyA6wraktfkSfm+XjgOGFHHNAOZNGFHLA8EImjihkWGHvd865WX4OHFnEgSOL2pVHosrGHc2sqWmgoSXMxBGFVJQVkOVP4QWSqhBqgtZ659WyC9rqnWm+LPBngS+we+jLAn9g9zTxuf8Hzv9CfDwadYeRhGF0j3n2mBZfTte/TzsjDoYh45L+tYj2phIZZubMmbp06dK9Xo6qUr29mZUbd7L+o/Xsql5JtHYNhY3rqZBNjJEaRkkNxdLU7nOtGmCjlrJBy9ioZdRQTBg/IERVUIQoztDv95EV8JPl9xPw+/BFWpC2BnIijQSlhSDNBKWZIC0EaSLoc8qyCdNKFi2aRTM5tGg2LWTTQhZRfy4E8vBl5xHIySNIE0XhOvJba8lp24ZoB90aOcVOC6V52+6y4rHOH9iIKTD8YGe8ZBz4uv6HDEeiNLY5V1m0hCLktdZRULuCnC1VZG2uxLdxGdK6y5k5Owj7TGf7kEP40qsTKSwo5ZEpSm5Xq6iuhosv7nTyb/+xmlv+8T43nnEw5x/hnHBrC0f5aHsTH9Y08kFtAx/WNvJBjbOD3Ooe5RblBpg0ooiJI5yd24EjnR1dYW7nrYqBQFVpDkXY1RxmV4vTanSGTlfUrpawWxZmZHEuBwx3js7HDM1PW0DsovLQsgN2VsPODbDzI6jfBKGW3TvXaDhhPPF9GMJt7s5/l/Nq2eW878VBXMY55Tdw2Nf79FERqVTVmR1OG4xB4cMNW1j59jIaNq5Ea9dS0LCOMbqRCtlEkTTH5wtLFs2F+xIqHEtrwShaCvahJX8fWgr2oTl/H1pzy1Ak3pfpE8jPDjgnlxJOMuVnBzr9p2pqC1Nb30ZNQyu17qum3h2vb6OlLcSIknz2KclzX7mMKsljRHEuOYEuup+iEWishYYtzqt+8+7xcCsMO8jZ+Q+fDHlDev0d9kg0CtvWwoZKqF7qDDcuY032iXxu12zOG678YnwXf39dBIXF79dwwX2vc8b0Ufz67Kk9OmJtaA3T2Bru0xGu6YQqhFt272RjO93YkXe8zD0C92e3PwL3ZyccjbvvI627d/y7NuwOBKHG9usWP2TlOUNf7BVw3/sSxt3l5xY7re+cImeY6w5zCp2DpZxCyAk6y46E3MAScsfd97HxSMjZdp9v9/plj3Gf33mfON5ufv/Hy6UXLaOiUVBQ2qefraugMCi7j2pfvJWTP/gdAFGEnTkjaCmqoLX807SOnkTO8AOgdAKB4tEUJrHfvyP52QHGlgYYW5qf3AX7/FA43Hl5xeeDsgnOa+q5Ttkv92O87uDbo5XfVQtTg8q5I3q32OrtTVz28HImDi/kxtOn9HgHH8xxArbppWgUdlVD3RqoW+sO3dfODc5OsjuBXGcYCfX86Dw43NnxlU+E8Z9zxotHQfEYZzw4LKnn5YxjUP6HTDjmy9QdNIOhYw7CN3Q/hmTlel2lwSNvCDQ0c9VY5a0G+OkHwqQCZVphzz7eGo5wyYJlRCLKXV+ZQV627RR6JRKCtkYINTv96aEmaGtyjsJDzc54WwPsWL87CGz7wGkNxGQHoXR/GDUDDjp99xF4h0fi7rg/oWsuGm1/BB5JHA87O/qifZzzaSbtBmVQKNl3Cuw7xetqDE55Q6F+G36B3x6gfHGF8O1VwjPTlNIedOnPfXYlK6p38vuvzKCirCD19c1kqtBY43QJNtZCU53zio/XQmPd7vHmHT07qgenK2doBZSOh/HHOcPYKzjcuRyrr3w+8OXYTj9DDcqgYDyUNwSiGwAYmgX/O0n50lvCpe8Jf5ysBLrY1/xleTV/enU93zp6Pz5/cC/7nPorVWja5hy1b1vrHrmv3X0E39bQwYcE8odCfinkl0HZeMg/wvnuswogOx+y3Fd2vtMvv2d5cLhzpY0ZdOxXN+mVNwSiu6/iOjgIc/dTvr/Gx6/Www/HdXziedXmXfzwybc4vGIo3z9xYrpqmzyqUPu+c2QfanG6bcItTpdNbBhqhnCzM7152+4A0LJz93LEByX7Ot03Yz8BQ/eDopG7A0BBmfMdW1+76SMLCia98oaAtr+09+zhsKJB+d8NwiFB5ZSy9h+pbwlx8YPLKMzN4o4vTyeQymvXkykSgnX/hvf+z3nt/G/3n/FnQyDP6Z8v3Q8OPssJAEP3d4Yl+0IgO/V1N4OWBQWTXnlDQFtBwyC7//x+WqG80wjfXy0ckK9McC/GUlW+/9ib/HdbEw9980iGFWb4RQEtO2H1C04QWP0CtO50rrzZ7zNw9Pecm42y8pyyrDx3PA+ycpN+l7sxfWFBwaRX7J6IcCNkFceLs31w10TlCyuEb60UnpqqFAH3/PtD/v7OZn5yyoEcXjHUmzp3Z8dHbmvgOadlEA05XTkHfREmnuwEhOwkX3JsTIpYUDDple/u2CPtgwLAiBy4c6Ly5beFq1YL38jK4Rf/t4qTDh7B1z9Vkb46RsKw/hXnpqmWnc6dtC07nVdzwnisPHayt3QCfOLbMPEUGD3TjvpNv2RBwaRXXokzDHd01QwcUQw/qlB+9qGPxQxjbFk+vzzrkPTcgVzzPlQ9CCsegYbN7aflFDv9/HnFkFviXK6ZW+JsT9E+MOEE5yY9Y/o5CwomvRK7jzpx0Uh4u0FZWKv8/iszUpuTqGUXvPMkLH8Qqt9wUg1MOB6mfRlGHOLs9HOK7KjfDBqeBAURuQL4Bs4Djd4CLgRGAg8DpUAl8FVVbfOifiaFYkEh0nFLAZz7on4zQanP3UDRiB7e6twb0SisexmqFsC7TzuXgZZNhONvgEPO9TY1iDEeS3tQEJFRwHeBg1S1WUQeBc4FTgZuUdWHReT3wNeBu9JdP5Ni8ZZC50EBnMBQ5EtyssbGOnh9PlT92bk8NKcYpp0H0853UjZYkjxjPOs+CgB5IhIC8oFNwGeBL7vTHwCuw4LCwJNTDEi3QSHptq+HP50O2z6E/Y6Fz10Lk05xLgk1xsSlPSio6gYRmQf8F2gGFuJ0F+1Q1dgTT6qBUR19XkTmAHMAxo4dm/oKm+Ty+UDynKuP0mXLu/CnM5w7hy96HsYekb51G9PPpP3WUBEZApwGVAD7AAXA53v6eVWdr6ozVXVmeXl5imppUsqXn76Wwn9fg/s+73QNXfR3CwjGdMOLfAGfAz5U1RpVDQFPAkcBJSLxW1xHAxs8qJtJB1+aWgrvL4Q/nubcSHbR8zDswNSv05h+zoug8F/gSBHJF+fi8+OAd4FFwFnuPLOBv3pQN5MOkoaWwpuPwsPnQfkBTkAYsm9q12fMAJH2oKCqrwGPA8twLkf1AfOBq4ErRWQNzmWp96S7biZNfHmpDQqv/h6e/KaTRXT2sxC0bkZjesqTq49U9Vrg2j2KPwAO96A6Jt0kPzXdR6qw6Ofw0i9h0hfgS/c4ieaMMT1mdzSb9PPlQ6TJeVavJOlO4WgEnvseLL0Xpn8VvnCrPSTGmD6w/xqTfj733oBIEwSScMdyuBWenAPvPgVHXQ6fu85uRDOmjywomPQTN410uGHvg4IqPDob3v8/OGEufPLSva+fMYNYP3mElRlQfLGgkITzCq31TkD45HctIBiTBBYUTPrFu4/q935ZTbXO0O5BMCYpLCiY9JMkthQa3aBQYJedGpMMFhRM+sVaCsm4V6Gxxhnml+79sowxFhSMByQPkOTcq2AtBWOSyoKCST/xgT9JqS5iLYWCsr1fljHGgoLxSKAgeS2F7KA9F8GYJLGgYLzhD0I4SVcfWSvBmKSxoGC8EShI0tVHNXY+wZgksqBgvOEPQiQZ5xRqneclGGOSwoKC8UYgmLz7FKz7yJiksaBgvBEIuplSo31fRjTqnlOw7iNjksWCgvGGvwBQJzD0VcsOiIatpWBMEllQMN4IBJ3h3tyr0FTnDK2lYEzSWFAw3ggUOMO9OdlsN64Zk3QWFIw3/LGWwl6cbI6luLCrj4xJGgsKxhvJ6D6KtxSs+8iYZLGgYLwRCwp7k+oi3lKwDKnGJIsFBeMNf+yZCnuR6qKpFnKLIZCdnDoZYywoGI+I382UujctBUtxYUyyWVAw3vEX7OXVR5biwphks6BgvLO3qS4sxYUxSWdBwXgnsJdJ8az7yJiks6BgvOPfi/TZ0YhzR7O1FIxJKgsKxjuBYN/vU2jeDqi1FIxJMgsKxjt+95GcfcmUaikujEkJCwrGO4EgTqbU5t5/1lJcGJMSngQFESkRkcdFZJWIrBSRT4jIUBF5QURWu8MhXtTNpNHeJMWzFBfGpIRXLYXfAn9X1UnAVGAlcA3woqpOAF5035uBzF/oDPtyXiHWUrCgYExSpT0oiEgxcDRwD4CqtqnqDuA04AF3tgeA09NdN5NmsZZCX65AaqoFBPKHJrVKxgx2XrQUKoAa4D4RWS4ifxCRAmC4qm5y59kMDO/owyIyR0SWisjSmpqaNFXZpEQ8KV4fu4/yh4LPn9w6GTPIeREUAsChwF2qOh1oZI+uIlVVQDv6sKrOV9WZqjqzvNy6Dvo1f6yl0MegYF1HxiSdF0GhGqhW1dfc94/jBIktIjISwB1u9aBuJp32pvuosc6uPDImBdIeFFR1M/CRiEx0i44D3gWeBma7ZbOBv6a7bibNJAC+3L53H9k9CsYkXcCj9V4KLBCRbOAD4EKcAPWoiHwdWA+c41HdTDoFCvvYUrDuI2NSwZOgoKpVwMwOJh2X5qoYrwX6kD47EoKWHdZSMCYF7I5m4y1/H/IfNdU5QwsKxiSdBQXjrUBB74OC3c1sTMpYUDDe8gedpHi9YXmPjEkZCwrGW7GWQm8ypVqKC2NSxoKC8VYsU2q0peefaYoFBWspGJNsFhSMt/xuqovenFdorAHxQ25JSqpkzGBmQcF4K9CHVBexG9d89udrTLLZf5XxVjwpXi9ONluKC2NSxoKC8VZfu4/sfIIxKWFBwXirz91HduWRMalgQcF4y9+H7qOmOmspGJMiFhSMt3xuptSethRCLdC6y4KCMSnSq6AgIkeKyN9F5F8icnqK6mQGm96kumiyG9eMSaUus6SKyAj3+QcxVwJnAAK8BjyVuqqZQaM3qS4sxYUxKdVd6uzfi8gy4Jeq2gLsAM4CosCuFNfNDBa9aSlYigtjUqrL7iNVPR1YDjwrIl8DLgdygFLg9BTXzQwWvWopxDKkWkvBmFTo9pyCqj4DnAgUA38B3lfV21S1JtWVM4NEoBfPVLC8R8akVJdBQUROFZFFwN+Bt4FZwGki8rCI7J+OCppBIFDgPJJTtft5G2vAnw05RamvlzGDUHfnFOYChwN5wPOqejhwlYhMAG4Ezk1x/cxg4A8CEYg2gz+/63kb65zzCSJpqZoxg013QWEncCaQD2yNFarqaiwgmGSJ5T8KN/YgKNRAfmnq62TMINXdOYUzcE4qB4Avp746ZlCKpbqI9OC8gqW4MCalumwpqGotcHua6mIGK39CS6E7TbVQdkBq62PMIGZpLoz3Ar3IlNpYa1ceGZNCFhSM9+LdR920FNoaIdRkQcGYFLKgYLzn72H6bEtxYUzKWVAw3vNlgy+n50HBTjQbkzIWFExm8Bd0f/VRPMWFBQVjUsWCgskMgWD3Vx/FU1zYfQrGpIoFBZMZAkFrKRiTASwomMzgL+i+pdBYC1n5kF2QnjoZMwhZUDCZoSeZUhtr7cojY1LMs6AgIn4RWS4iz7rvK0TkNRFZIyKPiEi2V3UzHoidaO4qU2pjjd2jYEyKedlSuAxYmfD+ZuAWVR0PbAe+7kmtjDcCQdAIRFs7n6ep1s4nGJNingQFERkNnAL8wX0vwGeBx91ZHsCe7Da49CQpnqW4MCblvGop3Ar8AOdZz+BkYt2hqmH3fTUwqqMPisgcEVkqIktrauzhbwNGd0nxVK37yJg0SHtQEJEvAFtVtbIvn1fV+ao6U1VnlpdbV8KA0V1SvNZ6iLRZ95ExKdbdQ3ZS4SjgVBE5GcgFioDfAiUiEnBbC6OBDR7UzXglFhQ66z6K3aNgVx8Zk1Jpbymo6g9VdbSqjsN5ets/VfV8YBFwljvbbOCv6a6b8VB3SfEs75ExaZFJ9ylcDVwpImtwzjHc43F9TDoFujmnEE9xYS0FY1LJi+6jOFX9F/Avd/wD4HAv62M85MsGyeq++8iCgjEplUktBTPYdZUUz84pGJMWFhRM5ugqKV5jHeQUQVZueutkzCBjQcFkDn9BFyeaayDfUmYbk2oWFEzm6Kr7yFJcGJMWFhRM5ujq6WuNFhSMSQcLCiZzxFoKHWVKbayxJ64ZkwYWFEzmCARBQ6Bt7cujUWiqs5aCMWlgQcFkDn8n+Y9adkA0bJejGpMGFhRM5gh0kurCUlwYkzYWFEzmiCfF2+MKJEtxYUzaWFAwmaOzpHiW4sKYtLGgYDJHZ0nx4kHBuo+MSTULCiZzdPZMhcY6Z2h3NBuTchYUTOYQN1NqR91HuSXgz/KkWsYMJhYUTOYQca5A2jMoWIoLY9LGgoLJLP7gx68+shQXxqSNBQWTWTpqKViKC2PSxoKCySzWUjDGUxYUTGYJBNu3FKIRy3tkTBpZUDCZZc/uo6ZtgFreI2PSxIKCySx+N1Nq1M2UaikujEkrCwoms+yZFM/uZjYmrSwomMyyZ1I8y3tkTFpZUDCZZc+keLEUF9ZSMCYtLCiYzBLY40E7jTUgPsgb4l2djBlELCiYzLJnUrzGGsgbCj6/d3UyZhCxoGAyi3+P9NmW98iYtLKgYDKLLwfEn9B9VGsnmY1JIwsKJrOItE91YUHBmLSyoGAyT+JdzY011n1kTBqlPSiIyBgRWSQi74rIOyJymVs+VEReEJHV7tAuNxmsYi0FjUDLDktxYUwaedFSCANXqepBwJHAJSJyEHAN8KKqTgBedN+bwSiWFC/qdiFZ95ExaZP2oKCqm1R1mTteD6wERgGnAQ+4sz0AnJ7uupkMEes+irpdSNZ9ZEzaeHpOQUTGAdOB14DhqrrJnbQZGN7JZ+aIyFIRWVpTU5Oeipr0inUfWUvBmLTzLCiISBB4ArhcVXclTlNVBbSjz6nqfFWdqaozy8vtCHJACgQh2grRHc57aykYkzaeBAURycIJCAtU9Um3eIuIjHSnjwS2elE3kwHimVItbbYx6ebF1UcC3AOsVNXfJEx6Gpjtjs8G/pruupkMEburObIVfAHILfG0OsYMJgEP1nkU8FXgLRGpcst+BNwEPCoiXwfWA+d4UDeTCeJJ8Wqcy1FFvK2PMYNI2oOCqv4b6Oy//Lh01sVkqFj3UWQbFEz2ti7GDDJ2R7PJPLHuI6JQUOppVYwZbCwomMwTaymAXXlkTJpZUDCZx5dH/E/TgoIxaWVBwWQekd0nm/Ot+8iYdLKgYDJTLChYS8GYtLKgYDKT3z2vYEHBmLTy4j4FY7oXbynY3cypFAqFqK6upqWlxeuqmBTIzc1l9OjRZGVl9fgzFhRMZoq3FCwopFJ1dTWFhYWMGzcOsZsEBxRVpa6ujurqaioqKnr8Oes+MpnJzimkRUtLC6WlpRYQBiARobS0tNetQAsKJjMVHgjZEyE72P28Zq9YQBi4+vLbWveRyUwlM6BhuOU9MibNrKVgjPHUjTfeyOTJkznkkEOYNm0ar732GgDf+MY3ePfdd1O67pNPPpkdO3Z8rPy6665j3rx5HZaPGjWKadOmMWHCBM4888we1XHVqlVMmzaN6dOns3bt2mRUPWWspWCM8cySJUt49tlnWbZsGTk5OdTW1tLW1gbAH/7wh5Sv/7nnnuv1Z6644gq+973vAfDII4/w2c9+lrfeeouuHvr11FNPcdZZZ/GTn/ykz3VNFwsKxhgArn/mHd7duKv7GXvhoH2KuPaLnWe63bRpE2VlZeTk5ABQVrb7arNjjz2WefPmMXPmTO655x5uvvlmSkpKmDp1Kjk5Odxxxx1ccMEF5OXlsXz5crZu3cq9997LH//4R5YsWcIRRxzB/fffD8BDDz3Ez3/+c1SVU045hZtvvhmAcePGsXTpUsrKyrjxxht54IEHGDZsGGPGjGHGjBndbt+sWbP429/+xp///Gcuu+wyKisrufLKK2loaKCsrIz777+f5cuXc+utt+L3+3nxxRdZtGgRDz74ILfddhttbW0cccQR/O53v8Pv9xMMBrnssst49tlnycvL469//SvDhw/nscce4/rrr8fv91NcXMxLL71EJBLhmmuu4V//+hetra1ccsklfOtb39qLX8th3UfGGM+ccMIJfPTRRxxwwAF8+9vfZvHixR+bZ+PGjfzsZz/j1Vdf5ZVXXmHVqlXtpm/fvp0lS5Zwyy23cOqpp3LFFVfwzjvv8NZbb1FVVcXGjRu5+uqr+ec//0lVVRVvvPEGTz31VLtlVFZW8vDDD1NVVcVzzz3HG2+80eNtOPTQQ1m1ahWhUIhLL72Uxx9/nMrKSi666CJ+/OMfc/LJJ/M///M/XHHFFSxatIiVK1fyyCOP8Morr1BVVYXf72fBggUANDY2cuSRR7JixQqOPvpo7r77bgBuuOEGnn/+eVasWMHTTz8NwD333ENxcTFvvPEGb7zxBnfffTcffvhhb77+DllLwRgD0OURfaoEg0EqKyt5+eWXWbRoEbNmzeKmm27iggsuiM/z+uuvc8wxxzB06FAAzj77bN5///349C9+8YuICFOmTGH48OFMmTIFgMmTJ7Nu3TrWr1/PscceG+/eOf/883nppZc4/fTT48t4+eWXOeOMM8jPzwfg1FNP7fE2OI+Uh/fee4+3336b448/HoBIJMLIkSM/Nv+LL75IZWUlhx12GADNzc0MGzYMgOzsbL7whS8AMGPGDF544QUAjjrqKC644ALOOecczjzzTAAWLlzIm2++yeOPPw7Azp07Wb16da/uSeiIBQVjjKf8fj/HHnssxx57LFOmTOGBBx5oFxS6E+t68vl88fHY+3A43Ku7efti+fLlzJw5E1Vl8uTJLFmypMv5VZXZs2fzi1/84mPTsrKy4peR+v1+wuEwAL///e957bXX+Nvf/saMGTOorKxEVbn99ts58cQTk7o91n1kjPHMe++9x+rVq+Pvq6qq2HfffdvNc9hhh7F48WK2b99OOBzmiSee6NU6Dj/8cBYvXkxtbS2RSISHHnqIY445pt08Rx99NE899RTNzc3U19fzzDPP9GjZTzzxBAsXLuS8885j4sSJ1NTUxINCKBTinXfe+dhnjjvuOB5//HG2bt0KwLZt21i/fn2X61m7di1HHHEEN9xwA+Xl5Xz00UeceOKJ3HXXXYRCIQDef/99Ghsbe1TvrlhLwRjjmYaGBi699FJ27NhBIBBg/PjxzJ8/v908o0aN4kc/+hGHH344Q4cOZdKkSRQXF/d4HSNHjuSmm27iM5/5TPxE82mnndZunkMPPZRZs2YxdepUhg0bFu/a6cgtt9zCgw8+SGNjIwcffDD//Oc/411Tjz/+ON/97nfZuXMn4XCYyy+/nMmT23fLHXTQQcydO5cTTjiBaDRKVlYWd95558eCYaLvf//7rF69GlXluOOOY+rUqRxyyCGsW7eOQw89FFWlvLz8Y+dK+kJi/WH90cyZM3Xp0qVeV8P01l13wejR3c9XXQ0XX5z6+gxiK1eu5MADD/S6Gt1qaGggGAwSDoc544wzuOiiizjjjDO8rla/0NFvLCKVqjqzo/mt+8gYk/Guu+46pk2bxsEHH0xFRUW7k8Qmuaz7yBiT8Tq6u9ikhrUUjDHGxFlQMMYYE2dBwRhjTJwFBWOMMXEWFIwxnhIRrrrqqvj7efPmcd111/X48/fffz/l5eVMnz6dCRMmcOKJJ/Kf//yn28/V1NRwxBFHMH36dF5++eW+VH1AsqBgjPFUTk4OTz75JLW1tX1exqxZs1i+fDmrV6/mmmuu4cwzz2TlypVdfubFF19kypQpLF++nE9/+tN9XvdAY5ekGmMc/3cNbH4rucscMQVOuqnLWQKBAHPmzOGWW27hxhtvbDdt3bp1XHTRRdTW1lJeXs59993H2LFju1zeZz7zGebMmcP8+fO55ZZbWLt2LZdccgk1NTXk5+dz991309LSwg9+8AOam5tZunQpS5Ys4eWXX+baa6+ltbWV/fffn/vuu49gMMi4ceOYPXs2zzzzDKFQiMcee4xJkyaxePFiLrvsMsBp7bz00ksUFhbyq1/9ikcffZTW1lbOOOMMrr/++r37DtPMWgrGGM9dcsklLFiwgJ07d7Yrv/TSS5k9ezZvvvkm559/Pt/97nd7tLxYOmuAOXPmcPvtt1NZWcm8efP49re/zbRp07jhhhuYNWsWVVVVNDY2MnfuXP7xj3+wbNkyZs6cyW9+85v48srKyli2bBkXX3xx/J6JefPmceedd1JVVcXLL79MXl4eCxcuZPXq1bz++utUVVVRWVnJSy+9lKRvKT2spWCMcXRzRJ9KRUVFfO1rX+O2224jLy8vXr5kyRKefPJJAL761a/ygx/8oEfLi6XvaWho4D//+Q9nn312fFpra+vH5n/11Vd59913OeqoowBoa2vjE5/4RHx6LF31jBkz4vU56qijuPLKKzn//PM588wzGT16NAsXLmThwoVMnz49vv7Vq1dz9NFH9/i78FpGBQUR+TzwW8AP/EFVvfsrNcak1eWXX86hhx7KhRdeuNfLWr58OQceeCDRaJSSkhKqqqq6nF9VOf7443nooYc6nB5LyZ2Yzvqaa67hlFNO4bnnnuOoo47i+eefR1X54Q9/mJQnoHklY7qPRMQP3AmcBBwEnCciB3lbK2NMugwdOpRzzjmHe+65J172yU9+kocffhiABQsW9OiE8OLFi5k/fz7f/OY3KSoqoqKigsceewxwdv4rVqz42GeOPPJIXnnlFdasWQM4T0BLfJBPR9auXcuUKVO4+uqrOeyww1i1ahUnnngi9957Lw0NDQBs2LAhniK7v8iklsLhwBpV/QBARB4GTgPe9bRWxpi0ueqqq7jjjjvi72+//XYuvPBCfvWrX8VPNHfkkUce4d///jdNTU1UVFTwxBNPxDODLliwgIsvvpi5c+cSCoU499xzmTp1arvPl5eXc//993PeeefFu5fmzp3LAQcc0Gldb731VhYtWoTP52Py5MmcdNJJ5OTksHLlynjXUzAY5MEHH4w/Wa0/yJjU2SJyFvB5Vf2G+/6rwBGq+p095psDzAEYO3bsjO4eTmEy0KOPQl1d9/OVlsI556S+PoNYf0mdbfqut6mzM6ml0COqOh+YD87zFDyujukL29Ebk7Ey5pwCsAEYk/B+tFtmjDEmTTIpKLwBTBCRChHJBs4Fnva4TsYMeJnShWySry+/bcYEBVUNA98BngdWAo+q6sefem2MSZrc3Fzq6uosMAxAqkpdXR25ubm9+lxGnVNQ1eeA57yuhzGDxejRo6murqampsbrqpgUyM3NZXRPnoeeIKOCgjEmvbKysqioqPC6GiaDZEz3kTHGGO9ZUDDGGBNnQcEYY0xcxtzR3BciUgPseUtzGdD3p3VknoG2PTDwtmmgbQ8MvG0aaNsDe7dN+6pqeUcT+nVQ6IiILO3s9u3+aKBtDwy8bRpo2wMDb5sG2vZA6rbJuo+MMcbEWVAwxhgTNxCDwnyvK5BkA217YOBt00DbHhh42zTQtgdStE0D7pyCMcaYvhuILQVjjDF9ZEHBGGNM3IAJCiLyeRF5T0TWiMg1XtcnGURknYi8JSJVIrLU6/r0hYjcKyJbReTthLKhIvKCiKx2h0O8rGNvdLI914nIBvd3qhKRk72sY2+IyBgRWSQi74rIOyJymVven3+jzrapX/5OIpIrIq+LyAp3e653yytE5DV3n/eI+8iBvV/fQDinICJ+4H3geKAa59kM56lqv36+s4isA2aqar+96UZEjgYagD+q6sFu2S+Bbap6kxvAh6jq1V7Ws6c62Z7rgAZVnedl3fpCREYCI1V1mYgUApXA6cAF9N/fqLNtOod++DuJiAAFqtogIlnAv4HLgCuBJ1X1YRH5PbBCVe/a2/UNlJbC4cAaVf1AVduAh4HTPK6TAVT1JWDbHsWnAQ+44w/g/MP2C51sT7+lqptUdZk7Xo/zLJNR9O/fqLNt6pfU0eC+zXJfCnwWeNwtT9pvNFCCwijgo4T31fTjP4IECiwUkUoRmeN1ZZJouKpucsc3A8O9rEySfEdE3nS7l/pNV0siERkHTAdeY4D8RntsE/TT30lE/CJSBWwFXgDWAjvch5NBEvd5AyUoDFSfUtVDgZOAS9yuiwFFnf7L/t6HeRewPzAN2AT82tPa9IGIBIEngMtVdVfitP76G3WwTf32d1LViKpOw3l2/eHApFSta6AEhQ3AmIT3o92yfk1VN7jDrcBfcP4YBoItbr9vrP93q8f12SuqusX9p40Cd9PPfie3n/oJYIGqPukW9+vfqKNt6u+/E4Cq7gAWAZ8ASkQk9qC0pO3zBkpQeAOY4J6NzwbOBZ72uE57RUQK3JNkiEgBcALwdtef6jeeBma747OBv3pYl70W23m6zqAf/U7uScx7gJWq+puESf32N+psm/rr7yQi5SJS4o7n4VxQsxInOJzlzpa032hAXH0E4F5edivgB+5V1Ru9rdHeEZH9cFoH4Dw29c/9cZtE5CHgWJw0v1uAa4GngEeBsTipz89R1X5x8raT7TkWp0tCgXXAtxL64zOaiHwKeBl4C4i6xT/C6YPvr79RZ9t0Hv3wdxKRQ3BOJPtxDuQfVdUb3H3Ew8BQYDnwFVVt3ev1DZSgYIwxZu8NlO4jY4wxSWBBwRhjTJwFBWOMMXEWFIwxxsRZUDDGGBNnQcGYLohIJCGrZlUyM/CKyLjEbKvGZIJA97MYM6g1u+kFjBkUrKVgTB+4z7r4pfu8i9dFZLxbPk5E/ukmXXtRRMa65cNF5C9uTvwVIvJJd1F+EbnbzZO/0L1j1RjPWFAwpmt5e3QfzUqYtlNVpwB34NxND3A78ICqHgIsAG5zy28DFqvqVOBQ4B23fAJwp6pOBnYAX0rp1hjTDbuj2ZguiEiDqgY7KF8HfFZVP3CTr21W1VIRqcV5wEvILd+kqmUiUgOMTkxD4KZ1fkFVJ7jvrwayVHVuGjbNmA5ZS8GYvtNOxnsjMVdNBDvPZzxmQcGYvpuVMFzijv8HJ0svwPk4idkAXgQuhvgDU4rTVUljesOOSozpWp77xKuYv6tq7LLUISLyJs7R/nlu2aXAfSLyfaAGuNAtvwyYLyJfx2kRXIzzoBdjMoqdUzCmD9xzCjNVtdbruhiTTNZ9ZIwxJs5aCsYYY+KspWCMMSbOgoIxxpg4CwrGGGPiLCgYY4yJs6BgjDEm7v8Dm6mmCsBx2P4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_plot_better(tracks2, 30, title=\"One Shot Noise Attack\", y_axis_lab=\"%\", should_average=False,  add_attack_region =(10,12), n=0, alpha = .3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs187",
   "language": "python",
   "name": "cs187"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
